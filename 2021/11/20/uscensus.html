<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="twitter:card" content="summary_large_image" /><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Uscensus | ML Notes</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="Uscensus" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="US cencus income : Ensembles, Bagging and Shap Values" />
<meta property="og:description" content="US cencus income : Ensembles, Bagging and Shap Values" />
<link rel="canonical" href="https://younesszaim.github.io/myportfolio/2021/11/20/uscensus.html" />
<meta property="og:url" content="https://younesszaim.github.io/myportfolio/2021/11/20/uscensus.html" />
<meta property="og:site_name" content="ML Notes" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2021-11-20T00:00:00-06:00" />
<script type="application/ld+json">
{"url":"https://younesszaim.github.io/myportfolio/2021/11/20/uscensus.html","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://younesszaim.github.io/myportfolio/2021/11/20/uscensus.html"},"headline":"Uscensus","dateModified":"2021-11-20T00:00:00-06:00","datePublished":"2021-11-20T00:00:00-06:00","description":"US cencus income : Ensembles, Bagging and Shap Values","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/myportfolio/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://younesszaim.github.io/myportfolio/feed.xml" title="ML Notes" /><link rel="shortcut icon" type="image/x-icon" href="/myportfolio/images/favicon.ico"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/Primer/15.2.0/primer.css" integrity="sha512-xTz2ys4coGAOz8vuV1NcQBkgVmKhsSEtjbqyMJbBHRplFuvKIUo6xhLHpAyPt9mfR6twHJgn9OgVLuqOvjeBhg==" crossorigin="anonymous" />
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.css" integrity="sha512-h7nl+xz8wgDlNM4NqKEM4F1NkIRS17M9+uJwIGwuo8vGqIl4BhuCKdxjWEINm+xyrUjNCnK5dCrhM0sj+wTIXw==" crossorigin="anonymous" />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.js" integrity="sha512-/CMIhXiDA3m2c9kzRyd97MTb3MC6OVnx4TElQ7fkkoRghwDf6gi41gaT1PwF270W6+J60uTmwgeRpNpJdRV6sg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/contrib/auto-render.min.js" integrity="sha512-Do7uJAaHZm5OLrIv/yN4w0iG1dbu01kzdMNnFfu/mAqgUk6Nniv2JYHcwH+cNwjqgLcqcuBBk+JRvprLVI8azg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha512-0doc9hKxR3PYwso42RD1p5ySZpzzuDiOwMrdCEh2WdJZCjcmFKc/wEnL+z8fBQrnHoiNWbo+3fiGkOYXBdQp4A==" crossorigin="anonymous"></script>
    <script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement( document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "[%", right: "%]", display: true},
            {left: "$", right: "$", display: false}
        ]}
        );
    });
    </script>


<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/myportfolio/">ML Notes</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/myportfolio/about/">About Me</a><a class="page-link" href="/myportfolio/search/">Search</a><a class="page-link" href="/myportfolio/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Uscensus</h1><p class="post-meta post-meta-title"><time class="dt-published" datetime="2021-11-20T00:00:00-06:00" itemprop="datePublished">
        Nov 20, 2021
      </time>
       • <span class="read-time" title="Estimated read time">
    
    
      28 min read
    
</span></p>

    

    </header>

  <div class="post-content e-content" itemprop="articleBody">
    <h1 id="us-cencus-income--ensembles-bagging-and-shap-values">US cencus income : Ensembles, Bagging and Shap Values</h1>

<ul>
  <li>toc: true</li>
  <li>badges: true</li>
  <li>comments: true</li>
  <li>categories: [fastpages, jupyter]</li>
  <li>image: images/shap_values.png</li>
</ul>

<h1 id="problem-framework">Problem Framework</h1>

<p>Our task is to determine the income level for the person represented by the record. Incomes have been binned at the $50K level to present a <strong>binary classification problem</strong>.</p>

<p>The dataset used in this analysis was extracted from the census bureau database found at. The data was split into train/test in approximately 2/3, 1/3 proportions.</p>

<p>The following mappings of the data is as follow :</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>import pandas as pd
from pandas.api.types import is_string_dtype, is_numeric_dtype, is_categorical_dtype
import numpy as np
df_labels = pd.read_csv(f'{PATH}/census_income_metadata_column.csv', sep=';')
df_labels.head(5)
</code></pre></div></div>

<p>In any sort of data science work, it’s important to look at our data directly to make sure we understand the format, how it’s stored, what types of values it holds, etc. Even if we’ve read a description of the data, the actual data may not be what we expect. We’ll start by reading the training set into a Pandas DataFrame :</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code># Loading the train data 
df = pd.read_csv(f'{PATH}/census_income_learn.csv', names = df_labels['column_name'])
df.shape
</code></pre></div></div>

<p>Let’s have a look at the columns, their types defined by Pandas and compared it to their actual mapping types :</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code># Chekcing the mapping of the data 
d1 = df.dtypes.apply(lambda x: x.name).to_dict()
d2 = {c: d for c,d in zip(df_labels['column_name'],df_labels['dtype'])}
mapping = [d1, d2]
d = {}
for k in d1.keys():
    d[k] = tuple(d[k] for d in mapping)
d
</code></pre></div></div>

<p>We can see that <strong>detailed_industry_recode</strong>, <strong>detailed_occupation_recode</strong>, <strong>own_business_or_self_employed</strong>, <strong>veterans_benefits</strong> and <strong>year</strong> is set by default as a continuos category.</p>

<p>Let’s redifined their types :</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code># Correcting data types
d1['detailed_industry_recode']='object'
d1['detailed_occupation_recode']='object'
d1['own_business_or_self_employed']='object'
d1['veterans_benefits']='object'
d1['year']='object'
</code></pre></div></div>

<p>Let’s reload the data with its correspind feature’s mapping :</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code># reload data with coorexted types
df = pd.read_csv(f'{PATH}/census_income_learn.csv', names =df_labels['column_name'],
                 dtype= d1)
</code></pre></div></div>

<p>The <strong>info()</strong> method is useful to get a quick description of the data, in particular the total number of rows, each attribute’s type, and the number of nonnull values :</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>df.info()
# dispplay first rows 
with pd.option_context('display.max_rows', None, 'display.max_columns', None):  
        display(df.head(3))
# drop 'ignore' column
df.drop('ignore', axis=1,inplace=True)
# list columns
df.columns
</code></pre></div></div>

<p>We load the test set with the same training data types :</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code># loading the test set 
test = pd.read_csv(f'{PATH}/census_income_test.csv', names =df_labels['column_name'], dtype= d1 )
test.info()
</code></pre></div></div>

<p>We verify if we got the same columns both on the train and the test set :</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code># checking columns on test set which not in train
set(test.columns).difference(set(df.columns))
# dropping 'ignore' columns
test.drop('ignore', inplace=True, axis=1)
test.columns
# display first rows of the test set
with pd.option_context('display.max_rows', None, 'display.max_columns', None):
        display(test.head(3))
df.shape, test.shape
</code></pre></div></div>

<h1 id="looking-at-the-data">Looking at the data</h1>

<p>The most important data column is the <strong>dependent variable</strong>—that is, the one we want to predict which is <strong>income_level</strong> :</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>dep_var = 'income_level'
</code></pre></div></div>

<p>Let’s see its distribution :</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>print(df[dep_var].value_counts(normalize = True))
df[dep_var].value_counts(normalize = True).plot(kind='bar',
                                 edgecolor='black',
                                 color='blue',
                                 title='income_level')
</code></pre></div></div>

<p>We have an <strong>imbalanced dataset</strong> where the income level of -50k is representing more than 93% of the total records.</p>

<p>Next, we automatically handle which columns are <strong>continuous</strong> and which are <strong>categorical</strong> :</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code># get categorical and numerical variables
def cont_cat_split(df, dep_var=None):
    "Helper function that returns column names of cont and cat variables from given `df`."
    cont_names, cat_names = [], []
    for label in df:
        if label in [dep_var]: continue
        if (pd.api.types.is_integer_dtype(df[label].dtype) or
            pd.api.types.is_float_dtype(df[label].dtype)):
            cont_names.append(label)
        else: cat_names.append(label)
    return cont_names, cat_names

cont, cat = cont_cat_split(df, dep_var= dep_var)
cont , cat
</code></pre></div></div>

<p>Let’s start by checking the modalties of our categorical variables :</p>

<p>Some categorical features are <strong>purely nominal</strong>-having multiple modalities (with modality <strong>?</strong> for nan values) and others are <strong>ordinal columns</strong> like <strong>education</strong> and <strong>year</strong>:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code># Ediucation modalities
df['education'].unique(), df['education'].nunique()
# Year modalities
df['year'].unique(), df['year'].nunique()
</code></pre></div></div>

<p>We can tell Pandas about a suitable ordering of these levels like so:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code># Setting the order of education variable
education = ' Children',' Less than 1st grade',' 1st 2nd 3rd or 4th grade',' 5th or 6th grade',\
' 7th and 8th grade',' 9th grade',' 10th grade',' 11th grade', ' 12th grade no diploma',\
' High school graduate', ' Associates degree-academic program',' Associates degree-occup /vocational',\
' Prof school degree (MD DDS DVM LLB JD)',' Some college but no degree',' Bachelors degree(BA AB BS)',\
' Masters degree(MA MS MEng MEd MSW MBA)',' Doctorate degree(PhD EdD)'
len(education)
# Setting the order of year variaable
year = '94', '95'
# apply the defined ordering fot our data :
df['education'] = df['education'].astype('category')
df['education'].cat.set_categories(education, ordered=True, inplace=True)

df['year'] = df['year'].astype('category')
df['year'].cat.set_categories(year, ordered=True, inplace=True)

#Same for test set :
test['education'] = test['education'].astype('category')
test['education'].cat.set_categories(education, ordered=True, inplace=True)

test['year'] = test['year'].astype('category')
test['year'].cat.set_categories(year, ordered=True, inplace=True)
</code></pre></div></div>

<p>Lets check our continous features: The <strong>describe()</strong> method shows a summary of the numerical attributes</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>df[cont].describe()
</code></pre></div></div>

<p>The <strong>count</strong>, <strong>mean</strong>, <strong>min</strong>, and <strong>max</strong> rows are self-explanatory.The <strong>std</strong> row shows the standard deviation, which measures how dispersed the values are. The 25%, 50%, and 75% rows show the corresponding percentiles.</p>

<p>We plot a histogram for each numerical attribute :</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>%matplotlib inline 
import matplotlib.pyplot as plt
df[cont].hist(bins=50, figsize=(20,15))
plt.show()
</code></pre></div></div>

<ul>
  <li>
    <p>We can see that these attributes have very <strong>different scales</strong>.</p>
  </li>
  <li>
    <p>Some numerical varaibles are countinous like <strong>age</strong> and others are discrete and finite like <strong>weeks_worked_in_year</strong> or infinete <strong>num_persons_worked_for_employer</strong>.</p>
  </li>
  <li>
    <p>Some features as <strong>wage_per_hour</strong>,<strong>capital_gains</strong>,<strong>capital_losses</strong>,<strong>dividends_from_stocks</strong> are tail-heavy: they extend much farther to the median right with high coefficient of variation :</p>
  </li>
</ul>

<!-- end list -->

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>df[cont].boxplot(column=['wage_per_hour','capital_gains','capital_losses','dividends_from_stocks'],
                 figsize=(10,5))
</code></pre></div></div>

<p>We can see the presence of <strong>extreme values</strong> for those features.</p>

<p>Using the <strong>skewness value</strong>, which explains the extent to which the data is normally distributed, in order to confirm that. Ideally, the skewness value should be between -1 and +1, and any major deviation from this range indicates the presence of extreme values.</p>

<p>We can calculate the skwenss value :</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code># skewness value
df[['wage_per_hour','capital_gains','capital_losses','dividends_from_stocks']].skew()
</code></pre></div></div>

<p>Using the <strong>IQR score</strong>, let’s see the number of obseravtions that are not in the <code class="language-plaintext highlighter-rouge">(Q1 - 1.5 IQR) and (Q3 + 1.5 IQR)</code> range :</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code># IQR score
Q1 = df[['wage_per_hour','capital_gains','capital_losses','dividends_from_stocks']].quantile(0.25)
Q3 = df[['wage_per_hour','capital_gains','capital_losses','dividends_from_stocks']].quantile(0.75)
IQR = Q3 - Q1
print(IQR)
# number of observation out of the definied range
out = df[['wage_per_hour','capital_gains','capital_losses','dividends_from_stocks']]
df_out = out[((out &lt; (Q1 - 1.5 * IQR)) |(out &gt; (Q3 + 1.5 * IQR))).any(axis=1)]

out.shape, df_out.shape, df_out.shape[0]/out.shape[0]
</code></pre></div></div>

<p>From 199.523 observation of the selcted features, 38.859 records (19%) represent extrem values.</p>

<p>For <strong>weeks_worked_in_year</strong> :</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>df['weeks_worked_in_year'].plot( kind='hist',
                                 bins=53,
                                 edgecolor='black',
                                 color='blue',
                                 title='weeks worked in a year',
                                 figsize=(10,5))
</code></pre></div></div>

<p>For <strong>num_persons_worked_for_employer</strong> :</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>df['num_persons_worked_for_employer'].plot( kind='hist',
                                 
                                 edgecolor='black',
                                 color='blue',
                                 title='num_persons_worked_for_employer',
                                 figsize=(7,5))
</code></pre></div></div>

<p>We notice an increase in the 7th bins <strong>num_persons_worked_for_employer=6</strong>. Check if this variable is capped ?</p>

<h1 id="exploratory-data-analysis">Exploratory data analysis</h1>

<ul>
  <li>Starting with <strong>numerical variables</strong> :</li>
</ul>

<!-- end list -->

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>import seaborn as sns
data_dia = df[dep_var]
data = df[['age']]
data = pd.concat([data_dia,data],axis=1)
data = pd.melt(data,id_vars="income_level",
                    var_name="features",
                    value_name='value')
plt.figure(figsize=(6,5))
sns.violinplot(x="features", y="value", hue="income_level", data=data,split=True, inner="quartile")
plt.xticks(rotation=90)
</code></pre></div></div>

<p>For the <strong>age</strong> feature, we can see that the medians of the income levels +/- 50k look separated. The income level of +50k with a median of 50 years old has a lower interquntile range (IQR) with value spread of 10 years. Whereas The income level of -50k has a median of 30 years old has and interquantile range (IQR) of 40 years. So, <strong>age</strong> can be good for classification.</p>

<p>Let’s look at the <strong>weeks_worked_in_year</strong> feature :</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code># get the number of income class in each week
weeks_worked_in_year = df.groupby(["weeks_worked_in_year", "income_level"])\
                        .size()\
                        .groupby(level=0).apply(lambda x: 100*x/x.sum()).unstack()

# print the percentage class for the first and last weeks 
weeks_worked_in_year.iloc[[0,1,2, -3,-2,-1]]
weeks_worked_in_year.plot(kind='bar', 
                          stacked=True,
                          edgecolor='black', 
                          figsize=(12,5))
</code></pre></div></div>

<p>We can see that the propotion of people making more than 50k a year is increasing with the number of working weeks in a given year where it can reach more than 14% for those working 52 weeks . However, the -50k level of income is representing the higher propotion regardless of the number of working weeks. We notice that among those how don’t work at all, 0.6% still make more than 50k a year.</p>

<p>Let’s look at <strong>num_persons_worked_for_employer</strong> :</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>num_persons_worked_for_employer = df.groupby(["num_persons_worked_for_employer", "income_level"])\
                        .size()\
                        .groupby(level=0).apply(lambda x: 100*x/x.sum()).unstack()

num_persons_worked_for_employer
num_persons_worked_for_employer.plot(kind='bar', 
                          stacked=True,
                          edgecolor='black', 
                          figsize=(12,5))
</code></pre></div></div>

<p>The proportion of +50k income level increases with the number of the num_preson_worked_for_employer where it reaches <strong>16% for num_preson_worked_for_employer= 6</strong>.</p>

<p>Let’s see the average of <strong>wage_per_hour</strong>,<strong>capital_gains</strong>,<strong>capital_losses</strong>,<strong>dividends_from_stocks</strong> across the income levels :</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>avg = df[['wage_per_hour','capital_gains','capital_losses','dividends_from_stocks','income_level']]\
.groupby('income_level')\
.mean()

avg.plot(kind='bar', title = 'average across income levels', figsize=(10,5))

avg
</code></pre></div></div>

<p>We can see that people making more than 50k a year, have on average, <strong>higher wage per hour</strong>,<strong>higher return on capital asset</strong> and <strong>dividends from stock options</strong>.</p>

<ul>
  <li>Next, let’s analyse some <strong>categorical variables</strong> :</li>
</ul>

<!-- end list -->

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code># Education variable
pd.crosstab(df['income_level'], 
            df['education'],
            margins = True,
           normalize = 'columns').style.format('{:.2%}')
pd.crosstab(df['income_level'], 
            df['education'],
            margins = True,
           normalize = 'columns').plot(kind='bar',stacked=True, edgecolor='black', 
                          figsize=(12,10))

plt.legend(bbox_to_anchor=(1.5, 1.0))
</code></pre></div></div>

<p>We can see the effect of education on income level where more than 50% of <strong>Prof school degree</strong> and <strong>Doctorate degree</strong> earn more than 50k a year. On the other hand, the majority of people (more than 90%) with <strong>no degree</strong> earn less than 50k a year.</p>

<p>Let’s further this analysis and see the effect of <strong>education</strong> and <strong>the number of working weeks</strong> :</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pd.crosstab(df['income_level'], 
            df['education'],
            values = df['weeks_worked_in_year'],
            aggfunc = 'mean').round(2)
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
plt.figure(figsize=(20, 4))
sns.heatmap(
    pd.crosstab(df['income_level'], 
            df['education'],
            values = df['weeks_worked_in_year'],
            aggfunc = 'mean').round(1) 
    ,annot = True
    ,linewidths=.5
    ,cmap="YlGnBu"
    
)
plt.show()
</code></pre></div></div>

<p>We can see that earning more than 50k a year demands high level of education but also lot of hard work !</p>

<p>Let’s analyse the effect of <strong>sex</strong> and <strong>marital_stat</strong> on income level :</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pd.crosstab(df['income_level'], 
            [df['sex'],df['marital_stat']],
           margins = True,
           normalize = 'columns').style.format('{:.2%}')
pd.crosstab(df['income_level'], 
            [df['sex'],df['marital_stat']]).plot(kind='bar',stacked=True, edgecolor='black',
                                                 
                          figsize=(12,10))
</code></pre></div></div>

<p>We can see that the highest proportion of people earning less than 50k a year are mostly <strong>female Married-A F spouse present</strong> or <strong>never married</strong> and <strong>seperated male</strong>. On the other hand, <strong>male Married-civilian spouse present</strong> represent the highest propotion on the +50k income level.</p>

<p>We can further the analysis more as we have got many interesting features with several modalities but for now let’s see how machine learning models can help us understanding more our data.</p>

<h1 id="data-preparation">Data preparation</h1>

<p>We set <strong>the feature vector</strong> and <strong>the target variable</strong> :</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code># setting feature vector and target variable for the train set

X = df.drop(['income_level'], axis = 1)
y = df['income_level']

# setting feature vector and target variable for the test set
test_x = test.drop(['income_level'], axis = 1)
test_y = test['income_level']
# Cheching the result
df.shape, X.shape, y.shape
</code></pre></div></div>

<p>We will keep the provided <strong>test set</strong> hidden and will use it as a realtime dataset when we make our model on production in order to avoid the risk of <strong>data snooping</strong>.</p>

<p>For that, we will be using a validation set derived from our training set (30%). Scikit-Learn provides a few functions to split datasets into multiple subsets in various ways. The simplest function is <strong>train_test_split()</strong>.</p>

<p>Since we have an imbalanced dataset, we can’t considered purely random sampling methods. For that, we do stratified sampling based on the <strong>income level</strong>.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>from sklearn.model_selection import train_test_split

X_train, X_val, y_train, y_val = train_test_split(X, 
                                                  y, 
                                                  test_size=0.3, 
                                                  random_state=12,
                                                  stratify=y)
# Checking the train and validation set
X_train.shape, X_val.shape
# Checking the income level proportion
y_train.value_counts(normalize = True), y_val.value_counts(normalize = True)
</code></pre></div></div>

<p>What if we didn’t stratify with respect to income level ?</p>

<p>We can compare the income level proportions in the <strong>overall dataset</strong>, in the <strong>test set</strong> generated with <code class="language-plaintext highlighter-rouge">stratified sampling</code>, and in <strong>a test set</strong> generated using <code class="language-plaintext highlighter-rouge">purely random sampling</code>.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>def income_cat_proportions(data):
    return data["income_level"].value_counts() / len(data)

train_set, test_set = train_test_split(df, test_size=0.3, random_state=12)

train_set, test_set_strat = train_test_split(df, test_size=0.3, random_state=12,stratify=df['income_level'])

compare_props = pd.DataFrame({
    "Overall": income_cat_proportions(df),
    "Stratified": income_cat_proportions(test_set_strat),
    "Random": income_cat_proportions(test_set),
}).sort_index()
compare_props["Rand. %error"] = 100 * compare_props["Random"] / compare_props["Overall"] - 100
compare_props["Strat. %error"] = 100 * compare_props["Stratified"] / compare_props["Overall"] - 100
</code></pre></div></div>

<p>As we can see, the test set generated using stratified sampling has income level proportions almost identical to those in the full dataset, whereas the test set generated using purely random sampling is skewed.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>compare_props
</code></pre></div></div>

<p>Now that we defined our training set, It’s time to prepare the data for our machine Learning algorithms.</p>

<ul>
  <li>Data cleaning :</li>
</ul>

<p>We have seen previously that we don’t have any <strong>missing values</strong>. For some cataegorical features, we assumed that the <strong>?</strong> modality is encoded for NaN values.</p>

<ul>
  <li>Handling Text and Categorical Attributes :</li>
</ul>

<p>Strating with the target variable <strong>income_level</strong>, we use <strong>LabelEncoder()</strong> to encode target labels with value between 0 and <code class="language-plaintext highlighter-rouge">n_classes-1 = 1</code>.</p>

<p>We have seen also that we have some ordinal variable as <strong>education</strong> and <strong>year</strong>, so we use <strong>OrdinalEncoder</strong> to encode the categorical features as an integer array. The results in a single column of integers (0 to n_categories - 1) per feature.</p>

<p>Since the remaining categorical features have several modalities per feature, we use also <strong>OrdinalEncoder</strong> instead of <strong>OneHotEncoder</strong>.</p>

<blockquote>
  <p>Working with <strong>OneHotEncoder</strong> leads, in our case, to high memory consumption. We can combine <strong>OneHotEncoder</strong> and <strong>PCA</strong> : The benefit in PCA is that combination of N attributes is better than any individual attribute. And the disadvantage is in harder explanation what exactly that PCA component means. Therefore, for this work, we will sacrifice a bit of predictive power to get more understandable model.</p>
</blockquote>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code># categorical variables encoding
from sklearn.preprocessing import LabelEncoder, OrdinalEncoder

# For the traget varaible
le = LabelEncoder()
y_train = le.fit_transform(y_train)  #fit on training set
y_val = le.transform(y_val)    
test_y = le.transform(test_y) 


# For categorical features :
Or = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value = -1)
for c in cat : 
        X_train[c] = Or.fit_transform(np.array(X_train[c]).reshape(-1,1).astype(str))  #fit on training set
        X_val[c] = Or.transform(np.array(X_val[c]).reshape(-1,1).astype(str))
        test_x[c] = Or.transform(np.array(test_x[c]).reshape(-1,1).astype(str))

   
# Cheking categorical features encoding
X_train[cat].head(3)
# Cheking target feature encoding
set(y_train)
</code></pre></div></div>

<ul>
  <li>Feature Scaling :</li>
</ul>

<p>We saw previously that out numerical inputs have different scales like the <strong>weeks_worked_in_year</strong> and <strong>capital_gains</strong>. We will be using <strong>StandardScaler</strong> since standardization is much less affected by outliers.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()

for c in cont:
    X_train[c] = scaler.fit_transform(np.array(X_train[c]).reshape(-1,1)) # fir on the train set
    X_val[c] = scaler.transform(np.array(X_val[c]).reshape(-1,1))
    test_x[c] = scaler.transform(np.array(test_x[c]).reshape(-1,1))

#checking the standardization
X_train[cont].head(3)
</code></pre></div></div>

<p>So far, we have handled the <strong>categorical columns</strong> and the <strong>numerical columns</strong> :</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code># Checking the training set
with pd.option_context('display.max_rows', None, 'display.max_columns', None):  
        display(X_train.head(3))

X_train.shape, X_val.shape, test_x.shape
</code></pre></div></div>

<h1 id="data-modeling">Data modeling</h1>

<ul>
  <li>Selecting a Performance Measure :</li>
</ul>

<p><strong>Accuracy</strong> is the simplest way to measure the effectiveness of a classification task, and it’s the percentage of correct predictions over all predictions. In other words, in a binary classification task, you can calculate this by adding the number of True Positives (TPs) and True Negatives (TNs) and dividing them by a tally of all predictions made. As with regression metrics, you can measure accuracy for both train and test to gauge <strong>overfitting</strong>.</p>

<p>But, we can get an accuracy of 94%, which sounds pretty good, but it turns out we are always predicting <strong>-50k</strong>! In other words, even if we get high accuracy, it is meaningless unless we are predicting accurately for the least represented class, <strong>+50k</strong>.</p>

<p>For this reasing, we will be using <strong>F1-score</strong>. The <strong>F1-score</strong> is also called the harmonic average of precision and recall because it’s calculated like this: 2TP / 2TP + FP + FN. Since it includes both precision and recall metrics, which pertain to the proportion of true positives, it’s a good metric choice to use when the dataset is <strong>imbalanced</strong>, and we don’t prefer either precision or recall.</p>

<ul>
  <li>Base model :</li>
</ul>

<p>Let’s start with <strong>Decision tree ensembles</strong>.</p>

<p>A decision tree asks a series of binary (that is, yes or no) questions about the data. After each question the data at that part of the tree is split between a “yes” and a “no” branch. After one or more questions, either a prediction can be made on the basis of all previous answers or another question is required.</p>

<p>We illustarte a tree classification using <strong>4 leaf nodes</strong>.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>from sklearn.tree import DecisionTreeClassifier, plot_tree

m = DecisionTreeClassifier(max_leaf_nodes=4, random_state=14) # to plot the tree classification
m.fit(X_train, y_train)
# to get the class output
m.classes_
!pip install pydotplus
!pip install graphviz
</code></pre></div></div>

<p>The top node represents the <strong>initial model</strong> before any splits have been done, when all the data is in the initial income levels. This is the simplest possible model. It is the result of asking zero questions and will always predict the more represented class which is -50k. We use the <strong>Gini method</strong> to create split points. The strategy is to select each pair of adjacent values as a possible split-point and the point with smaller gini index chosen as the splitting point. In our case, the <strong>capital gains</strong> at 1.47 was choosen first.</p>

<p>Moving down and to the left, this node shows us that there were 130,999 records for income level of -50k where <strong>capital gains</strong> was less than 1.47. The class predicted is -50k in this case. Moving down and to the right from the initial model takes us to the records where <strong>capital gains</strong> was greater than 1.47. The class predicted is +50k in this case where 1370 records have an income of +50k and <strong>capital gains</strong> &gt;0.4</p>

<p>The bottom row contains our leaf nodes: the nodes with no answers coming out of them, because there are no more questions to be answered.</p>

<p>Returning back to the top node after the first decision point, we can see that a second binary decision split has been made, based on asking whether <strong>weeks_worked_per_year</strong> is less than or equal to 0.9. For the group where this is true, the class predicted is -50k with a gini of 0.019 and there are 85,411 records. For the records where this decision is false, the class predicted is -50k with a gini of 0.019, and there are 52,361 records. So again, we can see that the decision tree algorithm has successfully split out more records into two more groups which differ in gini value significantly.</p>

<p>Now, let’s run our base model :</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>m = DecisionTreeClassifier(random_state=14)
m.fit(X_train, y_train)
</code></pre></div></div>

<p>We evaluate the model on our validation set using <strong>accuracy</strong>, <strong>recall</strong> and <strong>f1 score</strong> :</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>from sklearn.metrics import accuracy_score, f1_score, recall_score

# on the train set 
accuracy_score(y_train,m.predict(X_train)) , \
recall_score(y_train,m.predict(X_train)), \
f1_score(y_train,m.predict(X_train), average='binary', pos_label=1)
# on the valid set 
accuracy_score(y_val,m.predict(X_val)) , \
recall_score(y_val,m.predict(X_val)), \
f1_score(y_val,m.predict(X_val), average='binary', pos_label=1) 
</code></pre></div></div>

<p>It’s seems that we are doing badly on the validation set. Let’s see houw many leaf nodes we got :</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>m.get_n_leaves(), len(X_train)
</code></pre></div></div>

<p>Sklearn’s default settings allow it to continue splitting nodes until there is only one item in each leaf node. Let’s change the stopping rule to tell sklearn to ensure every leaf node contains at least 25 records:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>m = DecisionTreeClassifier(min_samples_leaf=25,random_state=14)
m.fit(X_train, y_train)

# on the train set 
accuracy_score(y_train,m.predict(X_train)) , \
recall_score(y_train,m.predict(X_train)), \
f1_score(y_train,m.predict(X_train), average='binary', pos_label=1)

# on the valid set 
accuracy_score(y_val,m.predict(X_val)) , \
recall_score(y_val,m.predict(X_val)), \
f1_score(y_val,m.predict(X_val), average='binary', pos_label=1) 
</code></pre></div></div>

<p>That looks much better. Let’s check the number of leaves again:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>m.get_n_leaves(), len(X_train)
</code></pre></div></div>

<p>We got less leaf nodes than before. So, the more we increase the number of leaf nodes, the more is the possibility of <strong>overfitting</strong>.</p>

<p>Building a <strong>decision tree</strong> is a good way to create a model of our data. It is very flexible, since it can clearly handle nonlinear relationships and interactions between variables. But we can see there is a fundamental compromise between how well it generalizes (which we can achieve by creating small trees) and how accurate it is on the training set (which we can achieve by using large trees).</p>

<p>So how do we get the best of both worlds?</p>

<ul>
  <li>Ensembling :</li>
</ul>

<p>An an example of an Ensemble method is <strong>Random Forest</strong> : we can train a group of Decision Tree classifiers, each on a different random subset of the training set. The process of subseting the data is called <strong>bagging</strong> done with <strong>max_samples</strong> hyperparameter ( we set it at 100.00 samples) and the ramdom selection process this called <strong>bootsraping</strong> done by setting <strong>bootstrap = True</strong>.</p>

<p>With bagging, some instances may be sampled several times for any given predictor, while others may not be sampled at all. The remaining sampled are called <strong>out-of-bag (oob) instances</strong> used as <strong>validation set</strong> in the training process and done by setting <strong>oob_score=True</strong>.</p>

<p>We train a Random Forest classifier <strong>with 50 trees</strong> (each limited to <strong>minimum 5 samples per leaf</strong>). and instead of searching for the very best feature when splitting a node, we searches for the best feature among a random subset of 50% of our initial features.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>from sklearn.ensemble import RandomForestClassifier

rf = RandomForestClassifier(n_estimators = 50, max_samples=100_000, max_features=0.5, min_samples_leaf= 5, 
                            bootstrap= True,oob_score = True,random_state=14)
%%time
rf.fit(X_train,y_train)
# on the train set 
accuracy_score(y_train,rf.predict(X_train)) , \
recall_score(y_train,rf.predict(X_train)), \
f1_score(y_train,rf.predict(X_train), average='binary', pos_label=1)
# on the valid set 
accuracy_score(y_val,rf.predict(X_val)) , \
recall_score(y_val,rf.predict(X_val)), \
f1_score(y_val,rf.predict(X_val), average='binary', pos_label=1) 
</code></pre></div></div>

<p>Looking at what happens to the <strong>oob error rate</strong> as we add more and more trees, we you can see that the improvement levels off quite a bit after around 40 trees:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>scores =[]
for k in range(1, 50):
    rfc = RandomForestClassifier(n_estimators = k, max_samples=100_000, max_features=0.5, min_samples_leaf= 5, 
                            bootstrap= True,oob_score = True,random_state=14)
    rfc.fit(X_train, y_train)
    #y_pred = rfc.predict(X_val)
    #scores.append(accuracy_score(y_test, y_pred)) oob_score_
    oob_error = 1 - rfc.oob_score_
    scores.append(oob_error)

import matplotlib.pyplot as plt
%matplotlib inline

# plot the relationship between K and testing accuracy
# plt.plot(x_axis, y_axis)
plt.plot(range(1, 50), scores)
plt.xlabel('Value of n_estimators for Random Forest Classifier')
#plt.ylabel('Testing Accuracy')
plt.ylabel('OOB error rate')
</code></pre></div></div>

<p>Let’s try to improve our model :</p>

<p>We may ask <strong>which columns are the strongest predictors, which can we ignore?</strong></p>

<p>It’s not normally enough just to know that a model can make accurate predictions—we also want to know how it’s making predictions. Feature importance gives us insight into this. We can get these directly from sklearn’s random forest by looking in the feature_importances_ attribute. Here’s a simple function we can use to pop them into a DataFrame and sort them:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>def rf_feat_importance(m, df):
    return pd.DataFrame({'cols':df.columns, 'imp':m.feature_importances_}
                       ).sort_values('imp', ascending=False)
fi = rf_feat_importance(rf, X_train)
fi[:14]
</code></pre></div></div>

<p>The feature importances for our model show that the first few most important columns have much higher importance scores than the rest, with (not surprisingly) <strong>capital_gains</strong> and <strong>dividends_from_stocks</strong> being at the top of the list.</p>

<p>A plot of the feature importances shows the relative importances more clearly:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>def plot_fi(fi):
    return fi.plot('cols', 'imp', 'barh', figsize=(12,7), legend=False)

plot_fi(fi[:30]);
</code></pre></div></div>

<p>The way these importances are calculated is quite simple yet elegant. The feature importance algorithm loops through each tree, and then recursively explores each branch. At each branch, it looks to see what feature was used for that split, and how much the model improves as a result of that split. The improvement (weighted by the number of rows in that group) is added to the importance score for that feature. This is summed across all branches of all trees, and finally the scores are normalized such that they add to 1.</p>

<p>It seems likely that we could use just a subset of the columns by removing the variables of low importance and still get good results. Let’s try just keeping those with a feature importance greater than <strong>0.005</strong>:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>to_keep = fi[fi.imp&gt;0.005].cols
len(to_keep)
</code></pre></div></div>

<p>We can retrain our model using just this subset of the columns:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>X_train_imp = X_train[to_keep]
X_val_imp = X_val[to_keep]
m = RandomForestClassifier(n_estimators = 50, max_samples=100_000, max_features=0.5, min_samples_leaf= 5, 
                            bootstrap= True,oob_score = True,random_state=14)

m.fit(X_train_imp,y_train)
# on the train set 
accuracy_score(y_train,m.predict(X_train_imp)) , \
recall_score(y_train,m.predict(X_train_imp)), \
f1_score(y_train,m.predict(X_train_imp), average='binary', pos_label=1)
# on the valid set 
accuracy_score(y_val,m.predict(X_val_imp)) , \
recall_score(y_val,m.predict(X_val_imp)), \
f1_score(y_val,m.predict(X_val_imp), average='binary', pos_label=1) 
</code></pre></div></div>

<p>Our <strong>accuracy is about the same</strong>, but we have <strong>far fewer columns</strong> to study:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>len(X_train.columns), len(X_train_imp.columns)
</code></pre></div></div>

<p>We’ve found that generally the first step to improving a model is <strong>simplifying it</strong>—48 columns was too many for us to study them all in depth! Furthermore, in practice often a simpler, more interpretable model is easier to roll out and maintain.</p>

<p>This also makes our feature importance plot easier to interpret. Let’s look at it again:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>plot_fi(rf_feat_importance(m, X_train_imp));
</code></pre></div></div>

<p>Let’s see if we have redundent feature in our model by determining their similarities :</p>

<blockquote>
  <p>Determining Similarity: The most similar pairs are found by calculating the rank correlation, which means that all the values are replaced with their rank (i.e., first, second, third, etc. within the column), and then the correlation is calculated.</p>
</blockquote>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>import scipy
from scipy.cluster import hierarchy as hc

def cluster_columns(df, figsize=(10,6), font_size=12):
    corr = np.round(scipy.stats.spearmanr(df).correlation, 4)
    corr_condensed = hc.distance.squareform(1-corr)
    z = hc.linkage(corr_condensed, method='average')
    fig = plt.figure(figsize=figsize)
    hc.dendrogram(z, labels=df.columns, orientation='left', leaf_font_size=font_size)
    plt.show()

cluster_columns(X_train_imp)
</code></pre></div></div>

<p>Looking good! This is really not much worse than the model with all the fields. Let’s create DataFrames without these columns, and save them:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>X_train_final = X_train_imp # train
X_val_final = X_val_imp # valid 

test_x_final = test_x[to_keep] # test set
X_train_final.shape , X_val_final.shape, test_x_final.shape
</code></pre></div></div>

<h1 id="model-assesment-">Model Assesment :</h1>

<p>We have seen the <strong>DecisionTreeClassifier</strong> as our basemodel, then we tried <strong>RandomForestClassifier</strong> and finaly we tried to optimize so we can have less features for better interpretation.</p>

<p>Here is the model metrics on our <strong>validation set</strong> :</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>models_metrics = {'DTC': [0.95, 0.42, 0.51], 
                 'RF': [0.96, 0.42, 0.53],
                 'RF_less_feat': [0.95, 0.41, 0.53]
                 
                }
df = pd.DataFrame(data = models_metrics)
df.rename(index={0:'Accuracy',1:'Recall', 2: 'F1 score'}, 
                 inplace=True)
ax = df.plot(kind='bar', figsize = (10,5), 
        color = ['gold', 'lightgreen','lightcoral'],
        rot = 0, title ='Models performance',
        edgecolor = 'grey', alpha = 0.5)
for p in ax.patches:
    ax.annotate(str(p.get_height()), (p.get_x() * 1.01, p.get_height() * 1.0005))
plt.show()
</code></pre></div></div>

<p>Based on <strong>F1 score</strong>, we select the <strong>RandomForestClassifier</strong> with 25 features as our best model.</p>

<p>Let’s see the <strong>experiment results</strong>* of this model :</p>

<p>The <strong>precision_recall_curve</strong> and <strong>roc_curve</strong> are useful tools to visualize the <strong>sensitivity-specificty</strong> tradeoff in the classifier. They help inform a data scientist where to set the decision threshold of the model to maximize either sensitivity or specificity. This is called the <strong>operating point</strong> of the model.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>from sklearn.metrics import roc_curve, precision_recall_curve, \
auc, make_scorer, recall_score, accuracy_score, precision_score, confusion_matrix
# We create an array of the class probabilites called y_scores
y_scores = m.predict_proba(X_val_imp)[:, 1]

# we enerate the precision-recall curve for the classifier:
p, r, thresholds = precision_recall_curve(y_val, y_scores)

# We calculate the  F1 scores
f1_scores = 2*r*p/(r+p)
</code></pre></div></div>

<p>Let’s plot the <strong>decision chart</strong> of our model :</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>def plot_precision_recall_vs_threshold(precisions, recalls, thresholds):
    
    plt.figure(figsize=(8, 8))
    plt.title("Precision, Recall Scores and F1 scores as a function of the decision threshold")
    plt.plot(thresholds, precisions[:-1], "b--", label="Precision")
    plt.plot(thresholds, recalls[:-1], "g-", label="Recall")
    plt.plot(thresholds, f1_scores[:-1], "r-", label="F1-score")
    plt.ylabel("Score")
    plt.xlabel("Decision Threshold")
    plt.legend(loc='best')
    
plot_precision_recall_vs_threshold(p, r, thresholds)    
</code></pre></div></div>

<p>We can see that the the optimal threshold to achieve the highest F1 score is set at <strong>0.30</strong> with <strong>59% F1-score</strong>.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>print('Best threshold: ', thresholds[np.argmax(f1_scores)])
print('Best F1-Score: ', np.max(f1_scores))
</code></pre></div></div>

<p>Let’s creat <strong>an animated confusion matrix</strong> where the users get to choose the <strong>threesholds</strong> and we <strong>dislpay the confusion matrix and recall vs precision curve</strong> :</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pip install ipywidgets
import ipywidgets as widgets
import itertools
def plot_confusion_matrix(cm, classes,
                          normalize = False,
                          title = 'Confusion matrix"',
                          cmap = plt.cm.Blues) :
    plt.imshow(cm, interpolation = 'nearest', cmap = cmap)
    plt.title(title)
    plt.colorbar()
    tick_marks = np.arange(len(classes))
    plt.xticks(tick_marks, classes, rotation = 0)
    plt.yticks(tick_marks, classes)

    thresh = cm.max() / 2.
    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])) :
        plt.text(j, i, cm[i, j],
                 horizontalalignment = 'center',
                 color = 'white' if cm[i, j] &gt; thresh else 'black')

    plt.tight_layout()
    plt.ylabel('True label')
    plt.xlabel('Predicted label')
    
def adjusted_classes(y_scores, t):
    """
    This function adjusts class predictions based on the prediction threshold (t).
    Will only work for binary classification problems.
    """
    return [1 if y &gt;= t else 0 for y in y_scores]

def precision_recall_threshold(p, r, thresholds, t=0.5):
    """
    plots the precision recall curve and shows the current value for each
    by identifying the classifier's threshold (t).
    """
    
    # generate new class predictions based on the adjusted_classes
    # function above and view the resulting confusion matrix.
    y_pred_adj = adjusted_classes(y_scores, t)
    
    cm = confusion_matrix(y_val, y_pred_adj)
    class_names = [0,1]
    plt.figure()
    plot_confusion_matrix(cm, 
                      classes=class_names, 
                      title='RF Confusion matrix')

    plt.figure(figsize=(8,8))
    plt.title("Precision and Recall curve ^ = current threshold")
    plt.step(r, p, color='b', alpha=0.2,
             where='post')
    plt.fill_between(r, p, step='post', alpha=0.2,
                     color='b')
    
    plt.xlabel('Recall');
    plt.ylabel('Precision');
    
    # plot the current threshold on the line
    close_default_clf = np.argmin(np.abs(thresholds - t))
    plt.plot(r[close_default_clf], p[close_default_clf], '^', c='k',
            markersize=15)
slider = widgets.IntSlider(
    min=0,
    max=10,
    step=1,
    description='Slider:',
    value=3 # The best threshhold for our model
)
display(slider)

print(f'For this threshold : {slider.value/10}, the confusion matrix is as follow :')
precision_recall_threshold(p, r, thresholds, slider.value/10)
def plot_roc_curve(fpr, tpr, label=None):
    plt.figure(figsize=(8,8))
    plt.title('ROC Curve')
    plt.plot(fpr, tpr, linewidth=2, label=label)
    plt.plot([0, 1], [0, 1], 'k--')
    plt.axis([-0.005, 1, 0, 1.005])
    plt.xticks(np.arange(0,1, 0.05), rotation=90)
    plt.xlabel("False Positive Rate")
    plt.ylabel("True Positive Rate (Recall)")
    plt.legend(loc='best')
fpr, tpr, auc_thresholds = roc_curve(y_val, y_scores)
print(f'AUC : {auc(fpr, tpr)}') # AUC of ROC
plot_roc_curve(fpr, tpr, 'recall_optimized')
</code></pre></div></div>

<p>Now, let’s test this model on our <strong>test set</strong> :</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>accuracy_score(test_y,m.predict(test_x_final)) , \
recall_score(test_y,m.predict(test_x_final)) , \
f1_score(test_y,m.predict(test_x_final), average='binary', pos_label=1)
</code></pre></div></div>

<h1 id="results--partial-dependency-and-shap-values">Results : Partial dependency and SHAP values</h1>

<p>Let’s look at <strong>partial dependence plots</strong>.</p>

<p><strong>Partial dependence</strong> plots try to answer the question: if a row varied on nothing other than the feature in question, how would it impact the dependent variable?</p>

<p>For instance, how does <strong>capital_gains</strong> and <strong>dividends_from_stocks</strong> impact probability of belonging to the +50k income levl, all other things being equal?</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>from sklearn.inspection import plot_partial_dependence

fig,ax = plt.subplots(figsize=(20, 8))
plot_partial_dependence(m, X_val_final, ['capital_gains','dividends_from_stocks'], percentiles=(0,1),
                        grid_resolution=15, ax=ax);
</code></pre></div></div>

<p>Looking first at the <strong>dividends_from_stocks</strong> plot, we can see a nearly linear relationship between capital dividends_from_stocks and the probabillity of income level. Same for <strong>capital_gains</strong> at 5 standad deviation from the mean after reaching a steady state above that.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>!pip install shap
import shap
row_to_show = 20
data_for_prediction = test_x_final.iloc[row_to_show]  # use 1 row of data here. Could use multiple rows if desired
# Create object that can calculate shap values
explainer = shap.TreeExplainer(m)

# Calculate Shap values
shap_values = explainer.shap_values(data_for_prediction)
</code></pre></div></div>

<p>The above explanation shows features each contributing to push the model output from the base value (the average model output over the training dataset we passed) to the model output. Features pushing the prediction higher are shown in red, those pushing the prediction lower are in blue</p>

<ul>
  <li>The base_value here is 0.062 while our predicted value is 0.0.</li>
  <li>sex = 1 has the biggest impact on increasing the prediction, while</li>
  <li>Weeks_worked_im_year (below the average) and Age (below the average) feature has the biggest effect in decreasing the prediction.</li>
</ul>

<!-- end list -->

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>explainer = shap.TreeExplainer(m)

# calculate shap values. This is what we will plot.
# Calculate shap_values for all of val_X rather than a single row, to have more data for plot.
shap_values = explainer.shap_values(test_x_final.iloc[:1000,])

# Make plot. Index of [1] is explained in text below.
shap.summary_plot(shap_values[1],test_x_final.iloc[:1000,])
</code></pre></div></div>

<p>For every dot:</p>

<ul>
  <li>Vertical location shows what feature it is depicting</li>
  <li>Color shows whether that feature was high or low for that row of the dataset</li>
  <li>Horizontal location shows whether the effect of that value caused a higher or lower prediction.</li>
</ul>

<p>For the <strong>age</strong> variable, the point in the upper left was depicts a person whose age level is less thereby reducing the prediction of income level +50k class by 0.2.</p>

<h1 id="conclusion-">Conclusion :</h1>

<p>In this work, we presented some techniques for dealing with a machine learning project :</p>

<ul>
  <li>
    <p>We used Decision Tree ensembles : Random Forest are the easiest to train, because they are extremely resilient to hyperparameter choices and require very little preprocessing. They are very fast to train, and should not overfit if we have enough trees.</p>
  </li>
  <li>
    <p>we used the model for feature selection and partial dependence analysis and Shap values, to get a better understanding of our data.</p>
  </li>
</ul>

<p>For futur improvements :</p>

<ul>
  <li>
    <p>We can try Gradient Boosting machines as in theory are just as fast to train as random forests, but in practice we will have to try lots of different hyperparameters. They can overfit, but they are often a little more accurate than random forests.</p>
  </li>
  <li>
    <p>We can try OneHotEncoder with PCA to deal with the multiple modalities on our categorical variables.</p>
  </li>
  <li>
    <p>We can creat new features to challenge the model performance.</p>
  </li>
</ul>

<!-- end list -->

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>


</code></pre></div></div>

  </div><a class="u-url" href="/myportfolio/2021/11/20/uscensus.html" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/myportfolio/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/myportfolio/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/myportfolio/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>Documenting my journey in data science.</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/younesszaim" title="younesszaim"><svg class="svg-icon grey"><use xlink:href="/myportfolio/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://www.linkedin.com/in/youness-zaim-9a344b101" title="youness-zaim-9a344b101"><svg class="svg-icon grey"><use xlink:href="/myportfolio/assets/minima-social-icons.svg#linkedin"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
