{
  
    
        "post0": {
            "title": "Us Cencus Income   Ensembles, Bagging And Shap Values",
            "content": "US cencus income : Ensembles, Bagging and Shap Values . Problem Framework . Our task is to determine the income level for the person represented by the record. Incomes have been binned at the $50K level to present a binary classification problem. . The dataset used in this analysis was extracted from the census bureau database found at. The data was split into train/test in approximately 2/3, 1/3 proportions. . The following mappings of the data is as follow : . Datasets can be found in this link : https://github.com/younesszaim/myportfolio/tree/master/_notebooks/Datasets . import pandas as pd from pandas.api.types import is_string_dtype, is_numeric_dtype, is_categorical_dtype import numpy as np PATH = &#39;/Users/rmbp/Desktop/Dataiku Data Scientist Technical Assessment&#39; df_labels = pd.read_csv(f&#39;{PATH}/census_income_metadata_column.csv&#39;, sep=&#39;;&#39;) df_labels.head(5) column_name dtype 0 age continuous 1 class_of_worker nominal 2 detailed_industry_recode nominal 3 detailed_occupation_recode nominal 4 education nominal . In any sort of data science work, it’s important to look at our data directly to make sure we understand the format, how it’s stored, what types of values it holds, etc. Even if we’ve read a description of the data, the actual data may not be what we expect. We’ll start by reading the training set into a Pandas DataFrame : . # Loading the train data df = pd.read_csv(f&#39;{PATH}/census_income_learn.csv&#39;, names = df_labels[&#39;column_name&#39;]) df.shape (199523, 42) . Let’s have a look at the columns, their types defined by Pandas and compared it to their actual mapping types : . # Chekcing the mapping of the data d1 = df.dtypes.apply(lambda x: x.name).to_dict() d2 = {c: d for c,d in zip(df_labels[&#39;column_name&#39;],df_labels[&#39;dtype&#39;])} mapping = [d1, d2] d = {} for k in d1.keys(): d[k] = tuple(d[k] for d in mapping) d {&#39;age&#39;: (&#39;int64&#39;, &#39;continuous&#39;), &#39;class_of_worker&#39;: (&#39;object&#39;, &#39;nominal&#39;), &#39;detailed_industry_recode&#39;: (&#39;int64&#39;, &#39;nominal&#39;), &#39;detailed_occupation_recode&#39;: (&#39;int64&#39;, &#39;nominal&#39;), &#39;education&#39;: (&#39;object&#39;, &#39;nominal&#39;), &#39;wage_per_hour&#39;: (&#39;int64&#39;, &#39;continuous&#39;), &#39;enroll_in_edu_inst_last_wk&#39;: (&#39;object&#39;, &#39;nominal&#39;), &#39;marital_stat&#39;: (&#39;object&#39;, &#39;nominal&#39;), &#39;major_industry_code&#39;: (&#39;object&#39;, &#39;nominal&#39;), &#39;major_occupation_code&#39;: (&#39;object&#39;, &#39;nominal&#39;), &#39;race&#39;: (&#39;object&#39;, &#39;nominal&#39;), &#39;hispanic_origin&#39;: (&#39;object&#39;, &#39;nominal&#39;), &#39;sex&#39;: (&#39;object&#39;, &#39;nominal&#39;), &#39;member_of_a_labor_union&#39;: (&#39;object&#39;, &#39;nominal&#39;), &#39;reason_for_unemployment&#39;: (&#39;object&#39;, &#39;nominal&#39;), &#39;full_or_part_time_employment_stat&#39;: (&#39;object&#39;, &#39;nominal&#39;), &#39;capital_gains&#39;: (&#39;int64&#39;, &#39;continuous&#39;), &#39;capital_losses&#39;: (&#39;int64&#39;, &#39;continuous&#39;), &#39;dividends_from_stocks&#39;: (&#39;int64&#39;, &#39;continuous&#39;), &#39;tax_filer_stat&#39;: (&#39;object&#39;, &#39;nominal&#39;), &#39;region_of_previous_residence&#39;: (&#39;object&#39;, &#39;nominal&#39;), &#39;state_of_previous_residence&#39;: (&#39;object&#39;, &#39;nominal&#39;), &#39;detailed_household_and_family_stat&#39;: (&#39;object&#39;, &#39;nominal&#39;), &#39;detailed_household_summary_in_household&#39;: (&#39;object&#39;, &#39;nominal&#39;), &#39;ignore&#39;: (&#39;float64&#39;, &#39;continuous&#39;), &#39;migration_code-change_in_msa&#39;: (&#39;object&#39;, &#39;nominal&#39;), &#39;migration_code-change_in_reg&#39;: (&#39;object&#39;, &#39;nominal&#39;), &#39;migration_code-move_within_reg&#39;: (&#39;object&#39;, &#39;nominal&#39;), &#39;live_in_this_house_1_year_ago&#39;: (&#39;object&#39;, &#39;nominal&#39;), &#39;migration_prev_res_in_sunbelt&#39;: (&#39;object&#39;, &#39;nominal&#39;), &#39;num_persons_worked_for_employer&#39;: (&#39;int64&#39;, &#39;continuous&#39;), &#39;family_members_under_18&#39;: (&#39;object&#39;, &#39;nominal&#39;), &#39;country_of_birth_father&#39;: (&#39;object&#39;, &#39;nominal&#39;), &#39;country_of_birth_mother&#39;: (&#39;object&#39;, &#39;nominal&#39;), &#39;country_of_birth_self&#39;: (&#39;object&#39;, &#39;nominal&#39;), &#39;citizenship&#39;: (&#39;object&#39;, &#39;nominal&#39;), &#39;own_business_or_self_employed&#39;: (&#39;int64&#39;, &#39;nominal&#39;), &quot;fill_inc_questionnaire_for_veteran&#39;s_admin&quot;: (&#39;object&#39;, &#39;nominal&#39;), &#39;veterans_benefits&#39;: (&#39;int64&#39;, &#39;nominal&#39;), &#39;weeks_worked_in_year&#39;: (&#39;int64&#39;, &#39;continuous&#39;), &#39;year&#39;: (&#39;int64&#39;, &#39;nominal&#39;), &#39;income_level&#39;: (&#39;object&#39;, &#39;nominal&#39;)} . We can see that detailed_industry_recode, detailed_occupation_recode, own_business_or_self_employed, veterans_benefits and year is set by default as a continuos category. . Let’s redifined their types : . # Correcting data types d1[&#39;detailed_industry_recode&#39;]=&#39;object&#39; d1[&#39;detailed_occupation_recode&#39;]=&#39;object&#39; d1[&#39;own_business_or_self_employed&#39;]=&#39;object&#39; d1[&#39;veterans_benefits&#39;]=&#39;object&#39; d1[&#39;year&#39;]=&#39;object&#39; . Let’s reload the data with its correspind feature’s mapping : . # reload data with coorexted types df = pd.read_csv(f&#39;{PATH}/census_income_learn.csv&#39;, names =df_labels[&#39;column_name&#39;], dtype= d1) . The info() method is useful to get a quick description of the data, in particular the total number of rows, each attribute’s type, and the number of nonnull values : . df.info() &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 199523 entries, 0 to 199522 Data columns (total 42 columns): # Column Non-Null Count Dtype -- -- 0 age 199523 non-null int64 1 class_of_worker 199523 non-null object 2 detailed_industry_recode 199523 non-null object 3 detailed_occupation_recode 199523 non-null object 4 education 199523 non-null object 5 wage_per_hour 199523 non-null int64 6 enroll_in_edu_inst_last_wk 199523 non-null object 7 marital_stat 199523 non-null object 8 major_industry_code 199523 non-null object 9 major_occupation_code 199523 non-null object 10 race 199523 non-null object 11 hispanic_origin 199523 non-null object 12 sex 199523 non-null object 13 member_of_a_labor_union 199523 non-null object 14 reason_for_unemployment 199523 non-null object 15 full_or_part_time_employment_stat 199523 non-null object 16 capital_gains 199523 non-null int64 17 capital_losses 199523 non-null int64 18 dividends_from_stocks 199523 non-null int64 19 tax_filer_stat 199523 non-null object 20 region_of_previous_residence 199523 non-null object 21 state_of_previous_residence 199523 non-null object 22 detailed_household_and_family_stat 199523 non-null object 23 detailed_household_summary_in_household 199523 non-null object 24 ignore 199523 non-null float64 25 migration_code-change_in_msa 199523 non-null object 26 migration_code-change_in_reg 199523 non-null object 27 migration_code-move_within_reg 199523 non-null object 28 live_in_this_house_1_year_ago 199523 non-null object 29 migration_prev_res_in_sunbelt 199523 non-null object 30 num_persons_worked_for_employer 199523 non-null int64 31 family_members_under_18 199523 non-null object 32 country_of_birth_father 199523 non-null object 33 country_of_birth_mother 199523 non-null object 34 country_of_birth_self 199523 non-null object 35 citizenship 199523 non-null object 36 own_business_or_self_employed 199523 non-null object 37 fill_inc_questionnaire_for_veteran&#39;s_admin 199523 non-null object 38 veterans_benefits 199523 non-null object 39 weeks_worked_in_year 199523 non-null int64 40 year 199523 non-null object 41 income_level 199523 non-null object dtypes: float64(1), int64(7), object(34) memory usage: 63.9+ MB # dispplay first rows with pd.option_context(&#39;display.max_rows&#39;, None, &#39;display.max_columns&#39;, None): display(df.head(3)) age class_of_worker detailed_industry_recode 0 73 Not in universe 0 1 58 Self-employed-not incorporated 4 2 18 Not in universe 0 detailed_occupation_recode education wage_per_hour 0 0 High school graduate 0 1 34 Some college but no degree 0 2 0 10th grade 0 enroll_in_edu_inst_last_wk marital_stat major_industry_code 0 Not in universe Widowed Not in universe or children 1 Not in universe Divorced Construction 2 High school Never married Not in universe or children major_occupation_code race 0 Not in universe White 1 Precision production craft &amp; repair White 2 Not in universe Asian or Pacific Islander hispanic_origin sex member_of_a_labor_union reason_for_unemployment 0 All other Female Not in universe Not in universe 1 All other Male Not in universe Not in universe 2 All other Female Not in universe Not in universe full_or_part_time_employment_stat capital_gains capital_losses 0 Not in labor force 0 0 1 Children or Armed Forces 0 0 2 Not in labor force 0 0 dividends_from_stocks tax_filer_stat region_of_previous_residence 0 0 Nonfiler Not in universe 1 0 Head of household South 2 0 Nonfiler Not in universe state_of_previous_residence detailed_household_and_family_stat 0 Not in universe Other Rel 18+ ever marr not in subfamily 1 Arkansas Householder 2 Not in universe Child 18+ never marr Not in a subfamily detailed_household_summary_in_household ignore 0 Other relative of householder 1700.09 1 Householder 1053.55 2 Child 18 or older 991.95 migration_code-change_in_msa migration_code-change_in_reg 0 ? ? 1 MSA to MSA Same county 2 ? ? migration_code-move_within_reg live_in_this_house_1_year_ago 0 ? Not in universe under 1 year old 1 Same county No 2 ? Not in universe under 1 year old migration_prev_res_in_sunbelt num_persons_worked_for_employer 0 ? 0 1 Yes 1 2 ? 0 family_members_under_18 country_of_birth_father country_of_birth_mother 0 Not in universe United-States United-States 1 Not in universe United-States United-States 2 Not in universe Vietnam Vietnam country_of_birth_self citizenship 0 United-States Native- Born in the United States 1 United-States Native- Born in the United States 2 Vietnam Foreign born- Not a citizen of U S own_business_or_self_employed fill_inc_questionnaire_for_veteran&#39;s_admin 0 0 Not in universe 1 0 Not in universe 2 0 Not in universe veterans_benefits weeks_worked_in_year year income_level 0 2 0 95 - 50000. 1 2 52 94 - 50000. 2 2 0 95 - 50000. # drop &#39;ignore&#39; column df.drop(&#39;ignore&#39;, axis=1,inplace=True) # list columns df.columns Index([&#39;age&#39;, &#39;class_of_worker&#39;, &#39;detailed_industry_recode&#39;, &#39;detailed_occupation_recode&#39;, &#39;education&#39;, &#39;wage_per_hour&#39;, &#39;enroll_in_edu_inst_last_wk&#39;, &#39;marital_stat&#39;, &#39;major_industry_code&#39;, &#39;major_occupation_code&#39;, &#39;race&#39;, &#39;hispanic_origin&#39;, &#39;sex&#39;, &#39;member_of_a_labor_union&#39;, &#39;reason_for_unemployment&#39;, &#39;full_or_part_time_employment_stat&#39;, &#39;capital_gains&#39;, &#39;capital_losses&#39;, &#39;dividends_from_stocks&#39;, &#39;tax_filer_stat&#39;, &#39;region_of_previous_residence&#39;, &#39;state_of_previous_residence&#39;, &#39;detailed_household_and_family_stat&#39;, &#39;detailed_household_summary_in_household&#39;, &#39;migration_code-change_in_msa&#39;, &#39;migration_code-change_in_reg&#39;, &#39;migration_code-move_within_reg&#39;, &#39;live_in_this_house_1_year_ago&#39;, &#39;migration_prev_res_in_sunbelt&#39;, &#39;num_persons_worked_for_employer&#39;, &#39;family_members_under_18&#39;, &#39;country_of_birth_father&#39;, &#39;country_of_birth_mother&#39;, &#39;country_of_birth_self&#39;, &#39;citizenship&#39;, &#39;own_business_or_self_employed&#39;, &#39;fill_inc_questionnaire_for_veteran&#39;s_admin&#39;, &#39;veterans_benefits&#39;, &#39;weeks_worked_in_year&#39;, &#39;year&#39;, &#39;income_level&#39;], dtype=&#39;object&#39;) . We load the test set with the same training data types : . # loading the test set test = pd.read_csv(f&#39;{PATH}/census_income_test.csv&#39;, names =df_labels[&#39;column_name&#39;], dtype= d1 ) test.info() &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 99762 entries, 0 to 99761 Data columns (total 42 columns): # Column Non-Null Count Dtype -- -- 0 age 99762 non-null int64 1 class_of_worker 99762 non-null object 2 detailed_industry_recode 99762 non-null object 3 detailed_occupation_recode 99762 non-null object 4 education 99762 non-null object 5 wage_per_hour 99762 non-null int64 6 enroll_in_edu_inst_last_wk 99762 non-null object 7 marital_stat 99762 non-null object 8 major_industry_code 99762 non-null object 9 major_occupation_code 99762 non-null object 10 race 99762 non-null object 11 hispanic_origin 99762 non-null object 12 sex 99762 non-null object 13 member_of_a_labor_union 99762 non-null object 14 reason_for_unemployment 99762 non-null object 15 full_or_part_time_employment_stat 99762 non-null object 16 capital_gains 99762 non-null int64 17 capital_losses 99762 non-null int64 18 dividends_from_stocks 99762 non-null int64 19 tax_filer_stat 99762 non-null object 20 region_of_previous_residence 99762 non-null object 21 state_of_previous_residence 99762 non-null object 22 detailed_household_and_family_stat 99762 non-null object 23 detailed_household_summary_in_household 99762 non-null object 24 ignore 99762 non-null float64 25 migration_code-change_in_msa 99762 non-null object 26 migration_code-change_in_reg 99762 non-null object 27 migration_code-move_within_reg 99762 non-null object 28 live_in_this_house_1_year_ago 99762 non-null object 29 migration_prev_res_in_sunbelt 99762 non-null object 30 num_persons_worked_for_employer 99762 non-null int64 31 family_members_under_18 99762 non-null object 32 country_of_birth_father 99762 non-null object 33 country_of_birth_mother 99762 non-null object 34 country_of_birth_self 99762 non-null object 35 citizenship 99762 non-null object 36 own_business_or_self_employed 99762 non-null object 37 fill_inc_questionnaire_for_veteran&#39;s_admin 99762 non-null object 38 veterans_benefits 99762 non-null object 39 weeks_worked_in_year 99762 non-null int64 40 year 99762 non-null object 41 income_level 99762 non-null object dtypes: float64(1), int64(7), object(34) memory usage: 32.0+ MB . We verify if we got the same columns both on the train and the test set : . # checking columns on test set which not in train set(test.columns).difference(set(df.columns)) {&#39;ignore&#39;} # dropping &#39;ignore&#39; columns test.drop(&#39;ignore&#39;, inplace=True, axis=1) test.columns Index([&#39;age&#39;, &#39;class_of_worker&#39;, &#39;detailed_industry_recode&#39;, &#39;detailed_occupation_recode&#39;, &#39;education&#39;, &#39;wage_per_hour&#39;, &#39;enroll_in_edu_inst_last_wk&#39;, &#39;marital_stat&#39;, &#39;major_industry_code&#39;, &#39;major_occupation_code&#39;, &#39;race&#39;, &#39;hispanic_origin&#39;, &#39;sex&#39;, &#39;member_of_a_labor_union&#39;, &#39;reason_for_unemployment&#39;, &#39;full_or_part_time_employment_stat&#39;, &#39;capital_gains&#39;, &#39;capital_losses&#39;, &#39;dividends_from_stocks&#39;, &#39;tax_filer_stat&#39;, &#39;region_of_previous_residence&#39;, &#39;state_of_previous_residence&#39;, &#39;detailed_household_and_family_stat&#39;, &#39;detailed_household_summary_in_household&#39;, &#39;migration_code-change_in_msa&#39;, &#39;migration_code-change_in_reg&#39;, &#39;migration_code-move_within_reg&#39;, &#39;live_in_this_house_1_year_ago&#39;, &#39;migration_prev_res_in_sunbelt&#39;, &#39;num_persons_worked_for_employer&#39;, &#39;family_members_under_18&#39;, &#39;country_of_birth_father&#39;, &#39;country_of_birth_mother&#39;, &#39;country_of_birth_self&#39;, &#39;citizenship&#39;, &#39;own_business_or_self_employed&#39;, &#39;fill_inc_questionnaire_for_veteran&#39;s_admin&#39;, &#39;veterans_benefits&#39;, &#39;weeks_worked_in_year&#39;, &#39;year&#39;, &#39;income_level&#39;], dtype=&#39;object&#39;) # display first rows of the test set with pd.option_context(&#39;display.max_rows&#39;, None, &#39;display.max_columns&#39;, None): display(test.head(3)) age class_of_worker detailed_industry_recode 0 38 Private 6 1 44 Self-employed-not incorporated 37 2 2 Not in universe 0 detailed_occupation_recode education 0 36 1st 2nd 3rd or 4th grade 1 12 Associates degree-occup /vocational 2 0 Children wage_per_hour enroll_in_edu_inst_last_wk marital_stat 0 0 Not in universe Married-civilian spouse present 1 0 Not in universe Married-civilian spouse present 2 0 Not in universe Never married major_industry_code major_occupation_code 0 Manufacturing-durable goods Machine operators assmblrs &amp; inspctrs 1 Business and repair services Professional specialty 2 Not in universe or children Not in universe race hispanic_origin sex member_of_a_labor_union 0 White Mexican (Mexicano) Female Not in universe 1 White All other Female Not in universe 2 White Mexican-American Male Not in universe reason_for_unemployment full_or_part_time_employment_stat capital_gains 0 Not in universe Full-time schedules 0 1 Not in universe PT for econ reasons usually PT 0 2 Not in universe Children or Armed Forces 0 capital_losses dividends_from_stocks tax_filer_stat 0 0 0 Joint one under 65 &amp; one 65+ 1 0 2500 Joint both under 65 2 0 0 Nonfiler region_of_previous_residence state_of_previous_residence 0 Not in universe Not in universe 1 Not in universe Not in universe 2 Not in universe Not in universe detailed_household_and_family_stat 0 Spouse of householder 1 Spouse of householder 2 Child &lt;18 never marr not in subfamily detailed_household_summary_in_household migration_code-change_in_msa 0 Spouse of householder ? 1 Spouse of householder ? 2 Child under 18 never married ? migration_code-change_in_reg migration_code-move_within_reg 0 ? ? 1 ? ? 2 ? ? live_in_this_house_1_year_ago migration_prev_res_in_sunbelt 0 Not in universe under 1 year old ? 1 Not in universe under 1 year old ? 2 Not in universe under 1 year old ? num_persons_worked_for_employer family_members_under_18 0 4 Not in universe 1 1 Not in universe 2 0 Both parents present country_of_birth_father country_of_birth_mother country_of_birth_self 0 Mexico Mexico Mexico 1 United-States United-States United-States 2 United-States United-States United-States citizenship own_business_or_self_employed 0 Foreign born- Not a citizen of U S 0 1 Native- Born in the United States 0 2 Native- Born in the United States 0 fill_inc_questionnaire_for_veteran&#39;s_admin veterans_benefits 0 Not in universe 2 1 Not in universe 2 2 Not in universe 0 weeks_worked_in_year year income_level 0 12 95 - 50000. 1 26 95 - 50000. 2 0 95 - 50000. df.shape, test.shape ((199523, 41), (99762, 41)) . Looking at the data . The most important data column is the dependent variable—that is, the one we want to predict which is income_level : . dep_var = &#39;income_level&#39; . Let’s see its distribution : . print(df[dep_var].value_counts(normalize = True)) df[dep_var].value_counts(normalize = True).plot(kind=&#39;bar&#39;, edgecolor=&#39;black&#39;, color=&#39;blue&#39;, title=&#39;income_level&#39;) - 50000. 0.937942 50000+. 0.062058 Name: income_level, dtype: float64 &lt;matplotlib.axes._subplots.AxesSubplot at 0x124bace90&gt; . . We have an imbalanced dataset where the income level of -50k is representing more than 93% of the total records. . Next, we automatically handle which columns are continuous and which are categorical : . # get categorical and numerical variables def cont_cat_split(df, dep_var=None): &quot;Helper function that returns column names of cont and cat variables from given `df`.&quot; cont_names, cat_names = [], [] for label in df: if label in [dep_var]: continue if (pd.api.types.is_integer_dtype(df[label].dtype) or pd.api.types.is_float_dtype(df[label].dtype)): cont_names.append(label) else: cat_names.append(label) return cont_names, cat_names cont, cat = cont_cat_split(df, dep_var= dep_var) cont , cat ([&#39;age&#39;, &#39;wage_per_hour&#39;, &#39;capital_gains&#39;, &#39;capital_losses&#39;, &#39;dividends_from_stocks&#39;, &#39;num_persons_worked_for_employer&#39;, &#39;weeks_worked_in_year&#39;], [&#39;class_of_worker&#39;, &#39;detailed_industry_recode&#39;, &#39;detailed_occupation_recode&#39;, &#39;education&#39;, &#39;enroll_in_edu_inst_last_wk&#39;, &#39;marital_stat&#39;, &#39;major_industry_code&#39;, &#39;major_occupation_code&#39;, &#39;race&#39;, &#39;hispanic_origin&#39;, &#39;sex&#39;, &#39;member_of_a_labor_union&#39;, &#39;reason_for_unemployment&#39;, &#39;full_or_part_time_employment_stat&#39;, &#39;tax_filer_stat&#39;, &#39;region_of_previous_residence&#39;, &#39;state_of_previous_residence&#39;, &#39;detailed_household_and_family_stat&#39;, &#39;detailed_household_summary_in_household&#39;, &#39;migration_code-change_in_msa&#39;, &#39;migration_code-change_in_reg&#39;, &#39;migration_code-move_within_reg&#39;, &#39;live_in_this_house_1_year_ago&#39;, &#39;migration_prev_res_in_sunbelt&#39;, &#39;family_members_under_18&#39;, &#39;country_of_birth_father&#39;, &#39;country_of_birth_mother&#39;, &#39;country_of_birth_self&#39;, &#39;citizenship&#39;, &#39;own_business_or_self_employed&#39;, &quot;fill_inc_questionnaire_for_veteran&#39;s_admin&quot;, &#39;veterans_benefits&#39;, &#39;year&#39;]) . Let’s start by checking the modalties of our categorical variables : . # Check modalities of categorical varaiblies for c in cat : print(pd.DataFrame({c : df[c].value_counts()/len(df)})) class_of_worker Not in universe 0.502423 Private 0.361001 Self-employed-not incorporated 0.042326 Local government 0.039013 State government 0.021186 Self-employed-incorporated 0.016364 Federal government 0.014660 Never worked 0.002200 Without pay 0.000827 detailed_industry_recode 0 0.504624 33 0.085554 43 0.041514 4 0.029992 42 0.023471 45 0.022464 29 0.021095 37 0.020158 41 0.019867 32 0.018023 35 0.016940 39 0.014720 34 0.013858 44 0.012775 2 0.011006 11 0.008841 50 0.008540 40 0.008275 47 0.008240 38 0.008164 24 0.007533 12 0.006766 19 0.006746 30 0.005919 31 0.005904 25 0.005433 9 0.004977 22 0.004771 36 0.004736 13 0.004506 1 0.004145 48 0.003268 27 0.003137 49 0.003057 3 0.002822 21 0.002802 6 0.002777 5 0.002772 8 0.002757 16 0.002701 23 0.002631 18 0.002421 15 0.002265 7 0.002115 14 0.001479 46 0.000937 17 0.000787 28 0.000717 26 0.000637 51 0.000180 20 0.000160 10 0.000020 detailed_occupation_recode 0 0.504624 2 0.043885 26 0.039529 19 0.027130 29 0.025586 36 0.020775 34 0.020173 10 0.018459 16 0.017266 23 0.017001 12 0.016740 33 0.016665 3 0.016013 35 0.015878 38 0.015051 31 0.013527 32 0.012019 37 0.011197 8 0.010781 42 0.009613 30 0.009508 24 0.009257 17 0.008876 28 0.008325 41 0.007979 44 0.007979 43 0.006927 4 0.006836 13 0.006370 18 0.005428 39 0.005097 14 0.004671 5 0.004285 15 0.004085 27 0.003909 25 0.003844 9 0.003699 7 0.003664 11 0.003193 40 0.003092 1 0.002727 21 0.002671 6 0.002210 22 0.002060 45 0.000862 20 0.000356 46 0.000180 education High school graduate 0.242614 Children 0.237677 Some college but no degree 0.139433 Bachelors degree(BA AB BS) 0.099562 7th and 8th grade 0.040131 10th grade 0.037875 11th grade 0.034462 Masters degree(MA MS MEng MEd MSW MBA) 0.032783 9th grade 0.031224 Associates degree-occup /vocational 0.026854 Associates degree-academic program 0.021867 5th or 6th grade 0.016424 12th grade no diploma 0.010655 1st 2nd 3rd or 4th grade 0.009017 Prof school degree (MD DDS DVM LLB JD) 0.008986 Doctorate degree(PhD EdD) 0.006330 Less than 1st grade 0.004105 enroll_in_edu_inst_last_wk Not in universe 0.936950 High school 0.034542 College or university 0.028508 marital_stat Never married 0.433459 Married-civilian spouse present 0.422117 Divorced 0.063702 Widowed 0.052440 Separated 0.017341 Married-spouse absent 0.007608 Married-A F spouse present 0.003333 major_industry_code Not in universe or children 0.504624 Retail trade 0.085554 Manufacturing-durable goods 0.045183 Education 0.041514 Manufacturing-nondurable goods 0.034567 Finance insurance and real estate 0.030798 Construction 0.029992 Business and repair services 0.028323 Medical except hospital 0.023471 Public administration 0.023105 Other professional services 0.022464 Transportation 0.021095 Hospital services 0.019867 Wholesale trade 0.018023 Agriculture 0.015151 Personal services except private HH 0.014720 Social services 0.012775 Entertainment 0.008275 Communications 0.005919 Utilities and sanitary services 0.005904 Private household services 0.004736 Mining 0.002822 Forestry and fisheries 0.000937 Armed Forces 0.000180 major_occupation_code Not in universe 0.504624 Adm support including clerical 0.074362 Professional specialty 0.069867 Executive admin and managerial 0.062624 Other service 0.060640 Sales 0.059056 Precision production craft &amp; repair 0.052716 Machine operators assmblrs &amp; inspctrs 0.031971 Handlers equip cleaners etc 0.020684 Transportation and material moving 0.020148 Farming forestry and fishing 0.015768 Technicians and related support 0.015126 Protective services 0.008325 Private household services 0.003909 Armed Forces 0.000180 race White 0.838826 Black 0.102319 Asian or Pacific Islander 0.029245 Other 0.018329 Amer Indian Aleut or Eskimo 0.011282 hispanic_origin All other 0.861590 Mexican-American 0.040492 Mexican (Mexicano) 0.036256 Central or South American 0.019522 Puerto Rican 0.016605 Other Spanish 0.012455 Cuban 0.005643 NA 0.004380 Do not know 0.001534 Chicano 0.001524 sex Female 0.521163 Male 0.478837 member_of_a_labor_union Not in universe 0.904452 No 0.080362 Yes 0.015186 reason_for_unemployment Not in universe 0.969577 Other job loser 0.010214 Re-entrant 0.010119 Job loser - on layoff 0.004892 Job leaver 0.002997 New entrant 0.002200 full_or_part_time_employment_stat Children or Armed Forces 0.620324 Full-time schedules 0.204167 Not in labor force 0.134360 PT for non-econ reasons usually FT 0.016650 Unemployed full-time 0.011583 PT for econ reasons usually PT 0.006059 Unemployed part- time 0.004225 PT for econ reasons usually FT 0.002631 tax_filer_stat Nonfiler 0.376368 Joint both under 65 0.337720 Single 0.187552 Joint both 65+ 0.041760 Head of household 0.037219 Joint one under 65 &amp; one 65+ 0.019381 region_of_previous_residence Not in universe 0.920946 South 0.024503 West 0.020419 Midwest 0.017918 Northeast 0.013557 Abroad 0.002656 state_of_previous_residence Not in universe 0.920946 California 0.008590 Utah 0.005328 Florida 0.004255 North Carolina 0.004070 ? 0.003548 Abroad 0.003363 Oklahoma 0.003137 Minnesota 0.002887 Indiana 0.002671 North Dakota 0.002501 New Mexico 0.002321 Michigan 0.002210 Alaska 0.001453 Kentucky 0.001223 Arizona 0.001218 New Hampshire 0.001213 Wyoming 0.001208 Colorado 0.001198 Oregon 0.001183 West Virginia 0.001158 Georgia 0.001138 Montana 0.001133 Alabama 0.001083 Ohio 0.001058 Texas 0.001047 Arkansas 0.001027 Mississippi 0.001022 Tennessee 0.001012 Pennsylvania 0.000997 New York 0.000977 Louisiana 0.000962 Vermont 0.000957 Iowa 0.000947 Illinois 0.000902 Nebraska 0.000892 Missouri 0.000877 Nevada 0.000872 Maine 0.000837 Massachusetts 0.000757 Kansas 0.000747 South Dakota 0.000692 Maryland 0.000682 Virginia 0.000632 Connecticut 0.000586 District of Columbia 0.000581 Wisconsin 0.000526 South Carolina 0.000476 New Jersey 0.000376 Delaware 0.000366 Idaho 0.000155 detailed_household_and_family_stat Householder 0.266877 Child &lt;18 never marr not in subfamily 0.252232 Spouse of householder 0.208973 Nonfamily householder 0.111331 Child 18+ never marr Not in a subfamily 0.060294 Secondary individual 0.030683 Other Rel 18+ ever marr not in subfamily 0.009803 Grandchild &lt;18 never marr child of subfamily RP 0.009362 Other Rel 18+ never marr not in subfamily 0.008661 Grandchild &lt;18 never marr not in subfamily 0.005343 Child 18+ ever marr Not in a subfamily 0.005077 Child under 18 of RP of unrel subfamily 0.003669 RP of unrelated subfamily 0.003433 Child 18+ ever marr RP of subfamily 0.003363 Other Rel &lt;18 never marr child of subfamily RP 0.003288 Other Rel 18+ ever marr RP of subfamily 0.003288 Other Rel 18+ spouse of subfamily RP 0.003198 Child 18+ never marr RP of subfamily 0.002952 Other Rel &lt;18 never marr not in subfamily 0.002927 Grandchild 18+ never marr not in subfamily 0.001879 In group quarters 0.000982 Child 18+ spouse of subfamily RP 0.000632 Other Rel 18+ never marr RP of subfamily 0.000471 Child &lt;18 never marr RP of subfamily 0.000401 Spouse of RP of unrelated subfamily 0.000261 Child &lt;18 ever marr not in subfamily 0.000180 Grandchild 18+ ever marr not in subfamily 0.000170 Grandchild 18+ spouse of subfamily RP 0.000050 Child &lt;18 ever marr RP of subfamily 0.000045 Grandchild 18+ ever marr RP of subfamily 0.000045 Grandchild 18+ never marr RP of subfamily 0.000030 Other Rel &lt;18 ever marr RP of subfamily 0.000030 Other Rel &lt;18 never married RP of subfamily 0.000020 Other Rel &lt;18 spouse of subfamily RP 0.000015 Child &lt;18 spouse of subfamily RP 0.000010 Grandchild &lt;18 never marr RP of subfamily 0.000010 Grandchild &lt;18 ever marr not in subfamily 0.000010 Other Rel &lt;18 ever marr not in subfamily 0.000005 detailed_household_summary_in_household Householder 0.378277 Child under 18 never married 0.252733 Spouse of householder 0.209044 Child 18 or older 0.072322 Other relative of householder 0.048631 Nonrelative of householder 0.038096 Group Quarters- Secondary individual 0.000662 Child under 18 ever married 0.000236 migration_code-change_in_msa ? 0.499672 Nonmover 0.413677 MSA to MSA 0.053132 NonMSA to nonMSA 0.014089 Not in universe 0.007598 MSA to nonMSA 0.003959 NonMSA to MSA 0.003082 Abroad to MSA 0.002270 Not identifiable 0.002155 Abroad to nonMSA 0.000366 migration_code-change_in_reg ? 0.499672 Nonmover 0.413677 Same county 0.049177 Different county same state 0.014018 Not in universe 0.007598 Different region 0.005904 Different state same division 0.004967 Abroad 0.002656 Different division same region 0.002331 migration_code-move_within_reg ? 0.499672 Nonmover 0.413677 Same county 0.049177 Different county same state 0.014018 Not in universe 0.007598 Different state in South 0.004877 Different state in West 0.003403 Different state in Midwest 0.002762 Abroad 0.002656 Different state in Northeast 0.002160 live_in_this_house_1_year_ago Not in universe under 1 year old 0.507270 Yes 0.413677 No 0.079054 migration_prev_res_in_sunbelt ? 0.499672 Not in universe 0.421275 No 0.050054 Yes 0.028999 family_members_under_18 Not in universe 0.722884 Both parents present 0.195381 Mother only present 0.064013 Father only present 0.009438 Neither parent present 0.008285 country_of_birth_father United-States 0.797718 Mexico 0.050160 ? 0.033645 Puerto-Rico 0.013432 Italy 0.011086 Canada 0.006916 Germany 0.006796 Dominican-Republic 0.006465 Poland 0.006074 Philippines 0.005784 Cuba 0.005638 El-Salvador 0.004922 China 0.004290 England 0.003974 Columbia 0.003077 India 0.002907 South Korea 0.002656 Ireland 0.002546 Jamaica 0.002321 Vietnam 0.002290 Guatemala 0.002230 Japan 0.001965 Portugal 0.001945 Ecuador 0.001900 Haiti 0.001759 Greece 0.001724 Peru 0.001679 Nicaragua 0.001579 Hungary 0.001534 Scotland 0.001238 Iran 0.001168 Yugoslavia 0.001088 Taiwan 0.000997 Cambodia 0.000982 Honduras 0.000972 France 0.000957 Outlying-U S (Guam USVI etc) 0.000797 Laos 0.000772 Trinadad&amp;Tobago 0.000566 Thailand 0.000536 Hong Kong 0.000531 Holand-Netherlands 0.000256 Panama 0.000125 country_of_birth_mother United-States 0.804313 Mexico 0.049022 ? 0.030668 Puerto-Rico 0.012395 Italy 0.009242 Canada 0.007272 Germany 0.006927 Philippines 0.006170 Poland 0.005563 El-Salvador 0.005553 Cuba 0.005553 Dominican-Republic 0.005528 England 0.004526 China 0.003809 Columbia 0.003067 South Korea 0.003052 Ireland 0.003002 India 0.002912 Vietnam 0.002371 Japan 0.002351 Jamaica 0.002270 Guatemala 0.002225 Ecuador 0.001879 Peru 0.001779 Haiti 0.001769 Portugal 0.001714 Nicaragua 0.001509 Hungary 0.001489 Greece 0.001308 Scotland 0.001208 Taiwan 0.001113 Honduras 0.001093 France 0.001063 Iran 0.000992 Yugoslavia 0.000887 Cambodia 0.000787 Outlying-U S (Guam USVI etc) 0.000787 Laos 0.000777 Thailand 0.000616 Hong Kong 0.000536 Trinadad&amp;Tobago 0.000496 Holand-Netherlands 0.000246 Panama 0.000160 country_of_birth_self United-States 0.887061 Mexico 0.028904 ? 0.017006 Puerto-Rico 0.007017 Germany 0.004265 Philippines 0.004235 Cuba 0.004195 Canada 0.003508 Dominican-Republic 0.003458 El-Salvador 0.003453 China 0.002396 South Korea 0.002361 England 0.002290 Columbia 0.002175 Italy 0.002100 India 0.002045 Vietnam 0.001960 Poland 0.001910 Guatemala 0.001724 Japan 0.001699 Jamaica 0.001604 Peru 0.001343 Ecuador 0.001293 Haiti 0.001143 Nicaragua 0.001093 Taiwan 0.001007 Portugal 0.000872 Iran 0.000787 Greece 0.000737 Honduras 0.000722 Ireland 0.000677 France 0.000606 Outlying-U S (Guam USVI etc) 0.000596 Thailand 0.000566 Laos 0.000526 Hong Kong 0.000501 Cambodia 0.000476 Hungary 0.000396 Scotland 0.000376 Trinadad&amp;Tobago 0.000331 Yugoslavia 0.000331 Panama 0.000140 Holand-Netherlands 0.000115 citizenship Native- Born in the United States 0.887076 Foreign born- Not a citizen of U S 0.067165 Foreign born- U S citizen by naturalization 0.029345 Native- Born abroad of American Parent(s) 0.008801 Native- Born in Puerto Rico or U S Outlying 0.007613 own_business_or_self_employed 0 0.905520 2 0.080958 1 0.013522 fill_inc_questionnaire_for_veteran&#39;s_admin Not in universe 0.990056 No 0.007984 Yes 0.001960 veterans_benefits 2 0.752445 0 0.237612 1 0.009944 year 94 0.500328 95 0.499672 . Some categorical features are purely nominal-having multiple modalities (with modality ? for nan values) and others are ordinal columns like education and year: . # Ediucation modalities df[&#39;education&#39;].unique(), df[&#39;education&#39;].nunique() (array([&#39; High school graduate&#39;, &#39; Some college but no degree&#39;, &#39; 10th grade&#39;, &#39; Children&#39;, &#39; Bachelors degree(BA AB BS)&#39;, &#39; Masters degree(MA MS MEng MEd MSW MBA)&#39;, &#39; Less than 1st grade&#39;, &#39; Associates degree-academic program&#39;, &#39; 7th and 8th grade&#39;, &#39; 12th grade no diploma&#39;, &#39; Associates degree-occup /vocational&#39;, &#39; Prof school degree (MD DDS DVM LLB JD)&#39;, &#39; 5th or 6th grade&#39;, &#39; 11th grade&#39;, &#39; Doctorate degree(PhD EdD)&#39;, &#39; 9th grade&#39;, &#39; 1st 2nd 3rd or 4th grade&#39;], dtype=object), 17) # Year modalities df[&#39;year&#39;].unique(), df[&#39;year&#39;].nunique() (array([&#39; 95&#39;, &#39; 94&#39;], dtype=object), 2) . We can tell Pandas about a suitable ordering of these levels like so: . # Setting the order of education variable education = &#39; Children&#39;,&#39; Less than 1st grade&#39;,&#39; 1st 2nd 3rd or 4th grade&#39;,&#39; 5th or 6th grade&#39;, &#39; 7th and 8th grade&#39;,&#39; 9th grade&#39;,&#39; 10th grade&#39;,&#39; 11th grade&#39;, &#39; 12th grade no diploma&#39;, &#39; High school graduate&#39;, &#39; Associates degree-academic program&#39;,&#39; Associates degree-occup /vocational&#39;, &#39; Prof school degree (MD DDS DVM LLB JD)&#39;,&#39; Some college but no degree&#39;,&#39; Bachelors degree(BA AB BS)&#39;, &#39; Masters degree(MA MS MEng MEd MSW MBA)&#39;,&#39; Doctorate degree(PhD EdD)&#39; len(education) 17 # Setting the order of year variaable year = &#39;94&#39;, &#39;95&#39; # apply the defined ordering fot our data : df[&#39;education&#39;] = df[&#39;education&#39;].astype(&#39;category&#39;) df[&#39;education&#39;].cat.set_categories(education, ordered=True, inplace=True) df[&#39;year&#39;] = df[&#39;year&#39;].astype(&#39;category&#39;) df[&#39;year&#39;].cat.set_categories(year, ordered=True, inplace=True) #Same for test set : test[&#39;education&#39;] = test[&#39;education&#39;].astype(&#39;category&#39;) test[&#39;education&#39;].cat.set_categories(education, ordered=True, inplace=True) test[&#39;year&#39;] = test[&#39;year&#39;].astype(&#39;category&#39;) test[&#39;year&#39;].cat.set_categories(year, ordered=True, inplace=True) /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages/pandas/core/arrays/categorical.py:2631: FutureWarning: The `inplace` parameter in pandas.Categorical.set_categories is deprecated and will be removed in a future version. Removing unused categories will always return a new Categorical object. res = method(*args, **kwargs) . Lets check our continous features: The describe() method shows a summary of the numerical attributes . df[cont].describe() age wage_per_hour capital_gains capital_losses count 199523.000000 199523.000000 199523.00000 199523.000000 mean 34.494199 55.426908 434.71899 37.313788 std 22.310895 274.896454 4697.53128 271.896428 min 0.000000 0.000000 0.00000 0.000000 25% 15.000000 0.000000 0.00000 0.000000 50% 33.000000 0.000000 0.00000 0.000000 75% 50.000000 0.000000 0.00000 0.000000 max 90.000000 9999.000000 99999.00000 4608.000000 dividends_from_stocks num_persons_worked_for_employer count 199523.000000 199523.000000 mean 197.529533 1.956180 std 1984.163658 2.365126 min 0.000000 0.000000 25% 0.000000 0.000000 50% 0.000000 1.000000 75% 0.000000 4.000000 max 99999.000000 6.000000 weeks_worked_in_year count 199523.000000 mean 23.174897 std 24.411488 min 0.000000 25% 0.000000 50% 8.000000 75% 52.000000 max 52.000000 . The count, mean, min, and max rows are self-explanatory.The std row shows the standard deviation, which measures how dispersed the values are. The 25%, 50%, and 75% rows show the corresponding percentiles. . We plot a histogram for each numerical attribute : . %matplotlib inline import matplotlib.pyplot as plt df[cont].hist(bins=50, figsize=(20,15)) plt.show() . . We can see that these attributes have very different scales. . | Some numerical varaibles are countinous like age and others are discrete and finite like weeks_worked_in_year or infinete num_persons_worked_for_employer. . | Some features as wage_per_hour,capital_gains,capital_losses,dividends_from_stocks are tail-heavy: they extend much farther to the median right with high coefficient of variation : . | . df[cont].boxplot(column=[&#39;wage_per_hour&#39;,&#39;capital_gains&#39;,&#39;capital_losses&#39;,&#39;dividends_from_stocks&#39;], figsize=(10,5)) &lt;matplotlib.axes._subplots.AxesSubplot at 0x125ab4190&gt; . . We can see the presence of extreme values for those features. . Using the skewness value, which explains the extent to which the data is normally distributed, in order to confirm that. Ideally, the skewness value should be between -1 and +1, and any major deviation from this range indicates the presence of extreme values. . We can calculate the skwenss value : . # skewness value df[[&#39;wage_per_hour&#39;,&#39;capital_gains&#39;,&#39;capital_losses&#39;,&#39;dividends_from_stocks&#39;]].skew() wage_per_hour 8.935097 capital_gains 18.990822 capital_losses 7.632565 dividends_from_stocks 27.786502 dtype: float64 . Using the IQR score, let’s see the number of obseravtions that are not in the (Q1 - 1.5 IQR) and (Q3 + 1.5 IQR) range : . # IQR score Q1 = df[[&#39;wage_per_hour&#39;,&#39;capital_gains&#39;,&#39;capital_losses&#39;,&#39;dividends_from_stocks&#39;]].quantile(0.25) Q3 = df[[&#39;wage_per_hour&#39;,&#39;capital_gains&#39;,&#39;capital_losses&#39;,&#39;dividends_from_stocks&#39;]].quantile(0.75) IQR = Q3 - Q1 print(IQR) wage_per_hour 0.0 capital_gains 0.0 capital_losses 0.0 dividends_from_stocks 0.0 dtype: float64 # number of observation out of the definied range out = df[[&#39;wage_per_hour&#39;,&#39;capital_gains&#39;,&#39;capital_losses&#39;,&#39;dividends_from_stocks&#39;]] df_out = out[((out &lt; (Q1 - 1.5 * IQR)) |(out &gt; (Q3 + 1.5 * IQR))).any(axis=1)] out.shape, df_out.shape, df_out.shape[0]/out.shape[0] ((199523, 4), (38859, 4), 0.1947595014108649) . From 199.523 observation of the selcted features, 38.859 records (19%) represent extrem values. . For weeks_worked_in_year : . df[&#39;weeks_worked_in_year&#39;].plot( kind=&#39;hist&#39;, bins=53, edgecolor=&#39;black&#39;, color=&#39;blue&#39;, title=&#39;weeks worked in a year&#39;, figsize=(10,5)) &lt;matplotlib.axes._subplots.AxesSubplot at 0x12311dd50&gt; . . For num_persons_worked_for_employer : . df[&#39;num_persons_worked_for_employer&#39;].plot( kind=&#39;hist&#39;, edgecolor=&#39;black&#39;, color=&#39;blue&#39;, title=&#39;num_persons_worked_for_employer&#39;, figsize=(7,5)) &lt;matplotlib.axes._subplots.AxesSubplot at 0x1236b43d0&gt; . . We notice an increase in the 7th bins num_persons_worked_for_employer=6. Check if this variable is capped ? . Exploratory data analysis . Starting with numerical variables : | . import seaborn as sns data_dia = df[dep_var] data = df[[&#39;age&#39;]] data = pd.concat([data_dia,data],axis=1) data = pd.melt(data,id_vars=&quot;income_level&quot;, var_name=&quot;features&quot;, value_name=&#39;value&#39;) plt.figure(figsize=(6,5)) sns.violinplot(x=&quot;features&quot;, y=&quot;value&quot;, hue=&quot;income_level&quot;, data=data,split=True, inner=&quot;quartile&quot;) plt.xticks(rotation=90) (array([0]), &lt;a list of 1 Text xticklabel objects&gt;) . . For the age feature, we can see that the medians of the income levels +/- 50k look separated. The income level of +50k with a median of 50 years old has a lower interquntile range (IQR) with value spread of 10 years. Whereas The income level of -50k has a median of 30 years old has and interquantile range (IQR) of 40 years. So, age can be good for classification. . Let’s look at the weeks_worked_in_year feature : . # get the number of income class in each week weeks_worked_in_year = df.groupby([&quot;weeks_worked_in_year&quot;, &quot;income_level&quot;]) .size() .groupby(level=0).apply(lambda x: 100*x/x.sum()).unstack() # print the percentage class for the first and last weeks weeks_worked_in_year.iloc[[0,1,2, -3,-2,-1]] income_level - 50000. 50000+. weeks_worked_in_year 0 99.379057 0.620943 1 98.275862 1.724138 2 98.908297 1.091703 50 89.279514 10.720486 51 89.010989 10.989011 52 85.199249 14.800751 weeks_worked_in_year.plot(kind=&#39;bar&#39;, stacked=True, edgecolor=&#39;black&#39;, figsize=(12,5)) &lt;matplotlib.axes._subplots.AxesSubplot at 0x125e2ded0&gt; . . We can see that the propotion of people making more than 50k a year is increasing with the number of working weeks in a given year where it can reach more than 14% for those working 52 weeks . However, the -50k level of income is representing the higher propotion regardless of the number of working weeks. We notice that among those how don’t work at all, 0.6% still make more than 50k a year. . Let’s look at num_persons_worked_for_employer : . num_persons_worked_for_employer = df.groupby([&quot;num_persons_worked_for_employer&quot;, &quot;income_level&quot;]) .size() .groupby(level=0).apply(lambda x: 100*x/x.sum()).unstack() num_persons_worked_for_employer income_level - 50000. 50000+. num_persons_worked_for_employer 0 99.379057 0.620943 1 90.942923 9.057077 2 91.687333 8.312667 3 90.763501 9.236499 4 89.776758 10.223242 5 88.980944 11.019056 6 84.990825 15.009175 num_persons_worked_for_employer.plot(kind=&#39;bar&#39;, stacked=True, edgecolor=&#39;black&#39;, figsize=(12,5)) &lt;matplotlib.axes._subplots.AxesSubplot at 0x125fbe4d0&gt; . . The proportion of +50k income level increases with the number of the num_preson_worked_for_employer where it reaches 16% for num_preson_worked_for_employer= 6. . Let’s see the average of wage_per_hour,capital_gains,capital_losses,dividends_from_stocks across the income levels : . avg = df[[&#39;wage_per_hour&#39;,&#39;capital_gains&#39;,&#39;capital_losses&#39;,&#39;dividends_from_stocks&#39;,&#39;income_level&#39;]] .groupby(&#39;income_level&#39;) .mean() avg.plot(kind=&#39;bar&#39;, title = &#39;average across income levels&#39;, figsize=(10,5)) avg wage_per_hour capital_gains capital_losses income_level - 50000. 53.692526 143.848013 27.003730 50000+. 81.640284 4830.930060 193.139557 dividends_from_stocks income_level - 50000. 107.816518 50000+. 1553.448070 . . We can see that people making more than 50k a year, have on average, higher wage per hour,higher return on capital asset and dividends from stock options. . Next, let’s analyse some categorical variables : | . # Education variable pd.crosstab(df[&#39;income_level&#39;], df[&#39;education&#39;], margins = True, normalize = &#39;columns&#39;).style.format(&#39;{:.2%}&#39;) &lt;pandas.io.formats.style.Styler at 0x125f7b790&gt; pd.crosstab(df[&#39;income_level&#39;], df[&#39;education&#39;], margins = True, normalize = &#39;columns&#39;).plot(kind=&#39;bar&#39;,stacked=True, edgecolor=&#39;black&#39;, figsize=(12,10)) plt.legend(bbox_to_anchor=(1.5, 1.0)) &lt;matplotlib.legend.Legend at 0x125b9d110&gt; . . We can see the effect of education on income level where more than 50% of Prof school degree and Doctorate degree earn more than 50k a year. On the other hand, the majority of people (more than 90%) with no degree earn less than 50k a year. . Let’s further this analysis and see the effect of education and the number of working weeks : . pd.crosstab(df[&#39;income_level&#39;], df[&#39;education&#39;], values = df[&#39;weeks_worked_in_year&#39;], aggfunc = &#39;mean&#39;).round(2) education Children Less than 1st grade 1st 2nd 3rd or 4th grade income_level - 50000. 0.0 12.69 15.93 50000+. NaN 0.00 35.00 education 5th or 6th grade 7th and 8th grade 9th grade 10th grade income_level - 50000. 18.56 11.12 13.01 16.23 50000+. 39.64 39.00 43.47 42.29 education 11th grade 12th grade no diploma High school graduate income_level - 50000. 20.42 21.59 30.88 50000+. 46.59 47.71 46.53 education Associates degree-academic program income_level - 50000. 38.48 50000+. 49.63 education Associates degree-occup /vocational income_level - 50000. 37.99 50000+. 49.46 education Prof school degree (MD DDS DVM LLB JD) income_level - 50000. 34.55 50000+. 49.88 education Some college but no degree Bachelors degree(BA AB BS) income_level - 50000. 33.35 37.24 50000+. 47.17 48.66 education Masters degree(MA MS MEng MEd MSW MBA) income_level - 50000. 36.94 50000+. 48.47 education Doctorate degree(PhD EdD) income_level - 50000. 34.51 50000+. 48.23 import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns plt.figure(figsize=(20, 4)) sns.heatmap( pd.crosstab(df[&#39;income_level&#39;], df[&#39;education&#39;], values = df[&#39;weeks_worked_in_year&#39;], aggfunc = &#39;mean&#39;).round(1) ,annot = True ,linewidths=.5 ,cmap=&quot;YlGnBu&quot; ) plt.show() . . We can see that earning more than 50k a year demands high level of education but also lot of hard work ! . Let’s analyse the effect of sex and marital_stat on income level : . pd.crosstab(df[&#39;income_level&#39;], [df[&#39;sex&#39;],df[&#39;marital_stat&#39;]], margins = True, normalize = &#39;columns&#39;).style.format(&#39;{:.2%}&#39;) &lt;pandas.io.formats.style.Styler at 0x123262f50&gt; pd.crosstab(df[&#39;income_level&#39;], [df[&#39;sex&#39;],df[&#39;marital_stat&#39;]]).plot(kind=&#39;bar&#39;,stacked=True, edgecolor=&#39;black&#39;, figsize=(12,10)) &lt;matplotlib.axes._subplots.AxesSubplot at 0x12932b290&gt; . . We can see that the highest proportion of people earning less than 50k a year are mostly female Married-A F spouse present or never married and seperated male. On the other hand, male Married-civilian spouse present represent the highest propotion on the +50k income level. . We can further the analysis more as we have got many interesting features with several modalities but for now let’s see how machine learning models can help us understanding more our data. . Data preparation . We set the feature vector and the target variable : . # setting feature vector and target variable for the train set X = df.drop([&#39;income_level&#39;], axis = 1) y = df[&#39;income_level&#39;] # setting feature vector and target variable for the test set test_x = test.drop([&#39;income_level&#39;], axis = 1) test_y = test[&#39;income_level&#39;] # Cheching the result df.shape, X.shape, y.shape ((199523, 41), (199523, 40), (199523,)) . We will keep the provided test set hidden and will use it as a realtime dataset when we make our model on production in order to avoid the risk of data snooping. . For that, we will be using a validation set derived from our training set (30%). Scikit-Learn provides a few functions to split datasets into multiple subsets in various ways. The simplest function is train_test_split(). . Since we have an imbalanced dataset, we can’t considered purely random sampling methods. For that, we do stratified sampling based on the income level. . from sklearn.model_selection import train_test_split X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.3, random_state=12, stratify=y) # Checking the train and validation set X_train.shape, X_val.shape ((139666, 40), (59857, 40)) # Checking the income level proportion y_train.value_counts(normalize = True), y_val.value_counts(normalize = True) ( - 50000. 0.937945 50000+. 0.062055 Name: income_level, dtype: float64, - 50000. 0.937935 50000+. 0.062065 Name: income_level, dtype: float64) . What if we didn’t stratify with respect to income level ? . We can compare the income level proportions in the overall dataset, in the test set generated with stratified sampling, and in a test set generated using purely random sampling. . def income_cat_proportions(data): return data[&quot;income_level&quot;].value_counts() / len(data) train_set, test_set = train_test_split(df, test_size=0.3, random_state=12) train_set, test_set_strat = train_test_split(df, test_size=0.3, random_state=12,stratify=df[&#39;income_level&#39;]) compare_props = pd.DataFrame({ &quot;Overall&quot;: income_cat_proportions(df), &quot;Stratified&quot;: income_cat_proportions(test_set_strat), &quot;Random&quot;: income_cat_proportions(test_set), }).sort_index() compare_props[&quot;Rand. %error&quot;] = 100 * compare_props[&quot;Random&quot;] / compare_props[&quot;Overall&quot;] - 100 compare_props[&quot;Strat. %error&quot;] = 100 * compare_props[&quot;Stratified&quot;] / compare_props[&quot;Overall&quot;] - 100 . As we can see, the test set generated using stratified sampling has income level proportions almost identical to those in the full dataset, whereas the test set generated using purely random sampling is skewed. . compare_props Overall Stratified Random Rand. %error Strat. %error - 50000. 0.937942 0.937935 0.939305 0.145356 -0.000701 50000+. 0.062058 0.062065 0.060695 -2.196901 0.010601 . Now that we defined our training set, It’s time to prepare the data for our machine Learning algorithms. . Data cleaning : | . We have seen previously that we don’t have any missing values. For some cataegorical features, we assumed that the ? modality is encoded for NaN values. . Handling Text and Categorical Attributes : | . Strating with the target variable income_level, we use LabelEncoder() to encode target labels with value between 0 and n_classes-1 = 1. . We have seen also that we have some ordinal variable as education and year, so we use OrdinalEncoder to encode the categorical features as an integer array. The results in a single column of integers (0 to n_categories - 1) per feature. . Since the remaining categorical features have several modalities per feature, we use also OrdinalEncoder instead of OneHotEncoder. . Working with OneHotEncoder leads, in our case, to high memory consumption. We can combine OneHotEncoder and PCA : The benefit in PCA is that combination of N attributes is better than any individual attribute. And the disadvantage is in harder explanation what exactly that PCA component means. Therefore, for this work, we will sacrifice a bit of predictive power to get more understandable model. . # categorical variables encoding from sklearn.preprocessing import LabelEncoder, OrdinalEncoder # For the traget varaible le = LabelEncoder() y_train = le.fit_transform(y_train) #fit on training set y_val = le.transform(y_val) test_y = le.transform(test_y) # For categorical features : Or = OrdinalEncoder(handle_unknown=&#39;use_encoded_value&#39;, unknown_value = -1) for c in cat : X_train[c] = Or.fit_transform(np.array(X_train[c]).reshape(-1,1).astype(str)) #fit on training set X_val[c] = Or.transform(np.array(X_val[c]).reshape(-1,1).astype(str)) test_x[c] = Or.transform(np.array(test_x[c]).reshape(-1,1).astype(str)) # Cheking categorical features encoding X_train[cat].head(3) class_of_worker detailed_industry_recode detailed_occupation_recode 88634 4.0 31.0 14.0 148296 8.0 37.0 12.0 163953 3.0 0.0 0.0 education enroll_in_edu_inst_last_wk marital_stat 88634 9.0 2.0 2.0 148296 12.0 2.0 2.0 163953 10.0 2.0 4.0 major_industry_code major_occupation_code race hispanic_origin 88634 2.0 0.0 4.0 0.0 148296 12.0 2.0 4.0 0.0 163953 14.0 6.0 4.0 0.0 ... migration_prev_res_in_sunbelt family_members_under_18 88634 ... 0.0 4.0 148296 ... 2.0 4.0 163953 ... 0.0 0.0 country_of_birth_father country_of_birth_mother 88634 40.0 40.0 148296 40.0 40.0 163953 40.0 40.0 country_of_birth_self citizenship own_business_or_self_employed 88634 40.0 4.0 2.0 148296 40.0 4.0 0.0 163953 40.0 4.0 0.0 fill_inc_questionnaire_for_veteran&#39;s_admin veterans_benefits year 88634 1.0 2.0 0.0 148296 1.0 2.0 0.0 163953 1.0 0.0 0.0 [3 rows x 33 columns] # Cheking target feature encoding set(y_train) {0, 1} . Feature Scaling : | . We saw previously that out numerical inputs have different scales like the weeks_worked_in_year and capital_gains. We will be using StandardScaler since standardization is much less affected by outliers. . from sklearn.preprocessing import StandardScaler scaler = StandardScaler() for c in cont: X_train[c] = scaler.fit_transform(np.array(X_train[c]).reshape(-1,1)) # fir on the train set X_val[c] = scaler.transform(np.array(X_val[c]).reshape(-1,1)) test_x[c] = scaler.transform(np.array(test_x[c]).reshape(-1,1)) #checking the standardization X_train[cont].head(3) age wage_per_hour capital_gains capital_losses 88634 0.334269 -0.201648 -0.092139 -0.137611 148296 0.468715 -0.201648 -0.092139 -0.137611 163953 -1.368713 -0.201648 -0.092139 -0.137611 dividends_from_stocks num_persons_worked_for_employer 88634 -0.069026 1.705234 148296 -0.098703 -0.405865 163953 -0.098703 -0.828085 weeks_worked_in_year 88634 1.177773 148296 1.177773 163953 -0.950488 . So far, we have handled the categorical columns and the numerical columns : . # Checking the training set with pd.option_context(&#39;display.max_rows&#39;, None, &#39;display.max_columns&#39;, None): display(X_train.head(3)) X_train.shape, X_val.shape, test_x.shape age class_of_worker detailed_industry_recode 88634 0.334269 4.0 31.0 148296 0.468715 8.0 37.0 163953 -1.368713 3.0 0.0 detailed_occupation_recode education wage_per_hour 88634 14.0 9.0 -0.201648 148296 12.0 12.0 -0.201648 163953 0.0 10.0 -0.201648 enroll_in_edu_inst_last_wk marital_stat major_industry_code 88634 2.0 2.0 2.0 148296 2.0 2.0 12.0 163953 2.0 4.0 14.0 major_occupation_code race hispanic_origin sex 88634 0.0 4.0 0.0 1.0 148296 2.0 4.0 0.0 0.0 163953 6.0 4.0 0.0 1.0 member_of_a_labor_union reason_for_unemployment 88634 1.0 3.0 148296 1.0 3.0 163953 1.0 3.0 full_or_part_time_employment_stat capital_gains capital_losses 88634 1.0 -0.092139 -0.137611 148296 0.0 -0.092139 -0.137611 163953 0.0 -0.092139 -0.137611 dividends_from_stocks tax_filer_stat region_of_previous_residence 88634 -0.069026 2.0 3.0 148296 -0.098703 2.0 3.0 163953 -0.098703 4.0 3.0 state_of_previous_residence detailed_household_and_family_stat 88634 36.0 37.0 148296 36.0 37.0 163953 36.0 8.0 detailed_household_summary_in_household migration_code-change_in_msa 88634 7.0 0.0 148296 7.0 7.0 163953 2.0 0.0 migration_code-change_in_reg migration_code-move_within_reg 88634 0.0 0.0 148296 6.0 7.0 163953 0.0 0.0 live_in_this_house_1_year_ago migration_prev_res_in_sunbelt 88634 1.0 0.0 148296 2.0 2.0 163953 1.0 0.0 num_persons_worked_for_employer family_members_under_18 88634 1.705234 4.0 148296 -0.405865 4.0 163953 -0.828085 0.0 country_of_birth_father country_of_birth_mother 88634 40.0 40.0 148296 40.0 40.0 163953 40.0 40.0 country_of_birth_self citizenship own_business_or_self_employed 88634 40.0 4.0 2.0 148296 40.0 4.0 0.0 163953 40.0 4.0 0.0 fill_inc_questionnaire_for_veteran&#39;s_admin veterans_benefits 88634 1.0 2.0 148296 1.0 2.0 163953 1.0 0.0 weeks_worked_in_year year 88634 1.177773 0.0 148296 1.177773 0.0 163953 -0.950488 0.0 ((139666, 40), (59857, 40), (99762, 40)) . Data modeling . Selecting a Performance Measure : | . Accuracy is the simplest way to measure the effectiveness of a classification task, and it’s the percentage of correct predictions over all predictions. In other words, in a binary classification task, you can calculate this by adding the number of True Positives (TPs) and True Negatives (TNs) and dividing them by a tally of all predictions made. As with regression metrics, you can measure accuracy for both train and test to gauge overfitting. . But, we can get an accuracy of 94%, which sounds pretty good, but it turns out we are always predicting -50k! In other words, even if we get high accuracy, it is meaningless unless we are predicting accurately for the least represented class, +50k. . For this reasing, we will be using F1-score. The F1-score is also called the harmonic average of precision and recall because it’s calculated like this: 2TP / 2TP + FP + FN. Since it includes both precision and recall metrics, which pertain to the proportion of true positives, it’s a good metric choice to use when the dataset is imbalanced, and we don’t prefer either precision or recall. . Base model : | . Let’s start with Decision tree ensembles. . A decision tree asks a series of binary (that is, yes or no) questions about the data. After each question the data at that part of the tree is split between a “yes” and a “no” branch. After one or more questions, either a prediction can be made on the basis of all previous answers or another question is required. . We illustarte a tree classification using 4 leaf nodes. . from sklearn.tree import DecisionTreeClassifier, plot_tree m = DecisionTreeClassifier(max_leaf_nodes=4, random_state=14) # to plot the tree classification m.fit(X_train, y_train) DecisionTreeClassifier(max_leaf_nodes=4, random_state=14) # to get the class output m.classes_ array([0, 1]) !pip install pydotplus Requirement already satisfied: pydotplus in /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages (2.0.2) Requirement already satisfied: pyparsing&gt;=2.0.1 in /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages (from pydotplus) (3.0.4) !pip install graphviz Requirement already satisfied: graphviz in /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages (0.16) from sklearn.tree import export_graphviz from io import StringIO from IPython.display import Image import pydotplus feature_cols = X_train.columns dot_data = StringIO() export_graphviz(m, out_file=dot_data, filled=True, rounded=True, special_characters=True,feature_names = feature_cols, class_names=[&#39;0&#39;,&#39;1&#39;]) graph = pydotplus.graph_from_dot_data(dot_data.getvalue()) Image(graph.create_png()) . . The top node represents the initial model before any splits have been done, when all the data is in the initial income levels. This is the simplest possible model. It is the result of asking zero questions and will always predict the more represented class which is -50k. We use the Gini method to create split points. The strategy is to select each pair of adjacent values as a possible split-point and the point with smaller gini index chosen as the splitting point. In our case, the capital gains at 1.47 was choosen first. . Moving down and to the left, this node shows us that there were 130,999 records for income level of -50k where capital gains was less than 1.47. The class predicted is -50k in this case. Moving down and to the right from the initial model takes us to the records where capital gains was greater than 1.47. The class predicted is +50k in this case where 1370 records have an income of +50k and capital gains &gt;0.4 . The bottom row contains our leaf nodes: the nodes with no answers coming out of them, because there are no more questions to be answered. . Returning back to the top node after the first decision point, we can see that a second binary decision split has been made, based on asking whether weeks_worked_per_year is less than or equal to 0.9. For the group where this is true, the class predicted is -50k with a gini of 0.019 and there are 85,411 records. For the records where this decision is false, the class predicted is -50k with a gini of 0.019, and there are 52,361 records. So again, we can see that the decision tree algorithm has successfully split out more records into two more groups which differ in gini value significantly. . Now, let’s run our base model : . m = DecisionTreeClassifier(random_state=14) m.fit(X_train, y_train) DecisionTreeClassifier(random_state=14) . We evaluate the model on our validation set using accuracy, recall and f1 score : . from sklearn.metrics import accuracy_score, f1_score, recall_score # on the train set accuracy_score(y_train,m.predict(X_train)) , recall_score(y_train,m.predict(X_train)), f1_score(y_train,m.predict(X_train), average=&#39;binary&#39;, pos_label=1) (0.9996348431257428, 0.9943463712934117, 0.997049806212761) # on the valid set accuracy_score(y_val,m.predict(X_val)) , recall_score(y_val,m.predict(X_val)), f1_score(y_val,m.predict(X_val), average=&#39;binary&#39;, pos_label=1) (0.9307683311893346, 0.4888290713324361, 0.4670781893004115) . It’s seems that we are doing badly on the validation set. Let’s see houw many leaf nodes we got : . m.get_n_leaves(), len(X_train) (8005, 139666) . Sklearn’s default settings allow it to continue splitting nodes until there is only one item in each leaf node. Let’s change the stopping rule to tell sklearn to ensure every leaf node contains at least 25 records: . m = DecisionTreeClassifier(min_samples_leaf=25,random_state=14) m.fit(X_train, y_train) # on the train set accuracy_score(y_train,m.predict(X_train)) , recall_score(y_train,m.predict(X_train)), f1_score(y_train,m.predict(X_train), average=&#39;binary&#39;, pos_label=1) (0.9571048071828505, 0.46198223145263645, 0.5720408600614331) # on the valid set accuracy_score(y_val,m.predict(X_val)) , recall_score(y_val,m.predict(X_val)), f1_score(y_val,m.predict(X_val), average=&#39;binary&#39;, pos_label=1) (0.9504986885410228, 0.42449528936742936, 0.515612228216446) . That looks much better. Let’s check the number of leaves again: . m.get_n_leaves(), len(X_train) (1533, 139666) . We got less leaf nodes than before. So, the more we increase the number of leaf nodes, the more is the possibility of overfitting. . Building a decision tree is a good way to create a model of our data. It is very flexible, since it can clearly handle nonlinear relationships and interactions between variables. But we can see there is a fundamental compromise between how well it generalizes (which we can achieve by creating small trees) and how accurate it is on the training set (which we can achieve by using large trees). . So how do we get the best of both worlds? . Ensembling : | . An an example of an Ensemble method is Random Forest : we can train a group of Decision Tree classifiers, each on a different random subset of the training set. The process of subseting the data is called bagging done with max_samples hyperparameter ( we set it at 100.00 samples) and the ramdom selection process this called bootsraping done by setting bootstrap = True. . With bagging, some instances may be sampled several times for any given predictor, while others may not be sampled at all. The remaining sampled are called out-of-bag (oob) instances used as validation set in the training process and done by setting oob_score=True. . We train a Random Forest classifier with 50 trees (each limited to minimum 5 samples per leaf). and instead of searching for the very best feature when splitting a node, we searches for the best feature among a random subset of 50% of our initial features. . from sklearn.ensemble import RandomForestClassifier rf = RandomForestClassifier(n_estimators = 50, max_samples=100_000, max_features=0.5, min_samples_leaf= 5, bootstrap= True,oob_score = True,random_state=14) %%time rf.fit(X_train,y_train) /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages/sklearn/base.py:446: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names &quot;X does not have valid feature names, but&quot; CPU times: user 29.1 s, sys: 643 ms, total: 29.7 s Wall time: 32.8 s RandomForestClassifier(max_features=0.5, max_samples=100000, min_samples_leaf=5, n_estimators=50, oob_score=True, random_state=14) # on the train set accuracy_score(y_train,rf.predict(X_train)) , recall_score(y_train,rf.predict(X_train)), f1_score(y_train,rf.predict(X_train), average=&#39;binary&#39;, pos_label=1) (0.9670571219910358, 0.5334025614399446, 0.6677258611973712) # on the valid set accuracy_score(y_val,rf.predict(X_val)) , recall_score(y_val,rf.predict(X_val)), f1_score(y_val,rf.predict(X_val), average=&#39;binary&#39;, pos_label=1) (0.9546418965200394, 0.41668909825033645, 0.5327826535880227) . Looking at what happens to the oob error rate as we add more and more trees, we you can see that the improvement levels off quite a bit after around 40 trees: . scores =[] for k in range(1, 50): rfc = RandomForestClassifier(n_estimators = k, max_samples=100_000, max_features=0.5, min_samples_leaf= 5, bootstrap= True,oob_score = True,random_state=14) rfc.fit(X_train, y_train) #y_pred = rfc.predict(X_val) #scores.append(accuracy_score(y_test, y_pred)) oob_score_ oob_error = 1 - rfc.oob_score_ scores.append(oob_error) import matplotlib.pyplot as plt %matplotlib inline # plot the relationship between K and testing accuracy # plt.plot(x_axis, y_axis) plt.plot(range(1, 50), scores) plt.xlabel(&#39;Value of n_estimators for Random Forest Classifier&#39;) #plt.ylabel(&#39;Testing Accuracy&#39;) plt.ylabel(&#39;OOB error rate&#39;) /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages/sklearn/base.py:446: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names &quot;X does not have valid feature names, but&quot; /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/_forest.py:554: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates. UserWarning, /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages/sklearn/base.py:446: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names &quot;X does not have valid feature names, but&quot; /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/_forest.py:554: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates. UserWarning, /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages/sklearn/base.py:446: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names &quot;X does not have valid feature names, but&quot; /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/_forest.py:554: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates. UserWarning, /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages/sklearn/base.py:446: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names &quot;X does not have valid feature names, but&quot; /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/_forest.py:554: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates. UserWarning, /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages/sklearn/base.py:446: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names &quot;X does not have valid feature names, but&quot; /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/_forest.py:554: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates. UserWarning, /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages/sklearn/base.py:446: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names &quot;X does not have valid feature names, but&quot; /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/_forest.py:554: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates. UserWarning, /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages/sklearn/base.py:446: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names &quot;X does not have valid feature names, but&quot; /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/_forest.py:554: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates. UserWarning, /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages/sklearn/base.py:446: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names &quot;X does not have valid feature names, but&quot; /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/_forest.py:554: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates. UserWarning, /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages/sklearn/base.py:446: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names &quot;X does not have valid feature names, but&quot; /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/_forest.py:554: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates. UserWarning, /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages/sklearn/base.py:446: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names &quot;X does not have valid feature names, but&quot; /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/_forest.py:554: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates. UserWarning, /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages/sklearn/base.py:446: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names &quot;X does not have valid feature names, but&quot; /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/_forest.py:554: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates. UserWarning, /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages/sklearn/base.py:446: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names &quot;X does not have valid feature names, but&quot; /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/_forest.py:554: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates. UserWarning, /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages/sklearn/base.py:446: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names &quot;X does not have valid feature names, but&quot; /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/_forest.py:554: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates. UserWarning, /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages/sklearn/base.py:446: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names &quot;X does not have valid feature names, but&quot; /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/_forest.py:554: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates. UserWarning, /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages/sklearn/base.py:446: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names &quot;X does not have valid feature names, but&quot; /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/_forest.py:554: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates. UserWarning, /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages/sklearn/base.py:446: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names &quot;X does not have valid feature names, but&quot; /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/_forest.py:554: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates. UserWarning, /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages/sklearn/base.py:446: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names &quot;X does not have valid feature names, but&quot; /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/_forest.py:554: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates. UserWarning, /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages/sklearn/base.py:446: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names &quot;X does not have valid feature names, but&quot; /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/_forest.py:554: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates. UserWarning, /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages/sklearn/base.py:446: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names &quot;X does not have valid feature names, but&quot; /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/_forest.py:554: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates. UserWarning, /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages/sklearn/base.py:446: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names &quot;X does not have valid feature names, but&quot; /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/_forest.py:554: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates. UserWarning, /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages/sklearn/base.py:446: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names &quot;X does not have valid feature names, but&quot; /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/_forest.py:554: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates. UserWarning, /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages/sklearn/base.py:446: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names &quot;X does not have valid feature names, but&quot; /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages/sklearn/base.py:446: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names &quot;X does not have valid feature names, but&quot; /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages/sklearn/base.py:446: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names &quot;X does not have valid feature names, but&quot; /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages/sklearn/base.py:446: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names &quot;X does not have valid feature names, but&quot; /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages/sklearn/base.py:446: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names &quot;X does not have valid feature names, but&quot; /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages/sklearn/base.py:446: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names &quot;X does not have valid feature names, but&quot; /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages/sklearn/base.py:446: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names &quot;X does not have valid feature names, but&quot; /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages/sklearn/base.py:446: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names &quot;X does not have valid feature names, but&quot; /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages/sklearn/base.py:446: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names &quot;X does not have valid feature names, but&quot; /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages/sklearn/base.py:446: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names &quot;X does not have valid feature names, but&quot; /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages/sklearn/base.py:446: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names &quot;X does not have valid feature names, but&quot; /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages/sklearn/base.py:446: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names &quot;X does not have valid feature names, but&quot; /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages/sklearn/base.py:446: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names &quot;X does not have valid feature names, but&quot; /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages/sklearn/base.py:446: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names &quot;X does not have valid feature names, but&quot; /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages/sklearn/base.py:446: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names &quot;X does not have valid feature names, but&quot; /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages/sklearn/base.py:446: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names &quot;X does not have valid feature names, but&quot; /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages/sklearn/base.py:446: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names &quot;X does not have valid feature names, but&quot; /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages/sklearn/base.py:446: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names &quot;X does not have valid feature names, but&quot; /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages/sklearn/base.py:446: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names &quot;X does not have valid feature names, but&quot; /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages/sklearn/base.py:446: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names &quot;X does not have valid feature names, but&quot; /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages/sklearn/base.py:446: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names &quot;X does not have valid feature names, but&quot; /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages/sklearn/base.py:446: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names &quot;X does not have valid feature names, but&quot; /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages/sklearn/base.py:446: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names &quot;X does not have valid feature names, but&quot; /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages/sklearn/base.py:446: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names &quot;X does not have valid feature names, but&quot; /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages/sklearn/base.py:446: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names &quot;X does not have valid feature names, but&quot; /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages/sklearn/base.py:446: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names &quot;X does not have valid feature names, but&quot; /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages/sklearn/base.py:446: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names &quot;X does not have valid feature names, but&quot; /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages/sklearn/base.py:446: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names &quot;X does not have valid feature names, but&quot; Text(0, 0.5, &#39;OOB error rate&#39;) . . Let’s try to improve our model : . We may ask which columns are the strongest predictors, which can we ignore? . It’s not normally enough just to know that a model can make accurate predictions—we also want to know how it’s making predictions. Feature importance gives us insight into this. We can get these directly from sklearn’s random forest by looking in the feature_importances_ attribute. Here’s a simple function we can use to pop them into a DataFrame and sort them: . def rf_feat_importance(m, df): return pd.DataFrame({&#39;cols&#39;:df.columns, &#39;imp&#39;:m.feature_importances_} ).sort_values(&#39;imp&#39;, ascending=False) fi = rf_feat_importance(rf, X_train) fi[:14] cols imp 16 capital_gains 0.161368 18 dividends_from_stocks 0.132382 0 age 0.094106 3 detailed_occupation_recode 0.081452 38 weeks_worked_in_year 0.076202 2 detailed_industry_recode 0.056239 12 sex 0.054394 4 education 0.049652 17 capital_losses 0.047552 29 num_persons_worked_for_employer 0.040222 9 major_occupation_code 0.039596 8 major_industry_code 0.031488 1 class_of_worker 0.014878 19 tax_filer_stat 0.013542 . The feature importances for our model show that the first few most important columns have much higher importance scores than the rest, with (not surprisingly) capital_gains and dividends_from_stocks being at the top of the list. . A plot of the feature importances shows the relative importances more clearly: . def plot_fi(fi): return fi.plot(&#39;cols&#39;, &#39;imp&#39;, &#39;barh&#39;, figsize=(12,7), legend=False) plot_fi(fi[:30]); . . The way these importances are calculated is quite simple yet elegant. The feature importance algorithm loops through each tree, and then recursively explores each branch. At each branch, it looks to see what feature was used for that split, and how much the model improves as a result of that split. The improvement (weighted by the number of rows in that group) is added to the importance score for that feature. This is summed across all branches of all trees, and finally the scores are normalized such that they add to 1. . It seems likely that we could use just a subset of the columns by removing the variables of low importance and still get good results. Let’s try just keeping those with a feature importance greater than 0.005: . to_keep = fi[fi.imp&gt;0.005].cols len(to_keep) 25 . We can retrain our model using just this subset of the columns: . X_train_imp = X_train[to_keep] X_val_imp = X_val[to_keep] m = RandomForestClassifier(n_estimators = 50, max_samples=100_000, max_features=0.5, min_samples_leaf= 5, bootstrap= True,oob_score = True,random_state=14) m.fit(X_train_imp,y_train) /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages/sklearn/base.py:446: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names &quot;X does not have valid feature names, but&quot; RandomForestClassifier(max_features=0.5, max_samples=100000, min_samples_leaf=5, n_estimators=50, oob_score=True, random_state=14) # on the train set accuracy_score(y_train,m.predict(X_train_imp)) , recall_score(y_train,m.predict(X_train_imp)), f1_score(y_train,m.predict(X_train_imp), average=&#39;binary&#39;, pos_label=1) (0.9670857617458795, 0.5334025614399446, 0.6679188037275157) # on the valid set accuracy_score(y_val,m.predict(X_val_imp)) , recall_score(y_val,m.predict(X_val_imp)), f1_score(y_val,m.predict(X_val_imp), average=&#39;binary&#39;, pos_label=1) (0.9543077668443123, 0.4142664872139973, 0.5295028384655084) . Our accuracy is about the same, but we have far fewer columns to study: . len(X_train.columns), len(X_train_imp.columns) (40, 25) . We’ve found that generally the first step to improving a model is simplifying it—48 columns was too many for us to study them all in depth! Furthermore, in practice often a simpler, more interpretable model is easier to roll out and maintain. . This also makes our feature importance plot easier to interpret. Let’s look at it again: . plot_fi(rf_feat_importance(m, X_train_imp)); . . Let’s see if we have redundent feature in our model by determining their similarities : . Determining Similarity: The most similar pairs are found by calculating the rank correlation, which means that all the values are replaced with their rank (i.e., first, second, third, etc. within the column), and then the correlation is calculated. . import scipy from scipy.cluster import hierarchy as hc def cluster_columns(df, figsize=(10,6), font_size=12): corr = np.round(scipy.stats.spearmanr(df).correlation, 4) corr_condensed = hc.distance.squareform(1-corr) z = hc.linkage(corr_condensed, method=&#39;average&#39;) fig = plt.figure(figsize=figsize) hc.dendrogram(z, labels=df.columns, orientation=&#39;left&#39;, leaf_font_size=font_size) plt.show() cluster_columns(X_train_imp) . . Looking good! This is really not much worse than the model with all the fields. Let’s create DataFrames without these columns, and save them: . X_train_final = X_train_imp # train X_val_final = X_val_imp # valid test_x_final = test_x[to_keep] # test set X_train_final.shape , X_val_final.shape, test_x_final.shape ((139666, 25), (59857, 25), (99762, 25)) . Model Assesment : . We have seen the DecisionTreeClassifier as our basemodel, then we tried RandomForestClassifier and finaly we tried to optimize so we can have less features for better interpretation. . Here is the model metrics on our validation set : . models_metrics = {&#39;DTC&#39;: [0.95, 0.42, 0.51], &#39;RF&#39;: [0.96, 0.42, 0.53], &#39;RF_less_feat&#39;: [0.95, 0.41, 0.53] } df = pd.DataFrame(data = models_metrics) df.rename(index={0:&#39;Accuracy&#39;,1:&#39;Recall&#39;, 2: &#39;F1 score&#39;}, inplace=True) ax = df.plot(kind=&#39;bar&#39;, figsize = (10,5), color = [&#39;gold&#39;, &#39;lightgreen&#39;,&#39;lightcoral&#39;], rot = 0, title =&#39;Models performance&#39;, edgecolor = &#39;grey&#39;, alpha = 0.5) for p in ax.patches: ax.annotate(str(p.get_height()), (p.get_x() * 1.01, p.get_height() * 1.0005)) plt.show() . . Based on F1 score, we select the RandomForestClassifier with 25 features as our best model. . Let’s see the experiment results* of this model : . The precision_recall_curve and roc_curve are useful tools to visualize the sensitivity-specificty tradeoff in the classifier. They help inform a data scientist where to set the decision threshold of the model to maximize either sensitivity or specificity. This is called the operating point of the model. . from sklearn.metrics import roc_curve, precision_recall_curve, auc, make_scorer, recall_score, accuracy_score, precision_score, confusion_matrix # We create an array of the class probabilites called y_scores y_scores = m.predict_proba(X_val_imp)[:, 1] # we enerate the precision-recall curve for the classifier: p, r, thresholds = precision_recall_curve(y_val, y_scores) # We calculate the F1 scores f1_scores = 2*r*p/(r+p) . Let’s plot the decision chart of our model : . def plot_precision_recall_vs_threshold(precisions, recalls, thresholds): plt.figure(figsize=(8, 8)) plt.title(&quot;Precision, Recall Scores and F1 scores as a function of the decision threshold&quot;) plt.plot(thresholds, precisions[:-1], &quot;b--&quot;, label=&quot;Precision&quot;) plt.plot(thresholds, recalls[:-1], &quot;g-&quot;, label=&quot;Recall&quot;) plt.plot(thresholds, f1_scores[:-1], &quot;r-&quot;, label=&quot;F1-score&quot;) plt.ylabel(&quot;Score&quot;) plt.xlabel(&quot;Decision Threshold&quot;) plt.legend(loc=&#39;best&#39;) plot_precision_recall_vs_threshold(p, r, thresholds) . . We can see that the the optimal threshold to achieve the highest F1 score is set at 0.30 with 59% F1-score. . print(&#39;Best threshold: &#39;, thresholds[np.argmax(f1_scores)]) print(&#39;Best F1-Score: &#39;, np.max(f1_scores)) Best threshold: 0.30834595959595956 Best F1-Score: 0.5950888192267503 . Let’s creat an animated confusion matrix where the users get to choose the threesholds and we dislpay the confusion matrix and recall vs precision curve : . pip install ipywidgets Requirement already satisfied: ipywidgets in /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages (7.6.5) Requirement already satisfied: ipython-genutils~=0.2.0 in /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages (from ipywidgets) (0.2.0) Requirement already satisfied: ipykernel&gt;=4.5.1 in /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages (from ipywidgets) (5.3.4) Requirement already satisfied: ipython&gt;=4.0.0 in /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages (from ipywidgets) (7.22.0) Requirement already satisfied: jupyterlab-widgets&gt;=1.0.0 in /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages (from ipywidgets) (1.0.0) Requirement already satisfied: nbformat&gt;=4.2.0 in /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages (from ipywidgets) (5.1.3) Requirement already satisfied: traitlets&gt;=4.3.1 in /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages (from ipywidgets) (5.1.1) Requirement already satisfied: widgetsnbextension~=3.5.0 in /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages (from ipywidgets) (3.5.1) Requirement already satisfied: tornado&gt;=4.2 in /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages (from ipykernel&gt;=4.5.1-&gt;ipywidgets) (6.1) Requirement already satisfied: jupyter-client in /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages (from ipykernel&gt;=4.5.1-&gt;ipywidgets) (7.0.1) Requirement already satisfied: appnope in /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages (from ipykernel&gt;=4.5.1-&gt;ipywidgets) (0.1.2) Requirement already satisfied: decorator in /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages (from ipython&gt;=4.0.0-&gt;ipywidgets) (5.1.0) Requirement already satisfied: pygments in /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages (from ipython&gt;=4.0.0-&gt;ipywidgets) (2.10.0) Requirement already satisfied: backcall in /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages (from ipython&gt;=4.0.0-&gt;ipywidgets) (0.2.0) Requirement already satisfied: pexpect&gt;4.3 in /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages (from ipython&gt;=4.0.0-&gt;ipywidgets) (4.8.0) Requirement already satisfied: setuptools&gt;=18.5 in /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages (from ipython&gt;=4.0.0-&gt;ipywidgets) (58.0.4) Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,&lt;3.1.0,&gt;=2.0.0 in /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages (from ipython&gt;=4.0.0-&gt;ipywidgets) (3.0.20) Requirement already satisfied: jedi&gt;=0.16 in /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages (from ipython&gt;=4.0.0-&gt;ipywidgets) (0.18.1) Requirement already satisfied: pickleshare in /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages (from ipython&gt;=4.0.0-&gt;ipywidgets) (0.7.5) Requirement already satisfied: parso&lt;0.9.0,&gt;=0.8.0 in /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages (from jedi&gt;=0.16-&gt;ipython&gt;=4.0.0-&gt;ipywidgets) (0.8.3) Requirement already satisfied: jupyter-core in /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages (from nbformat&gt;=4.2.0-&gt;ipywidgets) (4.9.1) Requirement already satisfied: jsonschema!=2.5.0,&gt;=2.4 in /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages (from nbformat&gt;=4.2.0-&gt;ipywidgets) (3.2.0) Requirement already satisfied: pyrsistent&gt;=0.14.0 in /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages (from jsonschema!=2.5.0,&gt;=2.4-&gt;nbformat&gt;=4.2.0-&gt;ipywidgets) (0.18.0) Requirement already satisfied: importlib-metadata in /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages (from jsonschema!=2.5.0,&gt;=2.4-&gt;nbformat&gt;=4.2.0-&gt;ipywidgets) (4.8.1) Requirement already satisfied: attrs&gt;=17.4.0 in /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages (from jsonschema!=2.5.0,&gt;=2.4-&gt;nbformat&gt;=4.2.0-&gt;ipywidgets) (21.2.0) Requirement already satisfied: six&gt;=1.11.0 in /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages (from jsonschema!=2.5.0,&gt;=2.4-&gt;nbformat&gt;=4.2.0-&gt;ipywidgets) (1.16.0) Requirement already satisfied: ptyprocess&gt;=0.5 in /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages (from pexpect&gt;4.3-&gt;ipython&gt;=4.0.0-&gt;ipywidgets) (0.7.0) Requirement already satisfied: wcwidth in /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,&lt;3.1.0,&gt;=2.0.0-&gt;ipython&gt;=4.0.0-&gt;ipywidgets) (0.2.5) Requirement already satisfied: notebook&gt;=4.4.1 in /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages (from widgetsnbextension~=3.5.0-&gt;ipywidgets) (6.4.5) Requirement already satisfied: prometheus-client in /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages (from notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.5.0-&gt;ipywidgets) (0.11.0) Requirement already satisfied: pyzmq&gt;=17 in /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages (from notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.5.0-&gt;ipywidgets) (22.2.1) Requirement already satisfied: jinja2 in /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages (from notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.5.0-&gt;ipywidgets) (2.11.3) Requirement already satisfied: nbconvert in /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages (from notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.5.0-&gt;ipywidgets) (6.1.0) Requirement already satisfied: terminado&gt;=0.8.3 in /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages (from notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.5.0-&gt;ipywidgets) (0.9.4) Requirement already satisfied: argon2-cffi in /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages (from notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.5.0-&gt;ipywidgets) (20.1.0) Requirement already satisfied: Send2Trash&gt;=1.5.0 in /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages (from notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.5.0-&gt;ipywidgets) (1.8.0) Requirement already satisfied: python-dateutil&gt;=2.1 in /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages (from jupyter-client-&gt;ipykernel&gt;=4.5.1-&gt;ipywidgets) (2.8.2) Requirement already satisfied: nest-asyncio&gt;=1.5 in /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages (from jupyter-client-&gt;ipykernel&gt;=4.5.1-&gt;ipywidgets) (1.5.1) Requirement already satisfied: entrypoints in /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages (from jupyter-client-&gt;ipykernel&gt;=4.5.1-&gt;ipywidgets) (0.3) Requirement already satisfied: cffi&gt;=1.0.0 in /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages (from argon2-cffi-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.5.0-&gt;ipywidgets) (1.15.0) Requirement already satisfied: pycparser in /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages (from cffi&gt;=1.0.0-&gt;argon2-cffi-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.5.0-&gt;ipywidgets) (2.21) Requirement already satisfied: typing-extensions&gt;=3.6.4 in /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages (from importlib-metadata-&gt;jsonschema!=2.5.0,&gt;=2.4-&gt;nbformat&gt;=4.2.0-&gt;ipywidgets) (3.10.0.2) Requirement already satisfied: zipp&gt;=0.5 in /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages (from importlib-metadata-&gt;jsonschema!=2.5.0,&gt;=2.4-&gt;nbformat&gt;=4.2.0-&gt;ipywidgets) (3.6.0) Requirement already satisfied: MarkupSafe&gt;=0.23 in /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages (from jinja2-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.5.0-&gt;ipywidgets) (2.0.1) Requirement already satisfied: jupyterlab-pygments in /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages (from nbconvert-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.5.0-&gt;ipywidgets) (0.1.2) Requirement already satisfied: mistune&lt;2,&gt;=0.8.1 in /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages (from nbconvert-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.5.0-&gt;ipywidgets) (0.8.4) Requirement already satisfied: testpath in /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages (from nbconvert-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.5.0-&gt;ipywidgets) (0.5.0) Requirement already satisfied: pandocfilters&gt;=1.4.1 in /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages (from nbconvert-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.5.0-&gt;ipywidgets) (1.4.3) Requirement already satisfied: defusedxml in /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages (from nbconvert-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.5.0-&gt;ipywidgets) (0.7.1) Requirement already satisfied: bleach in /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages (from nbconvert-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.5.0-&gt;ipywidgets) (4.0.0) Requirement already satisfied: nbclient&lt;0.6.0,&gt;=0.5.0 in /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages (from nbconvert-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.5.0-&gt;ipywidgets) (0.5.3) Requirement already satisfied: async-generator in /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages (from nbclient&lt;0.6.0,&gt;=0.5.0-&gt;nbconvert-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.5.0-&gt;ipywidgets) (1.10) Requirement already satisfied: webencodings in /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages (from bleach-&gt;nbconvert-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.5.0-&gt;ipywidgets) (0.5.1) Requirement already satisfied: packaging in /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages (from bleach-&gt;nbconvert-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.5.0-&gt;ipywidgets) (21.0) Requirement already satisfied: pyparsing&gt;=2.0.2 in /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages (from packaging-&gt;bleach-&gt;nbconvert-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.5.0-&gt;ipywidgets) (3.0.4) Note: you may need to restart the kernel to use updated packages. import ipywidgets as widgets import itertools def plot_confusion_matrix(cm, classes, normalize = False, title = &#39;Confusion matrix&quot;&#39;, cmap = plt.cm.Blues) : plt.imshow(cm, interpolation = &#39;nearest&#39;, cmap = cmap) plt.title(title) plt.colorbar() tick_marks = np.arange(len(classes)) plt.xticks(tick_marks, classes, rotation = 0) plt.yticks(tick_marks, classes) thresh = cm.max() / 2. for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])) : plt.text(j, i, cm[i, j], horizontalalignment = &#39;center&#39;, color = &#39;white&#39; if cm[i, j] &gt; thresh else &#39;black&#39;) plt.tight_layout() plt.ylabel(&#39;True label&#39;) plt.xlabel(&#39;Predicted label&#39;) def adjusted_classes(y_scores, t): &quot;&quot;&quot; This function adjusts class predictions based on the prediction threshold (t). Will only work for binary classification problems. &quot;&quot;&quot; return [1 if y &gt;= t else 0 for y in y_scores] def precision_recall_threshold(p, r, thresholds, t=0.5): &quot;&quot;&quot; plots the precision recall curve and shows the current value for each by identifying the classifier&#39;s threshold (t). &quot;&quot;&quot; # generate new class predictions based on the adjusted_classes # function above and view the resulting confusion matrix. y_pred_adj = adjusted_classes(y_scores, t) cm = confusion_matrix(y_val, y_pred_adj) class_names = [0,1] plt.figure() plot_confusion_matrix(cm, classes=class_names, title=&#39;RF Confusion matrix&#39;) plt.figure(figsize=(8,8)) plt.title(&quot;Precision and Recall curve ^ = current threshold&quot;) plt.step(r, p, color=&#39;b&#39;, alpha=0.2, where=&#39;post&#39;) plt.fill_between(r, p, step=&#39;post&#39;, alpha=0.2, color=&#39;b&#39;) plt.xlabel(&#39;Recall&#39;); plt.ylabel(&#39;Precision&#39;); # plot the current threshold on the line close_default_clf = np.argmin(np.abs(thresholds - t)) plt.plot(r[close_default_clf], p[close_default_clf], &#39;^&#39;, c=&#39;k&#39;, markersize=15) slider = widgets.IntSlider( min=0, max=10, step=1, description=&#39;Slider:&#39;, value=3 # The best threshhold for our model ) display(slider) {&quot;model_id&quot;:&quot;64c4bfbbeb2a4242855806666b132d49&quot;,&quot;version_major&quot;:2,&quot;version_minor&quot;:0} print(f&#39;For this threshold : {slider.value/10}, the confusion matrix is as follow :&#39;) precision_recall_threshold(p, r, thresholds, slider.value/10) For this threshold : 0.3, the confusion matrix is as follow : . . . def plot_roc_curve(fpr, tpr, label=None): plt.figure(figsize=(8,8)) plt.title(&#39;ROC Curve&#39;) plt.plot(fpr, tpr, linewidth=2, label=label) plt.plot([0, 1], [0, 1], &#39;k--&#39;) plt.axis([-0.005, 1, 0, 1.005]) plt.xticks(np.arange(0,1, 0.05), rotation=90) plt.xlabel(&quot;False Positive Rate&quot;) plt.ylabel(&quot;True Positive Rate (Recall)&quot;) plt.legend(loc=&#39;best&#39;) fpr, tpr, auc_thresholds = roc_curve(y_val, y_scores) print(f&#39;AUC : {auc(fpr, tpr)}&#39;) # AUC of ROC plot_roc_curve(fpr, tpr, &#39;recall_optimized&#39;) AUC : 0.9433941778952841 . . Now, let’s test this model on our test set : . accuracy_score(test_y,m.predict(test_x_final)) , recall_score(test_y,m.predict(test_x_final)) , f1_score(test_y,m.predict(test_x_final), average=&#39;binary&#39;, pos_label=1) (0.9546420480744171, 0.4183640478499838, 0.5335532419338213) . Results : Partial dependency and SHAP values . Let’s look at partial dependence plots. . Partial dependence plots try to answer the question: if a row varied on nothing other than the feature in question, how would it impact the dependent variable? . For instance, how does capital_gains and dividends_from_stocks impact probability of belonging to the +50k income levl, all other things being equal? . from sklearn.inspection import plot_partial_dependence fig,ax = plt.subplots(figsize=(20, 8)) plot_partial_dependence(m, X_val_final, [&#39;capital_gains&#39;,&#39;dividends_from_stocks&#39;], percentiles=(0,1), grid_resolution=15, ax=ax); /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function plot_partial_dependence is deprecated; Function `plot_partial_dependence` is deprecated in 1.0 and will be removed in 1.2. Use PartialDependenceDisplay.from_estimator instead warnings.warn(msg, category=FutureWarning) . . Looking first at the dividends_from_stocks plot, we can see a nearly linear relationship between capital dividends_from_stocks and the probabillity of income level. Same for capital_gains at 5 standad deviation from the mean after reaching a steady state above that. . !pip install shap Requirement already satisfied: shap in /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages (0.40.0) Requirement already satisfied: packaging&gt;20.9 in /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages (from shap) (21.0) Requirement already satisfied: cloudpickle in /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages (from shap) (2.0.0) Requirement already satisfied: tqdm&gt;4.25.0 in /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages (from shap) (4.62.3) Requirement already satisfied: pandas in /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages (from shap) (1.3.4) Requirement already satisfied: scipy in /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages (from shap) (1.7.1) Requirement already satisfied: slicer==0.0.7 in /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages (from shap) (0.0.7) Requirement already satisfied: numpy in /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages (from shap) (1.21.2) Requirement already satisfied: numba in /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages (from shap) (0.53.1) Requirement already satisfied: scikit-learn in /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages (from shap) (1.0.1) Requirement already satisfied: pyparsing&gt;=2.0.2 in /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages (from packaging&gt;20.9-&gt;shap) (3.0.4) Requirement already satisfied: llvmlite&lt;0.37,&gt;=0.36.0rc1 in /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages (from numba-&gt;shap) (0.36.0) Requirement already satisfied: setuptools in /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages (from numba-&gt;shap) (58.0.4) Requirement already satisfied: python-dateutil&gt;=2.7.3 in /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages (from pandas-&gt;shap) (2.8.2) Requirement already satisfied: pytz&gt;=2017.3 in /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages (from pandas-&gt;shap) (2021.3) Requirement already satisfied: six&gt;=1.5 in /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages (from python-dateutil&gt;=2.7.3-&gt;pandas-&gt;shap) (1.16.0) Requirement already satisfied: joblib&gt;=0.11 in /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages (from scikit-learn-&gt;shap) (1.1.0) Requirement already satisfied: threadpoolctl&gt;=2.0.0 in /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages (from scikit-learn-&gt;shap) (2.2.0) import shap row_to_show = 20 data_for_prediction = test_x_final.iloc[row_to_show] # use 1 row of data here. Could use multiple rows if desired # Create object that can calculate shap values explainer = shap.TreeExplainer(m) # Calculate Shap values shap_values = explainer.shap_values(data_for_prediction) shap.initjs() shap.force_plot(explainer.expected_value[1], shap_values[1], data_for_prediction) &lt;IPython.core.display.HTML object&gt; &lt;shap.plots._force.AdditiveForceVisualizer at 0x13c39bd50&gt; . The above explanation shows features each contributing to push the model output from the base value (the average model output over the training dataset we passed) to the model output. Features pushing the prediction higher are shown in red, those pushing the prediction lower are in blue . The base_value here is 0.062 while our predicted value is 0.0. | sex = 1 has the biggest impact on increasing the prediction, while | Weeks_worked_im_year (below the average) and Age (below the average) feature has the biggest effect in decreasing the prediction. | . explainer = shap.TreeExplainer(m) # calculate shap values. This is what we will plot. # Calculate shap_values for all of val_X rather than a single row, to have more data for plot. shap_values = explainer.shap_values(test_x_final.iloc[:1000,]) # Make plot. Index of [1] is explained in text below. shap.summary_plot(shap_values[1],test_x_final.iloc[:1000,]) . . For every dot: . Vertical location shows what feature it is depicting | Color shows whether that feature was high or low for that row of the dataset | Horizontal location shows whether the effect of that value caused a higher or lower prediction. | . For the age variable, the point in the upper left was depicts a person whose age level is less thereby reducing the prediction of income level +50k class by 0.2. . Conclusion : . In this work, we presented some techniques for dealing with a machine learning project : . We used Decision Tree ensembles : Random Forest are the easiest to train, because they are extremely resilient to hyperparameter choices and require very little preprocessing. They are very fast to train, and should not overfit if we have enough trees. . | we used the model for feature selection and partial dependence analysis and Shap values, to get a better understanding of our data. . | . For futur improvements : . We can try Gradient Boosting machines as in theory are just as fast to train as random forests, but in practice we will have to try lots of different hyperparameters. They can overfit, but they are often a little more accurate than random forests. . | We can try OneHotEncoder with PCA to deal with the multiple modalities on our categorical variables. . | We can creat new features to challenge the model performance. . | . .",
            "url": "https://younesszaim.github.io/myportfolio/2021/11/20/US-cencus-income-Ensembles,-Bagging-and-Shap-Values.html",
            "relUrl": "/2021/11/20/US-cencus-income-Ensembles,-Bagging-and-Shap-Values.html",
            "date": " • Nov 20, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Sickit Learn For Machine Learning",
            "content": "Sickit-Learn for Machine Learning . In this notebook we will work through an example project end to end using Sickit Learn library. . About . In this chapter we’ll use the California Housing Prices dataset from the StatLib repository This dataset is based on data from the 1990 California census. . This data includes metrics such as the population, median income, and median housing price for each block group in California. Block groups are the smallest geographical unit for which the US Census Bureau publishes sample data (a block group typical). . Your model should learn from this data and be able to predict the median housing price in any district, given all the other metrics. . Goal : Your boss answers that your model’s output (a prediction of a district’s median housing price) will be fed to another Machine Learning system , along with many other signals. This downstream system will determine whether it is worth investing in a given area or not. Getting this right is critical, as it directly affects revenue. . First, you need to frame the problem: is it supervised, unsupervised, or Reinforcement Learning? Is it a classification task, a regression task, or something else? Should you use batch learning or online learning techniques? . Let’s see: it is clearly a typical supervised learning task, since you are given labeled training examples. . It is also a typical regression task, since you are asked to predict a value. More specifically, this is a multiple regression problem, since the system will use multiple features to make a prediction. . It is also a univariate regression problem, since we are only trying to predict a single value for each district. If we were trying to predict multiple values per district, it would be a multivariate regression problem. . Finally, there is no continuous flow of data coming into the system, there is no particular need to adjust to changing data rapidly, and the data is small enough to fit in memory, so plain batch learning should do just fine. . Select a Performance Measure . Performance Measures for our univariate regression problem . Typical performance measure for regression problems : . Root Mean Square Error (RMSE): it gives an idea of how much error the system typically makes in its predictions, with a higher weight for large errors : Since the errors are squared before they are averaged, the RMSE gives a relatively high weight to large errors. This means the RMSE should be more useful when large errors are particularly undesirable. | . RMSE is sensitive to outliers : If we make a single very bad prediction, taking the square will make the error even worse and it may skew the metric towards overestimating the model’s badness. Actually, it’s hard to realize if our model is good or not by looking at the absolute values of MSE or MSE : We would probably want to measure how much our model is better than the constant baseline : A model should at least perform better than the RMSE score constant baseline. . RMSE has the benefit of penalizing large errors more so can be more appropriate in some cases, for example, if being off by 10 is more than twice as bad as being off by 5. But if being off by 10 is just twice as bad as being off by 5, then MAE is more appropriate. . | . Root Mean Square Log Error (RMSLE): It is an extension on root Mean Squared Error (RMSE) that is mainly used when predictions have large deviations | . RMSLE is preferable when : . targets having exponential growth, such as population counts, average sales of a commodity over a span of years etc . | we care about percentage errors rather than the absolute value of errors : The reason we use log is because generally, you care not so much about missing by €10 but missing by 10%. So if it was €1000,000 item and you are €100,000 off or if it was a 10,000 item and you are €1,000 off — we would consider those equivalent scale issues. . | There is a wide range in the target variables and we don’t want to penalize big differences when both the predicted and the actual are big numbers. . | We want to penalize under estimates more than over estimates. . | Let’s imagine two cases of predictions, . | . Case-1: our model makes a prediction of 30 when the actual number is 40 Case-2: our model makes a prediction of 300 when the actual number is 400 . With RMSE the second result is scored as 10 times more than the first result Conversely, with RMSLogE two results are scored the same. RMSLogE takes into account just the ratio of change Lets have a look at the below example . Case-3 : Prediction = 600, Actual = 1000 (the absolute difference is 400) . RMSE = 400, RMSLogE = 0.5108 . Case-4 : Prediction = 1400, Actual = 1000 (the absolute difference is 400) . RMSE = 400, RMSLogE = 0.3365 . When the differences are the same between actual and predicted in both cases. RMSE treated them equally, however RMSLogE penalized the under estimate more than over estimate (under estimated prediction score is higher than over estimated prediction score). Often, penalizing the under estimate more than over estimate is important for prediction of sales and inventory demands. . | . | . Mean Absolute Error (MAE): also called the average absolute deviation : MAE measures the average magnitude of the errors in a set of predictions, without considering their direction. . | R-Squared (R2) : proportional improvement in prediction of the regression model, compared to the mean model (model predicting all given samples as mean value) : - If we were exactly as effective as just predicting the mean, SSres/SStot = 1 and R² = 0 - If we were perfect (i.e. yi = fi for all cases), SSres/SStot = 0 and R² = 1 However, it does not take into consideration of overfitting problem. . Interpreted as the proportion of total variance that is explained by the model. | R² is the ratio between how good your model is (RMSE)vs. how good is the naïve mean model (RMSE). | . | . RMSE vs RMSLE vs MAE . See links below : . RMSE vs MAE : https://medium.com/human-in-a-machine-world/mae-and-rmse-which-metric-is-better-e60ac3bde13d | RMSLE metric and defining baseline : https://www.kaggle.com/carlolepelaars/understanding-the-metric-rmsle/notebook | Model fot metrics : https://www.kaggle.com/residentmario/model-fit-metrics | . Scikit-learn implementation . ### R-square : # sklean from sklearn.metrics import r2_score # hand implemetation import numpy as np def r2_score(y, y_pred): rss_adj = np.sum((y - y_pred)**2) n = len(y) y_bar_adj = (1 / n) * np.sum(y) ess_adj = np.sum((y - y_bar_adj)**2) return 1 - rss_adj / ess_adj r2_score(y, y_pred) ### Root Mean Squared Error (RMSE) from sklearn.metrics import mean_squared_error mean_squared_error(y,y_pred, squared = False) # hand implemetation import math def rmse(y, y_pred): return math.sqrt( ((y-y_pred)**2).mean() ) root_mean_squared_error(y, y_pred) ### Root Mean log Squared Error (RMLSE) from sklearn.metrics import mean_squared_log_error mean_squared_error(y,y_pred, squared = False) # or import numpy as np y = np.log(df.y) RMSLE = rmse(y,y_pred) ### Mean Absolute Error (MAE) from sklearn.metrics import mean_absolute_error # hand implemetation import numpy as np def mae(y,y_pred): return (np.abs(y-y_pred)).mean() NameError Traceback (most recent call last) &lt;ipython-input-1061-7e2a74a2dd02&gt; in &lt;module&gt; 14 return 1 - rss_adj / ess_adj 15 &gt; 16 r2_score(y, y_pred) 17 18 NameError: name &#39;y_pred&#39; is not defined . Download the Data . PATH = &#39;/Users/rmbp/handson-ml2/datasets/&#39; !ls {PATH} housing inception jsb_chorales lifesat titanic import pandas as pd housing = pd.read_csv(f&#39;{PATH}/housing/housing.csv&#39;) housing.head() longitude latitude housing_median_age total_rooms total_bedrooms 0 -122.23 37.88 41.0 880.0 129.0 1 -122.22 37.86 21.0 7099.0 1106.0 2 -122.24 37.85 52.0 1467.0 190.0 3 -122.25 37.85 52.0 1274.0 235.0 4 -122.25 37.85 52.0 1627.0 280.0 population households median_income median_house_value ocean_proximity 0 322.0 126.0 8.3252 452600.0 NEAR BAY 1 2401.0 1138.0 8.3014 358500.0 NEAR BAY 2 496.0 177.0 7.2574 352100.0 NEAR BAY 3 558.0 219.0 5.6431 341300.0 NEAR BAY 4 565.0 259.0 3.8462 342200.0 NEAR BAY . Automating the process of fetching and loading the data . import os import tarfile import urllib DOWNLOAD_ROOT = &quot;https://raw.githubusercontent.com/ageron/handson-ml2/master/&quot; HOUSING_PATH = os.path.join(&quot;/Users/rmbp/Desktop&quot;, &quot;housing&quot;) HOUSING_URL = DOWNLOAD_ROOT + &quot;datasets/housing/housing.tgz&quot; def fetch_housing_data(housing_url=HOUSING_URL, housing_path=HOUSING_PATH): os.makedirs(housing_path, exist_ok=True) tgz_path = os.path.join(housing_path, &quot;housing.tgz&quot;) urllib.request.urlretrieve(housing_url, tgz_path) housing_tgz = tarfile.open(tgz_path) housing_tgz.extractall(path=HOUSING_PATH) housing_tgz.close() fetch_housing_data(HOUSING_URL,HOUSING_PATH) import pandas as pd def load_housing_data(housing_path=HOUSING_PATH): csv_path = os.path.join(housing_path, &quot;housing.csv&quot;) return pd.read_csv(csv_path) load_housing_data().head() longitude latitude housing_median_age total_rooms total_bedrooms 0 -122.23 37.88 41.0 880.0 129.0 1 -122.22 37.86 21.0 7099.0 1106.0 2 -122.24 37.85 52.0 1467.0 190.0 3 -122.25 37.85 52.0 1274.0 235.0 4 -122.25 37.85 52.0 1627.0 280.0 population households median_income median_house_value ocean_proximity 0 322.0 126.0 8.3252 452600.0 NEAR BAY 1 2401.0 1138.0 8.3014 358500.0 NEAR BAY 2 496.0 177.0 7.2574 352100.0 NEAR BAY 3 558.0 219.0 5.6431 341300.0 NEAR BAY 4 565.0 259.0 3.8462 342200.0 NEAR BAY . Take a Quick Look at the Data Structure . housing.head() longitude latitude housing_median_age total_rooms total_bedrooms 0 -122.23 37.88 41.0 880.0 129.0 1 -122.22 37.86 21.0 7099.0 1106.0 2 -122.24 37.85 52.0 1467.0 190.0 3 -122.25 37.85 52.0 1274.0 235.0 4 -122.25 37.85 52.0 1627.0 280.0 population households median_income median_house_value ocean_proximity 0 322.0 126.0 8.3252 452600.0 NEAR BAY 1 2401.0 1138.0 8.3014 358500.0 NEAR BAY 2 496.0 177.0 7.2574 352100.0 NEAR BAY 3 558.0 219.0 5.6431 341300.0 NEAR BAY 4 565.0 259.0 3.8462 342200.0 NEAR BAY housing.info() &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 20640 entries, 0 to 20639 Data columns (total 10 columns): # Column Non-Null Count Dtype -- -- 0 longitude 20640 non-null float64 1 latitude 20640 non-null float64 2 housing_median_age 20640 non-null float64 3 total_rooms 20640 non-null float64 4 total_bedrooms 20433 non-null float64 5 population 20640 non-null float64 6 households 20640 non-null float64 7 median_income 20640 non-null float64 8 median_house_value 20640 non-null float64 9 ocean_proximity 20640 non-null object dtypes: float64(9), object(1) memory usage: 1.6+ MB . There are 20,640 instances in the dataset. Notice that the total_bedrooms attribute has only 20,433 nonnull values, meaning that 207 districts are missing this feature. All attributes are numerical, except the ocean_proximity field. Its type is object. Since we loaded this data from a CSV file, it must be a text attribute. : the values in the ocean_proximity column were repetitive, which means that it is probably a categorical attribute. . housing[&#39;ocean_proximity&#39;].value_counts() &lt;1H OCEAN 9136 INLAND 6551 NEAR OCEAN 2658 NEAR BAY 2290 ISLAND 5 Name: ocean_proximity, dtype: int64 housing.describe(include=&#39;all&#39;).T count unique top freq mean longitude 20640.0 NaN NaN NaN -119.569704 latitude 20640.0 NaN NaN NaN 35.631861 housing_median_age 20640.0 NaN NaN NaN 28.639486 total_rooms 20640.0 NaN NaN NaN 2635.763081 total_bedrooms 20433.0 NaN NaN NaN 537.870553 population 20640.0 NaN NaN NaN 1425.476744 households 20640.0 NaN NaN NaN 499.53968 median_income 20640.0 NaN NaN NaN 3.870671 median_house_value 20640.0 NaN NaN NaN 206855.816909 ocean_proximity 20640 5 &lt;1H OCEAN 9136 NaN std min 25% 50% 75% longitude 2.003532 -124.35 -121.8 -118.49 -118.01 latitude 2.135952 32.54 33.93 34.26 37.71 housing_median_age 12.585558 1.0 18.0 29.0 37.0 total_rooms 2181.615252 2.0 1447.75 2127.0 3148.0 total_bedrooms 421.38507 1.0 296.0 435.0 647.0 population 1132.462122 3.0 787.0 1166.0 1725.0 households 382.329753 1.0 280.0 409.0 605.0 median_income 1.899822 0.4999 2.5634 3.5348 4.74325 median_house_value 115395.615874 14999.0 119600.0 179700.0 264725.0 ocean_proximity NaN NaN NaN NaN NaN max longitude -114.31 latitude 41.95 housing_median_age 52.0 total_rooms 39320.0 total_bedrooms 6445.0 population 35682.0 households 6082.0 median_income 15.0001 median_house_value 500001.0 ocean_proximity NaN . hist() method on the whole dataset will plot a histogram for each numerical attribute . A histogram is used for continuous data, where the bins represent ranges of data, counts the data points in each bin, and shows the bins on the x-axis and the counts on the y-axis. : https://towardsdatascience.com/histograms-and-density-plots-in-python-f6bda88f5ac0 | A bar chart is a plot of categorical variables. | . #This tells Jupyter to set up Matplotlib so it uses Jupyter’s own backend. %matplotlib inline import matplotlib.pyplot as plt housing.hist(bins=60, figsize=(15,10)) plt.show() . . # plot &#39;median_house_value&#39; housing[&#39;median_house_value&#39;].plot(kind=&#39;hist&#39;, bins= 60) &lt;AxesSubplot:ylabel=&#39;Frequency&#39;&gt; . . housing[&#39;ocean_proximity&#39;].value_counts().plot(kind= &#39;barh&#39;) &lt;AxesSubplot:&gt; . . pd.DataFrame(housing[&#39;median_income&#39;].describe()).T count mean std min 25% 50% 75% median_income 20640.0 3.870671 1.899822 0.4999 2.5634 3.5348 4.74325 max median_income 15.0001 n, bins, patches = plt.hist(housing.median_income, bins = int((15.000100 - 0.499900)/0.1),edgecolor = &#39;black&#39; ,color = &#39;blue&#39;) # bins = int((15.000100 - 0.499900)/0.1) : We choose the number of bins with an interval lenght of 100€ . . pd.DataFrame(housing[&#39;housing_median_age&#39;].describe()).T count mean std min 25% 50% 75% max housing_median_age 20640.0 28.639486 12.585558 1.0 18.0 29.0 37.0 52.0 n, bins, patches = plt.hist(housing.housing_median_age, bins = int((52.000000 - 1.000000)/1) , color = &#39;blue&#39; , edgecolor = &#39;black&#39;) bins array([ 1., 2., 3., 4., 5., 6., 7., 8., 9., 10., 11., 12., 13., 14., 15., 16., 17., 18., 19., 20., 21., 22., 23., 24., 25., 26., 27., 28., 29., 30., 31., 32., 33., 34., 35., 36., 37., 38., 39., 40., 41., 42., 43., 44., 45., 46., 47., 48., 49., 50., 51., 52.]) . . # Target pd.DataFrame(housing[&#39;median_house_value&#39;].describe()).T count mean std min 25% median_house_value 20640.0 206855.816909 115395.615874 14999.0 119600.0 50% 75% max median_house_value 179700.0 264725.0 500001.0 n, bins, patches = plt.hist(housing.median_house_value , bins = int((500001.000000 - 14999.000000)/10000) , color = &#39;blue&#39; ,edgecolor = &#39;black&#39;) bins array([ 14999. , 25103.20833333, 35207.41666667, 45311.625 , 55415.83333333, 65520.04166667, 75624.25 , 85728.45833333, 95832.66666667, 105936.875 , 116041.08333333, 126145.29166667, 136249.5 , 146353.70833333, 156457.91666667, 166562.125 , 176666.33333333, 186770.54166667, 196874.75 , 206978.95833333, 217083.16666667, 227187.375 , 237291.58333333, 247395.79166667, 257500. , 267604.20833333, 277708.41666667, 287812.625 , 297916.83333333, 308021.04166667, 318125.25 , 328229.45833333, 338333.66666667, 348437.875 , 358542.08333333, 368646.29166667, 378750.5 , 388854.70833333, 398958.91666667, 409063.125 , 419167.33333333, 429271.54166667, 439375.75 , 449479.95833333, 459584.16666667, 469688.375 , 479792.58333333, 489896.79166667, 500001. ]) . . From the figure below, we can see how the data was computed : . We can see that median_income was scaled and capped at 15 (actually, 15.0001) for higher median incomes, and at 0.5 (actually, 0.4999) for lower median incomes. The numbers represent roughly tens of thousands of dollars. | The housing median age and the median house value were also capped. The latter may be a serious problem since it is our target attribute. In this caseour Machine Learning algorithms may learn that prices never go beyond that limit (€500,000). We need to check with our team to see if this is a problem or not. If the team needs precise predictions even beyond €500,000, then you have two options: Collect proper labels for the districts whose labels were capped. | Remove those districts from the training set (and also from the test set, since your system should not be evaluated poorly if it predicts values beyond €500,000). | . | . We can also see that : . These attributes have very different scales. | Many histograms are tail-heavy : they extend much farther to the right of the median than to the left. | . Create a Test Set . We ahve only taken a quick glance at the data : numeric / categorical features, missing values, scale of attributes, distribution, how values are computed, distribution of the target variable. It’s enough. Why ? . if you look at the test set, you may stumble upon some seemingly interesting pattern in the test data that leads you to select a particular kind of Machine Learning model. When you estimate the generalization error using the test set, your estimate will be too optimistic, and you will launch a system that will not perform as well as expected. This is called data snooping bias. | . Train and test set stability . Creating a test set is theoretically simple: pick some instances randomly, typically 20% of the dataset . import numpy as np def split_train_test(data, test_ratio): shuffled_indices = np.random.permutation(len(data)) test_set_size = int(len(data) * test_ratio) test_indices = shuffled_indices[:test_set_size] train_indices = shuffled_indices[test_set_size:] return data.iloc[train_indices], data.iloc[test_indices] train_set, test_set = split_train_test(housing,0.2) print(len(train_set)) print(len(test_set)) 16512 4128 . Well, this works, but it is not perfect: if you run the program again, it will generate a different test set! . Over time, you (or your Machine Learning algorithms) will get to see the whole dataset, which is what you want to avoid. | . # first run test set split_train_test(housing,0.2)[1].head(5) longitude latitude housing_median_age total_rooms total_bedrooms 12003 -117.57 33.90 7.0 3797.0 850.0 13304 -117.63 34.09 19.0 3490.0 816.0 19037 -121.99 38.36 35.0 2728.0 451.0 9871 -121.82 36.61 24.0 2437.0 438.0 16526 -121.20 37.80 37.0 311.0 61.0 population households median_income median_house_value 12003 2369.0 720.0 3.5525 137600.0 13304 2818.0 688.0 2.8977 126200.0 19037 1290.0 452.0 3.2768 117600.0 9871 1430.0 444.0 3.8015 169100.0 16526 171.0 54.0 4.0972 101800.0 ocean_proximity 12003 INLAND 13304 INLAND 19037 INLAND 9871 &lt;1H OCEAN 16526 INLAND # second run test set split_train_test(housing,0.2)[1].head(5) longitude latitude housing_median_age total_rooms total_bedrooms 5709 -118.23 34.21 32.0 1464.0 406.0 16381 -121.30 38.02 4.0 1515.0 384.0 16458 -121.30 38.13 26.0 2256.0 360.0 8613 -118.37 33.87 23.0 1829.0 331.0 2738 -115.56 32.78 35.0 1185.0 202.0 population households median_income median_house_value 5709 693.0 380.0 2.5463 200000.0 16381 491.0 348.0 2.8523 87500.0 16458 937.0 372.0 5.0528 153700.0 8613 891.0 356.0 6.5755 359900.0 2738 615.0 191.0 4.6154 86200.0 ocean_proximity 5709 &lt;1H OCEAN 16381 INLAND 16458 INLAND 8613 &lt;1H OCEAN 2738 INLAND . Solution : . One solution is to save the test set on the first run and then load it in subsequent runs. | Another option is to set the random number generator’s seed (e.g., with np.ran dom.seed(42))14 before calling np.random.permutation() so that it always generates the same shuffled indices : | . import numpy as np def split_train_test(data, test_ratio): np.random.seed(1997) shuffled_indices = np.random.permutation(len(data)) test_set_size = int(len(data) * test_ratio) test_indices = shuffled_indices[:test_set_size] train_indices = shuffled_indices[test_set_size:] return data.iloc[train_indices], data.iloc[test_indices] # first run test set split_train_test(housing,0.2)[1].head(5) longitude latitude housing_median_age total_rooms total_bedrooms 9009 -118.60 34.07 16.0 319.0 59.0 17779 -121.83 37.38 15.0 4430.0 992.0 20209 -119.21 34.28 27.0 2219.0 312.0 3170 -119.69 36.41 38.0 1016.0 202.0 2200 -119.85 36.83 15.0 2563.0 335.0 population households median_income median_house_value 9009 149.0 64.0 4.6250 433300.0 17779 3278.0 1018.0 4.5533 209900.0 20209 937.0 315.0 5.7601 281100.0 3170 540.0 187.0 2.2885 75000.0 2200 1080.0 356.0 6.7181 160300.0 ocean_proximity 9009 &lt;1H OCEAN 17779 &lt;1H OCEAN 20209 NEAR OCEAN 3170 INLAND 2200 INLAND # second run test set split_train_test(housing,0.2)[1].head(5) longitude latitude housing_median_age total_rooms total_bedrooms 9009 -118.60 34.07 16.0 319.0 59.0 17779 -121.83 37.38 15.0 4430.0 992.0 20209 -119.21 34.28 27.0 2219.0 312.0 3170 -119.69 36.41 38.0 1016.0 202.0 2200 -119.85 36.83 15.0 2563.0 335.0 population households median_income median_house_value 9009 149.0 64.0 4.6250 433300.0 17779 3278.0 1018.0 4.5533 209900.0 20209 937.0 315.0 5.7601 281100.0 3170 540.0 187.0 2.2885 75000.0 2200 1080.0 356.0 6.7181 160300.0 ocean_proximity 9009 &lt;1H OCEAN 17779 &lt;1H OCEAN 20209 NEAR OCEAN 3170 INLAND 2200 INLAND . But both these solutions will break the next time you fetch an updated dataset. . If the dataset is updated, we want to ensure that the test set will remain consistent across multiple runs, even if you refresh the dataset : The new test set will contain 20% of the new instances, but it will not contain any instance that was previously in the training set. . To have a stable train/test split even after updating the dataset, a common solution is to use each instance’s identifier to decide whether or not it should go in the test set (assuming instances have a unique and immutable identifier). For example, we could compute a hash of each instance’s identifier and put that instance in the test set if the hash is lower than or equal to 20% of the maximum hash value. . from zlib import crc32 def test_set_check(identifier, test_ratio): return crc32(np.int64(identifier)) &amp; 0xffffffff &lt; test_ratio * 2**32 # crc32(np.int64(identifier)) = create a hash from a given value # crc32(np.int64(identifier)) &amp; 0xffffffff = make sure the hash value does not exceed 2^32 (or 4294967296). # crc32(np.int64(identifier)) &amp; 0xffffffff &lt; test_ratio * 2**32. # crc32(np.int64(identifier)) &amp; 0xffffffff &lt; test_ratio * 2**32 # This line returns True or False. Let test_ratio be 0.2. # Then, any hash value less than 0.2 * 4294967296 returns True and will be # added to the test set; otherwise, it returns False and will be added to the training set. */ def split_train_test_by_id(data, test_ratio, id_column): ids = data[id_column] # compute a hash of each instance’s identifier in_test_set = ids.apply(lambda id_: test_set_check(id_, test_ratio)) # if hash is lower than or equal to 20% of the maximum hash value return data.loc[~in_test_set], data.loc[in_test_set] . Unfortunately, the housing dataset does not have an identifier column. The simplest solution is to use the row index as the ID: . housing_with_id = housing.reset_index() # adds an `index` column train_set, test_set = split_train_test_by_id(housing_with_id, 0.2, &quot;index&quot;) . If we use the row index as a unique identifier, you need to make sure that new data gets appended to the end of the dataset and that no row ever gets deleted. If this is not possible, then we can try to use the most stable features to build a unique identifier. . For example, a district’s latitude and longitude are guaranteed to be stable for a few million years, so you could combine them into an ID like so: . housing_with_id[&quot;id&quot;] = housing[&quot;longitude&quot;] * 1000 + housing[&quot;latitude&quot;] train_set, test_set = split_train_test_by_id(housing_with_id, 0.2, &quot;id&quot;) . See explanation of this method in : https://ichi.pro/fr/ameliorez-la-repartition-des-tests-de-train-avec-la-fonction-de-hachage-267796356735483 and https://datascience.stackexchange.com/questions/51348/splitting-train-test-sets-by-an-identifier . Train / Test split using Sckit-learn . Scikit-Learn provides a few functions to split datasets into multiple subsets in various ways. The simplest function is train_test_split(), which does pretty much the same thing as the function split_train_test(), with a couple of additional features : . First, there is a random_state parameter random_state that allows you to set the random generator seed and a test size test_size. | Second, we can pass it multiple datasets with an identical number of rows, and it will split them on the same indices (this is very useful, for example, if you have a separate DataFrame for labels): | . from sklearn.model_selection import train_test_split train_set, test_set = train_test_split(housing, test_size=0.2, random_state = 1997) . Sampling bias in Test set . Using train_test_splitmethod, we using purely random sampling methods to generate our test set. This is generally fine if our dataset is large enough (especially relative to the number of attributes), but if it is not, we run the risk of introducing a significant sampling bias. . If an attribute (continues or categorical) is important (after discussing with experts for exemple) : We may want to ensure that the test set is representative of the various categories of that variable in the whole dataset. . Suppose that the median income is a very important attribute to predict median housing prices. . housing[&#39;median_income&#39;].describe() count 20640.000000 mean 3.870671 std 1.899822 min 0.499900 25% 2.563400 50% 3.534800 75% 4.743250 max 15.000100 Name: median_income, dtype: float64 plt.hist(housing[&#39;median_income&#39;] #, bins = int( (housing[&#39;median_income&#39;].max() - housing[&#39;median_income&#39;].min()) / 0.5) , bins = 60 , color = &#39;blue&#39; ,edgecolor = &#39;black&#39; ) plt.show() . . housing[&quot;income_cat&quot;] = pd.cut(housing[&quot;median_income&quot;], bins=[0., 1.5, 3.0, 4.5, 6., np.inf], labels=[1, 2, 3, 4, 5]) housing[&#39;income_cat&#39;] = pd.cut( housing[&#39;median_income&#39;] , bins = [0., 1.5, 3.0, 4.5, 6., np.inf] , labels = [1, 2, 3, 4, 5] ) housing[&quot;income_cat&quot;].hist() &lt;AxesSubplot:&gt; . . housing[&quot;income_cat&quot;].value_counts() / len(housing) 3 0.350581 2 0.318847 4 0.176308 5 0.114438 1 0.039826 Name: income_cat, dtype: float64 from sklearn.model_selection import StratifiedShuffleSplit split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42) for train_index, test_index in split.split(housing, housing[&quot;income_cat&quot;]): strat_train_set = housing.loc[train_index] strat_test_set = housing.loc[test_index] strat_train_set[&quot;income_cat&quot;].value_counts() / len(strat_train_set) 3 0.350594 2 0.318859 4 0.176296 5 0.114462 1 0.039789 Name: income_cat, dtype: float64 strat_test_set[&quot;income_cat&quot;].value_counts() / len(strat_test_set) 3 0.350533 2 0.318798 4 0.176357 5 0.114341 1 0.039971 Name: income_cat, dtype: float64 def income_cat_proportions(data): return data[&quot;income_cat&quot;].value_counts() / len(data) train_set, test_set = train_test_split(housing, test_size=0.2, random_state=42) compare_props = pd.DataFrame({ &quot;Overall&quot;: income_cat_proportions(housing), &quot;Stratified&quot;: income_cat_proportions(strat_test_set), &quot;Random&quot;: income_cat_proportions(test_set), }).sort_index() compare_props[&quot;Rand. %error&quot;] = 100 * compare_props[&quot;Random&quot;] / compare_props[&quot;Overall&quot;] - 100 compare_props[&quot;Strat. %error&quot;] = 100 * compare_props[&quot;Stratified&quot;] / compare_props[&quot;Overall&quot;] - 100 compare_props Overall Stratified Random Rand. %error Strat. %error 1 0.039826 0.039971 0.040213 0.973236 0.364964 2 0.318847 0.318798 0.324370 1.732260 -0.015195 3 0.350581 0.350533 0.358527 2.266446 -0.013820 4 0.176308 0.176357 0.167393 -5.056334 0.027480 5 0.114438 0.114341 0.109496 -4.318374 -0.084674 . Further analysis later | . for set_ in (strat_train_set, strat_test_set): set_.drop(&quot;income_cat&quot;, axis=1, inplace=True) . Discover and Visualize the Data to Gain Insights . First, we make sure that we have put the test set aside and we are only exploring the training set. Also, if the training set is very large, you may want to sample an exploration set, to make manipulations easy and fast. In our case, the set is quite small, so we can just work directly on the full set. . Let’s create a copy so that you can play with it without harming the training set: | . housing = strat_train_set.copy() housing.head(5) longitude latitude housing_median_age total_rooms total_bedrooms 12655 -121.46 38.52 29.0 3873.0 797.0 15502 -117.23 33.09 7.0 5320.0 855.0 2908 -119.04 35.37 44.0 1618.0 310.0 14053 -117.13 32.75 24.0 1877.0 519.0 20496 -118.70 34.28 27.0 3536.0 646.0 population households median_income median_house_value 12655 2237.0 706.0 2.1736 72100.0 15502 2015.0 768.0 6.3373 279600.0 2908 667.0 300.0 2.8750 82700.0 14053 898.0 483.0 2.2264 112500.0 20496 1837.0 580.0 4.4964 238300.0 ocean_proximity income_cat 12655 INLAND 2 15502 NEAR OCEAN 5 2908 INLAND 2 14053 NEAR OCEAN 2 20496 &lt;1H OCEAN 3 . Since we have geographic information (lon / lat), let’s create a scatterplot of all districts to visualize the data : doc of a scatterplot parameter https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.scatter.html | . housing.plot(kind = &#39;scatter&#39;, x = &#39;longitude&#39;, y = &#39;latitude&#39;) &lt;AxesSubplot:xlabel=&#39;longitude&#39;, ylabel=&#39;latitude&#39;&gt; . . Scatter plots work well for hundreds of observations but overplotting becomes an issue once the number of observations gets into tens of thousands. . We can see that in some areas, there are vast numbers of dots, so it is hard to see any particular pattern. . Simple options to address overplotting : . reducing the point size : usisng the s parameter - This parameter indicates the marker size. | alpha blending : using alpha parameter This option indicates the blending value, between 0 (transparent) and 1 (opaque). | . housing.plot(kind = &#39;scatter&#39; ,x = &#39;longitude&#39; ,y = &#39;latitude&#39; , s= 0.2) &lt;AxesSubplot:xlabel=&#39;longitude&#39;, ylabel=&#39;latitude&#39;&gt; . . housing.plot(kind = &#39;scatter&#39; ,x = &#39;longitude&#39; ,y = &#39;latitude&#39; , alpha= 0.1) &lt;AxesSubplot:xlabel=&#39;longitude&#39;, ylabel=&#39;latitude&#39;&gt; . . We can get the names of the cities in the map and conclude which have the highest density - Many article covers this subject - we will do it later . # To save a picture in our folder project : IMAGES_PATH = &quot;/Users/rmbp/Desktop/housing&quot; def save_fig(fig_id, tight_layout=True, fig_extension=&quot;png&quot;, resolution=300): path = os.path.join(IMAGES_PATH, fig_id + &quot;.&quot; + fig_extension) print(&quot;Saving figure&quot;, fig_id) if tight_layout: plt.tight_layout() plt.savefig(path, format=fig_extension, dpi=resolution) housing.plot(kind = &#39;scatter&#39; ,x = &#39;longitude&#39; ,y = &#39;latitude&#39; , alpha= 0.1, c=&#39;black&#39;) save_fig(&quot;better_visualization_plot&quot;) Saving figure better_visualization_plot . . We can see the houses price crossing with the population on the map below : . the parameter s re presenting the radius of each circle will represents the `district’s population`` | the paramter c representing the color will represents the price. | We will use a predefined color map (option cmap) called jet, which ranges from blue (low values) to red (high prices): | . housing.plot(kind=&quot;scatter&quot;, x=&quot;longitude&quot;, y=&quot;latitude&quot;, alpha=0.4, s=housing[&quot;population&quot;]/100, label=&quot;population&quot;, figsize=(10,7), c=&quot;median_house_value&quot;, cmap=plt.get_cmap(&quot;jet&quot;), colorbar=True, ) plt.legend() &lt;matplotlib.legend.Legend at 0x12f4d1650&gt; . . This image tells us that the housing prices are very much related to the location (e.g., close to the ocean) and to the population density. | A clustering algorithm should be useful for detecting the main cluster and for adding new features that measure the proximity to the cluster centers. See later.. Check this blog : https://dev.to/travelleroncode/analyzing-a-dataset-with-unsupervised-learning-31ld | The ocean proximity attribute may be useful as well, although in Northern California the housing prices in coastal districts are not too high, so it is not a simple rule. | . Looking for correlations . If we want to explore our data it is good to compute correlation between numeric variable : Spearman S and Pearon P. W can compute them both since the relation between the Spearman (S) and Pearson (P) correlations will give some good information : . Briefly, S is computed on ranks and so depicts monotonic relationships while P is on true values and depicts linear relationships. . | We the corr method : By default, method = ‘Pearson’ . | . s = {} for x in range(1,100): s[x] = math.exp(x) s = pd.DataFrame(s.items()) s.corr(&#39;pearson&#39;) 0 1 0 1.000000 0.253274 1 0.253274 1.000000 s.corr(&#39;spearman&#39;) 0 1 0 1.0 1.0 1 1.0 1.0 . This is because 𝑦 increases monotonically with 𝑥 so the Spearman correlation is perfect, but not linearly, so the Pearson correlation is imperfect. . Doing both is interesting because if we have S &gt; P, that means that we have a correlation that is monotonic but not linear. Since it is good to have linearity in statistics (it is easier) we can try to apply a transformation on 𝑦(such a log). . corr_matrix = housing.corr() corr_matrix longitude latitude housing_median_age total_rooms longitude 1.000000 -0.924478 -0.105823 0.048909 latitude -0.924478 1.000000 0.005737 -0.039245 housing_median_age -0.105823 0.005737 1.000000 -0.364535 total_rooms 0.048909 -0.039245 -0.364535 1.000000 total_bedrooms 0.076686 -0.072550 -0.325101 0.929391 population 0.108071 -0.115290 -0.298737 0.855103 households 0.063146 -0.077765 -0.306473 0.918396 median_income -0.019615 -0.075146 -0.111315 0.200133 median_house_value -0.047466 -0.142673 0.114146 0.135140 total_bedrooms population households median_income longitude 0.076686 0.108071 0.063146 -0.019615 latitude -0.072550 -0.115290 -0.077765 -0.075146 housing_median_age -0.325101 -0.298737 -0.306473 -0.111315 total_rooms 0.929391 0.855103 0.918396 0.200133 total_bedrooms 1.000000 0.876324 0.980167 -0.009643 population 0.876324 1.000000 0.904639 0.002421 households 0.980167 0.904639 1.000000 0.010869 median_income -0.009643 0.002421 0.010869 1.000000 median_house_value 0.047781 -0.026882 0.064590 0.687151 median_house_value longitude -0.047466 latitude -0.142673 housing_median_age 0.114146 total_rooms 0.135140 total_bedrooms 0.047781 population -0.026882 households 0.064590 median_income 0.687151 median_house_value 1.000000 . Now let’s look at how much each attribute correlates with the median house value: . corr_matrix[&#39;median_house_value&#39;].sort_values(ascending = False) median_house_value 1.000000 median_income 0.687151 total_rooms 0.135140 housing_median_age 0.114146 households 0.064590 total_bedrooms 0.047781 population -0.026882 longitude -0.047466 latitude -0.142673 Name: median_house_value, dtype: float64 corr_matrix = housing.corr(&#39;spearman&#39;) corr_matrix[&#39;median_house_value&#39;].sort_values(ascending = False) median_house_value 1.000000 median_income 0.675714 total_rooms 0.204476 households 0.110722 total_bedrooms 0.084284 housing_median_age 0.083301 population 0.001309 longitude -0.071562 latitude -0.162283 Name: median_house_value, dtype: float64 . Another way to check for correlation between attributes is to use the pandas scatter_matrix() function, which plots every numerical attribute against every other numerical attribute. ( if we have 11 attribiute, we will plot 11**2 plots ) . From the pearson coefficient below, we focus on a few promising attributes that seem most correlated with the median housing value : . from pandas.plotting import scatter_matrix attributes = [&#39;median_house_value&#39;, &#39;median_income&#39;, &#39;total_rooms&#39;, &#39;housing_median_age&#39; ] scatter_matrix(housing[attributes], figsize=(10,6), ) array([[&lt;AxesSubplot:xlabel=&#39;median_house_value&#39;, ylabel=&#39;median_house_value&#39;&gt;, &lt;AxesSubplot:xlabel=&#39;median_income&#39;, ylabel=&#39;median_house_value&#39;&gt;, &lt;AxesSubplot:xlabel=&#39;total_rooms&#39;, ylabel=&#39;median_house_value&#39;&gt;, &lt;AxesSubplot:xlabel=&#39;housing_median_age&#39;, ylabel=&#39;median_house_value&#39;&gt;], [&lt;AxesSubplot:xlabel=&#39;median_house_value&#39;, ylabel=&#39;median_income&#39;&gt;, &lt;AxesSubplot:xlabel=&#39;median_income&#39;, ylabel=&#39;median_income&#39;&gt;, &lt;AxesSubplot:xlabel=&#39;total_rooms&#39;, ylabel=&#39;median_income&#39;&gt;, &lt;AxesSubplot:xlabel=&#39;housing_median_age&#39;, ylabel=&#39;median_income&#39;&gt;], [&lt;AxesSubplot:xlabel=&#39;median_house_value&#39;, ylabel=&#39;total_rooms&#39;&gt;, &lt;AxesSubplot:xlabel=&#39;median_income&#39;, ylabel=&#39;total_rooms&#39;&gt;, &lt;AxesSubplot:xlabel=&#39;total_rooms&#39;, ylabel=&#39;total_rooms&#39;&gt;, &lt;AxesSubplot:xlabel=&#39;housing_median_age&#39;, ylabel=&#39;total_rooms&#39;&gt;], [&lt;AxesSubplot:xlabel=&#39;median_house_value&#39;, ylabel=&#39;housing_median_age&#39;&gt;, &lt;AxesSubplot:xlabel=&#39;median_income&#39;, ylabel=&#39;housing_median_age&#39;&gt;, &lt;AxesSubplot:xlabel=&#39;total_rooms&#39;, ylabel=&#39;housing_median_age&#39;&gt;, &lt;AxesSubplot:xlabel=&#39;housing_median_age&#39;, ylabel=&#39;housing_median_age&#39;&gt;]], dtype=object) . . The main diagonal (top left to bottom right) would be full of straight lines if pandas plotted each variable against itself, which would not be very useful. So instead pandas displays a histogram of each attribute. The diagonal option in scatter_matrix pick between ‘kde’ and ‘hist’ for either Kernel Density Estimation or Histogram plot in the diagonal. . The most promising attribute to predict the median house value is the median income( Pearson and Spearman correlation coefficient = 0.67 ), so let’s zoom in on their correlation scatterplot : . housing.plot( kind = &#39;scatter&#39; ,x = &#39;median_income&#39; ,y = &#39;median_house_value&#39; ,alpha = 0.2) &lt;AxesSubplot:xlabel=&#39;median_income&#39;, ylabel=&#39;median_house_value&#39;&gt; . . This plot reveals a few things : . First, the correlation is indeed very strong; we can clearly see the upward trend, and the points are not too dispersed. | Second, the price cap that we noticed earlier is clearly visible as a horizontal line at $500,000. But this plot reveals other less obvious straight lines: a horizontal line around $450,000, another around $350,000, perhaps one around $280,000, and a few more below that, we can see this picks in the histogram above: As result, we may want to try removing the corresponding districts to prevent our algorithms from learning to reproduce these data quirks. | . housing[&#39;median_house_value&#39;].describe() count 16512.000000 mean 207005.322372 std 115701.297250 min 14999.000000 25% 119800.000000 50% 179500.000000 75% 263900.000000 max 500001.000000 Name: median_house_value, dtype: float64 # Data picks in the target variable plt.hist(housing[&#39;median_house_value&#39;] #, bins = int( (housing[&#39;median_income&#39;].max() - housing[&#39;median_income&#39;].min()) / 0.5) , bins = int ((500001.000000 - 14999.000000)/1000) , color = &#39;blue&#39; ,edgecolor = &#39;black&#39; ) (array([ 3., 0., 0., 0., 0., 0., 0., 3., 0., 0., 0., 2., 1., 1., 0., 1., 0., 4., 1., 3., 1., 2., 4., 2., 3., 4., 5., 10., 13., 11., 14., 11., 19., 17., 21., 27., 24., 36., 33., 39., 59., 30., 53., 50., 38., 43., 39., 45., 45., 35., 54., 48., 74., 59., 62., 54., 47., 61., 51., 39., 51., 41., 33., 47., 37., 39., 64., 43., 54., 59., 63., 55., 106., 73., 55., 86., 53., 84., 74., 73., 81., 70., 77., 71., 64., 79., 59., 45., 73., 50., 55., 49., 55., 68., 65., 62., 72., 110., 78., 54., 56., 66., 55., 79., 53., 57., 55., 60., 59., 43., 83., 61., 54., 45., 59., 52., 61., 51., 55., 65., 59., 68., 144., 51., 75., 72., 65., 75., 74., 64., 68., 79., 57., 52., 38., 111., 76., 64., 67., 77., 63., 94., 75., 83., 76., 96., 74., 145., 73., 74., 92., 88., 55., 61., 61., 79., 68., 52., 64., 53., 91., 50., 57., 63., 68., 52., 69., 59., 84., 74., 65., 70., 114., 44., 56., 63., 70., 69., 57., 51., 64., 52., 37., 49., 40., 59., 41., 35., 32., 52., 47., 45., 34., 47., 51., 41., 37., 51., 53., 50., 51., 44., 49., 66., 44., 55., 50., 49., 46., 38., 107., 56., 50., 48., 48., 52., 60., 51., 44., 52., 40., 41., 49., 44., 43., 53., 49., 27., 51., 39., 43., 30., 47., 37., 22., 50., 33., 36., 42., 43., 35., 20., 34., 40., 29., 29., 37., 44., 41., 39., 38., 40., 40., 39., 35., 30., 43., 34., 34., 65., 21., 33., 25., 29., 38., 22., 23., 26., 30., 20., 22., 24., 36., 22., 25., 28., 26., 26., 21., 24., 26., 16., 15., 9., 31., 12., 17., 19., 19., 18., 17., 18., 14., 15., 19., 11., 22., 14., 18., 21., 23., 15., 9., 24., 16., 17., 18., 23., 20., 28., 12., 12., 21., 11., 22., 17., 22., 19., 22., 19., 25., 21., 15., 14., 20., 25., 22., 20., 18., 22., 22., 16., 13., 22., 75., 14., 19., 19., 19., 15., 22., 13., 18., 16., 21., 16., 19., 24., 11., 13., 16., 17., 13., 11., 15., 4., 18., 9., 8., 26., 8., 14., 6., 8., 12., 12., 11., 10., 12., 14., 5., 13., 16., 7., 7., 11., 10., 12., 14., 15., 9., 11., 10., 10., 22., 4., 2., 12., 2., 10., 12., 11., 2., 4., 14., 9., 10., 10., 5., 13., 5., 13., 8., 13., 7., 9., 3., 8., 8., 12., 5., 5., 5., 5., 2., 11., 7., 6., 9., 11., 7., 7., 7., 9., 7., 7., 6., 7., 8., 9., 6., 7., 5., 2., 28., 6., 5., 7., 6., 5., 3., 6., 10., 6., 6., 1., 6., 4., 4., 3., 3., 6., 5., 3., 5., 3., 4., 6., 3., 10., 1., 2., 8., 4., 1., 3., 1., 3., 10., 7., 2., 4., 4., 3., 3., 4., 3., 4., 4., 2., 6., 2., 2., 5., 810.]), array([ 14999. , 15999.00412371, 16999.00824742, 17999.01237113, 18999.01649485, 19999.02061856, 20999.02474227, 21999.02886598, 22999.03298969, 23999.0371134 , 24999.04123711, 25999.04536082, 26999.04948454, 27999.05360825, 28999.05773196, 29999.06185567, 30999.06597938, 31999.07010309, 32999.0742268 , 33999.07835052, 34999.08247423, 35999.08659794, 36999.09072165, 37999.09484536, 38999.09896907, 39999.10309278, 40999.10721649, 41999.11134021, 42999.11546392, 43999.11958763, 44999.12371134, 45999.12783505, 46999.13195876, 47999.13608247, 48999.14020619, 49999.1443299 , 50999.14845361, 51999.15257732, 52999.15670103, 53999.16082474, 54999.16494845, 55999.16907216, 56999.17319588, 57999.17731959, 58999.1814433 , 59999.18556701, 60999.18969072, 61999.19381443, 62999.19793814, 63999.20206186, 64999.20618557, 65999.21030928, 66999.21443299, 67999.2185567 , 68999.22268041, 69999.22680412, 70999.23092784, 71999.23505155, 72999.23917526, 73999.24329897, 74999.24742268, 75999.25154639, 76999.2556701 , 77999.25979381, 78999.26391753, 79999.26804124, 80999.27216495, 81999.27628866, 82999.28041237, 83999.28453608, 84999.28865979, 85999.29278351, 86999.29690722, 87999.30103093, 88999.30515464, 89999.30927835, 90999.31340206, 91999.31752577, 92999.32164948, 93999.3257732 , 94999.32989691, 95999.33402062, 96999.33814433, 97999.34226804, 98999.34639175, 99999.35051546, 100999.35463918, 101999.35876289, 102999.3628866 , 103999.36701031, 104999.37113402, 105999.37525773, 106999.37938144, 107999.38350515, 108999.38762887, 109999.39175258, 110999.39587629, 111999.4 , 112999.40412371, 113999.40824742, 114999.41237113, 115999.41649485, 116999.42061856, 117999.42474227, 118999.42886598, 119999.43298969, 120999.4371134 , 121999.44123711, 122999.44536082, 123999.44948454, 124999.45360825, 125999.45773196, 126999.46185567, 127999.46597938, 128999.47010309, 129999.4742268 , 130999.47835052, 131999.48247423, 132999.48659794, 133999.49072165, 134999.49484536, 135999.49896907, 136999.50309278, 137999.50721649, 138999.51134021, 139999.51546392, 140999.51958763, 141999.52371134, 142999.52783505, 143999.53195876, 144999.53608247, 145999.54020619, 146999.5443299 , 147999.54845361, 148999.55257732, 149999.55670103, 150999.56082474, 151999.56494845, 152999.56907216, 153999.57319588, 154999.57731959, 155999.5814433 , 156999.58556701, 157999.58969072, 158999.59381443, 159999.59793814, 160999.60206186, 161999.60618557, 162999.61030928, 163999.61443299, 164999.6185567 , 165999.62268041, 166999.62680412, 167999.63092784, 168999.63505155, 169999.63917526, 170999.64329897, 171999.64742268, 172999.65154639, 173999.6556701 , 174999.65979381, 175999.66391753, 176999.66804124, 177999.67216495, 178999.67628866, 179999.68041237, 180999.68453608, 181999.68865979, 182999.69278351, 183999.69690722, 184999.70103093, 185999.70515464, 186999.70927835, 187999.71340206, 188999.71752577, 189999.72164948, 190999.7257732 , 191999.72989691, 192999.73402062, 193999.73814433, 194999.74226804, 195999.74639175, 196999.75051546, 197999.75463918, 198999.75876289, 199999.7628866 , 200999.76701031, 201999.77113402, 202999.77525773, 203999.77938144, 204999.78350515, 205999.78762887, 206999.79175258, 207999.79587629, 208999.8 , 209999.80412371, 210999.80824742, 211999.81237113, 212999.81649485, 213999.82061856, 214999.82474227, 215999.82886598, 216999.83298969, 217999.8371134 , 218999.84123711, 219999.84536082, 220999.84948454, 221999.85360825, 222999.85773196, 223999.86185567, 224999.86597938, 225999.87010309, 226999.8742268 , 227999.87835052, 228999.88247423, 229999.88659794, 230999.89072165, 231999.89484536, 232999.89896907, 233999.90309278, 234999.90721649, 235999.91134021, 236999.91546392, 237999.91958763, 238999.92371134, 239999.92783505, 240999.93195876, 241999.93608247, 242999.94020619, 243999.9443299 , 244999.94845361, 245999.95257732, 246999.95670103, 247999.96082474, 248999.96494845, 249999.96907216, 250999.97319588, 251999.97731959, 252999.9814433 , 253999.98556701, 254999.98969072, 255999.99381443, 256999.99793814, 258000.00206186, 259000.00618557, 260000.01030928, 261000.01443299, 262000.0185567 , 263000.02268041, 264000.02680412, 265000.03092784, 266000.03505155, 267000.03917526, 268000.04329897, 269000.04742268, 270000.05154639, 271000.0556701 , 272000.05979381, 273000.06391753, 274000.06804124, 275000.07216495, 276000.07628866, 277000.08041237, 278000.08453608, 279000.08865979, 280000.09278351, 281000.09690722, 282000.10103093, 283000.10515464, 284000.10927835, 285000.11340206, 286000.11752577, 287000.12164948, 288000.1257732 , 289000.12989691, 290000.13402062, 291000.13814433, 292000.14226804, 293000.14639175, 294000.15051546, 295000.15463918, 296000.15876289, 297000.1628866 , 298000.16701031, 299000.17113402, 300000.17525773, 301000.17938144, 302000.18350515, 303000.18762887, 304000.19175258, 305000.19587629, 306000.2 , 307000.20412371, 308000.20824742, 309000.21237113, 310000.21649485, 311000.22061856, 312000.22474227, 313000.22886598, 314000.23298969, 315000.2371134 , 316000.24123711, 317000.24536082, 318000.24948454, 319000.25360825, 320000.25773196, 321000.26185567, 322000.26597938, 323000.27010309, 324000.2742268 , 325000.27835052, 326000.28247423, 327000.28659794, 328000.29072165, 329000.29484536, 330000.29896907, 331000.30309278, 332000.30721649, 333000.31134021, 334000.31546392, 335000.31958763, 336000.32371134, 337000.32783505, 338000.33195876, 339000.33608247, 340000.34020619, 341000.3443299 , 342000.34845361, 343000.35257732, 344000.35670103, 345000.36082474, 346000.36494845, 347000.36907216, 348000.37319588, 349000.37731959, 350000.3814433 , 351000.38556701, 352000.38969072, 353000.39381443, 354000.39793814, 355000.40206186, 356000.40618557, 357000.41030928, 358000.41443299, 359000.4185567 , 360000.42268041, 361000.42680412, 362000.43092784, 363000.43505155, 364000.43917526, 365000.44329897, 366000.44742268, 367000.45154639, 368000.4556701 , 369000.45979381, 370000.46391753, 371000.46804124, 372000.47216495, 373000.47628866, 374000.48041237, 375000.48453608, 376000.48865979, 377000.49278351, 378000.49690722, 379000.50103093, 380000.50515464, 381000.50927835, 382000.51340206, 383000.51752577, 384000.52164948, 385000.5257732 , 386000.52989691, 387000.53402062, 388000.53814433, 389000.54226804, 390000.54639175, 391000.55051546, 392000.55463918, 393000.55876289, 394000.5628866 , 395000.56701031, 396000.57113402, 397000.57525773, 398000.57938144, 399000.58350515, 400000.58762887, 401000.59175258, 402000.59587629, 403000.6 , 404000.60412371, 405000.60824742, 406000.61237113, 407000.61649485, 408000.62061856, 409000.62474227, 410000.62886598, 411000.63298969, 412000.6371134 , 413000.64123711, 414000.64536082, 415000.64948454, 416000.65360825, 417000.65773196, 418000.66185567, 419000.66597938, 420000.67010309, 421000.6742268 , 422000.67835052, 423000.68247423, 424000.68659794, 425000.69072165, 426000.69484536, 427000.69896907, 428000.70309278, 429000.70721649, 430000.71134021, 431000.71546392, 432000.71958763, 433000.72371134, 434000.72783505, 435000.73195876, 436000.73608247, 437000.74020619, 438000.7443299 , 439000.74845361, 440000.75257732, 441000.75670103, 442000.76082474, 443000.76494845, 444000.76907216, 445000.77319588, 446000.77731959, 447000.7814433 , 448000.78556701, 449000.78969072, 450000.79381443, 451000.79793814, 452000.80206186, 453000.80618557, 454000.81030928, 455000.81443299, 456000.8185567 , 457000.82268041, 458000.82680412, 459000.83092784, 460000.83505155, 461000.83917526, 462000.84329897, 463000.84742268, 464000.85154639, 465000.8556701 , 466000.85979381, 467000.86391753, 468000.86804124, 469000.87216495, 470000.87628866, 471000.88041237, 472000.88453608, 473000.88865979, 474000.89278351, 475000.89690722, 476000.90103093, 477000.90515464, 478000.90927835, 479000.91340206, 480000.91752577, 481000.92164948, 482000.9257732 , 483000.92989691, 484000.93402062, 485000.93814433, 486000.94226804, 487000.94639175, 488000.95051546, 489000.95463918, 490000.95876289, 491000.9628866 , 492000.96701031, 493000.97113402, 494000.97525773, 495000.97938144, 496000.98350515, 497000.98762887, 498000.99175258, 499000.99587629, 500001. ]), &lt;BarContainer object of 485 artists&gt;) . . Check docs on how to detect picks : . Finding peaks in the histograms of the variables : https://www.kaggle.com/simongrest/finding-peaks-in-the-histograms-of-the-variables . | Peak-finding algorithm for Python/SciPy : https://stackoverflow.com/questions/1713335/peak-finding-algorithm-for-python-scipy . | . Experimenting with Attribute Combinations . We identified a few data quirks that we may want to clean up before feeding the data to a Machine Learning algorithm, | We found interesting correlations between attributes, in particular with the target attribute. | We also noticed that some attributes have a tail-heavy distribution, so you may want to transform them (e.g., by computing their logarithm). | One last thing we may want to do before preparing the data for Machine Learning algorithms is to try out various attribute combinations : For example, the total number of rooms in a district is not very useful if we don’t know how many households there are. What we really want is the number of rooms per household. Similarly, the total number of bedrooms by itself is not very useful: you probably want to compare it to the number of rooms. And the population per household also seems like an interesting attribute combination to look at. Let’s create these new attributes: | . # We can see that some attributes are very linked to each others housing[[&#39;total_rooms&#39;,&#39;total_bedrooms&#39;,&#39;households&#39;,&#39;population&#39; ]].corr() total_rooms total_bedrooms households population total_rooms 1.000000 0.930380 0.918484 0.857126 total_bedrooms 0.930380 1.000000 0.979728 0.877747 households 0.918484 0.979728 1.000000 0.907222 population 0.857126 0.877747 0.907222 1.000000 . To highlight the matrix correlation, we can use heatmap from seaborn: . import seaborn as sns cor= housing[[&#39;total_rooms&#39;,&#39;total_bedrooms&#39;,&#39;households&#39;,&#39;population&#39; ]].corr() sns.heatmap(cor, cmap=&#39;Blues&#39;, annot= True) &lt;AxesSubplot:&gt; . . housing[&quot;rooms_per_household&quot;] = housing[&quot;total_rooms&quot;]/housing[&quot;households&quot;] housing[&quot;bedrooms_per_room&quot;] = housing[&quot;total_bedrooms&quot;]/housing[&quot;total_rooms&quot;] housing[&quot;population_per_household&quot;]=housing[&quot;population&quot;]/housing[&quot;households&quot;] # nbre of person per houshold housing[&quot;bedrooms_per_room&quot;].describe() count 20433.000000 mean 0.213039 std 0.057983 min 0.100000 25% 0.175427 50% 0.203162 75% 0.239821 max 1.000000 Name: bedrooms_per_room, dtype: float64 # on average, we have 21 bedrooms for 100 rooms from fractions import Fraction z = Fraction(0.21).limit_denominator() z Fraction(21, 100) corr_matrix = housing.corr() corr_matrix[&#39;median_house_value&#39;].sort_values(ascending = False) median_house_value 1.000000 median_income 0.688075 rooms_per_household 0.151948 total_rooms 0.134153 housing_median_age 0.105623 households 0.065843 total_bedrooms 0.049686 population_per_household -0.023737 population -0.024650 longitude -0.045967 latitude -0.144160 bedrooms_per_room -0.255880 Name: median_house_value, dtype: float64 . The new bedrooms_per_room attribute is much more correlated (0.25)with the median house value than the total number of rooms(0.13) or bedrooms (0.04) : . Apparently houses with a lower bedroom/room ratio tend to be more expensive. | The number of rooms per household is also more informative than the total number of rooms in a district—obviously the larger the houses, the more expensive they are. | . Prepare the Data for Machine Learning Algorithms . let’s revert to a clean training set (by copying strat_train_set once again). | Let’s also separate the predictors and the labels, since we don’t necessarily want to apply the same transformations to the predictors and the target values. | . # drop() creates a copy of the data and does not affect strat_train_set housing = strat_train_set.drop(&quot;median_house_value&quot;, axis=1) housing_labels = strat_train_set[&quot;median_house_value&quot;].copy() housing = strat_train_set.drop(&#39;median_house_value&#39;, axis = 1) housing_labels = strat_train_set[&#39;median_house_value&#39;].copy() . Data Cleaning . Formissing values (like for total_bedrooms), we have three options: . Get rid of the corresponding districts. | Get rid of the whole attribute. | Set the values to some value (zero, the mean, the median, etc.) | We can accomplish these easily using DataFrame’s dropna(), drop(), and fillna() . housing[&#39;total_bedrooms&#39;].describe() count 16354.000000 mean 534.914639 std 412.665649 min 2.000000 25% 295.000000 50% 433.000000 75% 644.000000 max 6210.000000 Name: total_bedrooms, dtype: float64 # in bar chart, NanN&#39;s are filled with 0&#39;s plt.hist(housing[&#39;total_bedrooms&#39;] , bins = int ((6210.000000 - 2.000000 )/500) , color = &#39;blue&#39; , edgecolor = &#39;black&#39; ) (array([1.0221e+04, 4.7670e+03, 9.1400e+02, 2.6100e+02, 9.9000e+01, 5.1000e+01, 1.7000e+01, 1.1000e+01, 7.0000e+00, 3.0000e+00, 2.0000e+00, 1.0000e+00]), array([2.00000000e+00, 5.19333333e+02, 1.03666667e+03, 1.55400000e+03, 2.07133333e+03, 2.58866667e+03, 3.10600000e+03, 3.62333333e+03, 4.14066667e+03, 4.65800000e+03, 5.17533333e+03, 5.69266667e+03, 6.21000000e+03]), &lt;BarContainer object of 12 artists&gt;) . . # to count the number of Nan&#39;s housing[&#39;total_bedrooms&#39;].isna().sum() 158 housing.isna().sum() longitude 0 latitude 0 housing_median_age 0 total_rooms 0 total_bedrooms 158 population 0 households 0 median_income 0 ocean_proximity 0 income_cat 0 dtype: int64 # option 1 : Get rid of the corresponding districts housing.dropna(subset = [&#39;total_bedrooms&#39;]) # drop from 16512 to 16354 using len() # option 2 : Get rid of the whole attribute housing.drop(&#39;total_bedrooms&#39;, axis = 1) # option 3 : Set the values to some value (zero, the mean, the median, etc.) median = housing[&#39;total_bedrooms&#39;].median() housing[&#39;total_bedrooms&#39;].fillna(median, inplace = True) longitude latitude housing_median_age total_rooms population 12655 -121.46 38.52 29.0 3873.0 2237.0 15502 -117.23 33.09 7.0 5320.0 2015.0 2908 -119.04 35.37 44.0 1618.0 667.0 14053 -117.13 32.75 24.0 1877.0 898.0 20496 -118.70 34.28 27.0 3536.0 1837.0 ... ... ... ... ... ... 15174 -117.07 33.03 14.0 6665.0 2026.0 12661 -121.42 38.51 15.0 7901.0 4769.0 19263 -122.72 38.44 48.0 707.0 458.0 19140 -122.70 38.31 14.0 3155.0 1208.0 19773 -122.14 39.97 27.0 1079.0 625.0 households median_income ocean_proximity income_cat 12655 706.0 2.1736 INLAND 2 15502 768.0 6.3373 NEAR OCEAN 5 2908 300.0 2.8750 INLAND 2 14053 483.0 2.2264 NEAR OCEAN 2 20496 580.0 4.4964 &lt;1H OCEAN 3 ... ... ... ... ... 15174 1001.0 5.0900 &lt;1H OCEAN 4 12661 1418.0 2.8139 INLAND 2 19263 172.0 3.1797 &lt;1H OCEAN 3 19140 501.0 4.1964 &lt;1H OCEAN 3 19773 197.0 3.1319 INLAND 3 [16512 rows x 9 columns] . For ‘option’ 3 : fill missings with some value, the median for example, we should : . Compute the median value on the training and use it to fill the missing values in the training set. | Save the median value that you have computed. | Using later for to replace missing values in the test set when we want to evaluate our system | Using it once the system goes live to replace missing values in new data. | . Scikit-Learn provides a handy class to take care of missing values: SimpleImputer. . from sklearn.impute import SimpleImputer . First, you need to create a SimpleImputer instance, specifying that we want to replace each numeric attribute’s missing values with the median of that attribute : . imputer = SimpleImputer(strategy = &#39;median&#39;) housing.dtypes longitude float64 latitude float64 housing_median_age float64 total_rooms float64 total_bedrooms float64 population float64 households float64 median_income float64 ocean_proximity object dtype: object # We drop the categorical variables since the median can only be computed on numerical attributes housing_num = housing.drop([&#39;ocean_proximity&#39;], axis = 1) . Now you can fit the imputer instance to the training data using the fit() method . imputer.fit(housing_num) SimpleImputer(strategy=&#39;median&#39;) . The imputer has simply computed the median of each attribute and stored the result in its statistics_ instance variable. We apply the imputer to all the numerical attributes : . imputer.statistics_ array([-118.51 , 34.26 , 29. , 2119. , 433. , 1164. , 408. , 3.54155]) housing_num.columns Index([&#39;longitude&#39;, &#39;latitude&#39;, &#39;housing_median_age&#39;, &#39;total_rooms&#39;, &#39;total_bedrooms&#39;, &#39;population&#39;, &#39;households&#39;, &#39;median_income&#39;], dtype=&#39;object&#39;) # equivalant to imputer.statistics_ : we have the median for each numeric variable housing_num.median().values array([-118.51 , 34.26 , 29. , 2119. , 433. , 1164. , 408. , 3.54155]) . Now we can use this trained imputer to transform the training set by replacing missing values with the learned medians: . # The result is a plain NumPy array containing the transformed features. X = imputer.transform(housing_num) #If you want to put it back into a pandas DataFrame, it’s simple: housing_tr = pd.DataFrame(X, columns=housing_num.columns, index=housing_num.index) . Alternative method to fit() and transform() method is using directy fit_transform() method . X = imputer.fit_transform(housing_num) housing_tr = pd.DataFrame( X, columns = housing_num.columns , index = housing_num.index) housing_tr longitude latitude housing_median_age total_rooms total_bedrooms 12655 -121.46 38.52 29.0 3873.0 797.0 15502 -117.23 33.09 7.0 5320.0 855.0 2908 -119.04 35.37 44.0 1618.0 310.0 14053 -117.13 32.75 24.0 1877.0 519.0 20496 -118.70 34.28 27.0 3536.0 646.0 ... ... ... ... ... ... 15174 -117.07 33.03 14.0 6665.0 1231.0 12661 -121.42 38.51 15.0 7901.0 1422.0 19263 -122.72 38.44 48.0 707.0 166.0 19140 -122.70 38.31 14.0 3155.0 580.0 19773 -122.14 39.97 27.0 1079.0 222.0 population households median_income 12655 2237.0 706.0 2.1736 15502 2015.0 768.0 6.3373 2908 667.0 300.0 2.8750 14053 898.0 483.0 2.2264 20496 1837.0 580.0 4.4964 ... ... ... ... 15174 2026.0 1001.0 5.0900 12661 4769.0 1418.0 2.8139 19263 458.0 172.0 3.1797 19140 1208.0 501.0 4.1964 19773 625.0 197.0 3.1319 [16512 rows x 8 columns] . Handling Text and Categorical Attributes . So far we have only dealt with numerical attributes, but now let’s look at text attributes. In this dataset, there is just one: the ocean_proximity attribute. Let’s look at its value for the first 10 instances: . housing_cat = housing[[&#39;ocean_proximity&#39;]] housing_cat.head(10) housing_cat.nunique() ocean_proximity 5 dtype: int64 housing[&#39;ocean_proximity&#39;].unique() array([&#39;INLAND&#39;, &#39;NEAR OCEAN&#39;, &#39;&lt;1H OCEAN&#39;, &#39;NEAR BAY&#39;, &#39;ISLAND&#39;], dtype=object) . It’s not arbitrary text: there are a limited number of possible values, each of which represents a category. So this attribute is a categorical attribute. Most Machine Learning algorithms prefer to work with numbers, so let’s convert these categories from text to numbers. For this, we can use Scikit-Learn’s OrdinalEncoder class : . from sklearn.preprocessing import OrdinalEncoder ordinal_encoder = OrdinalEncoder() housing_cat_encoded = ordinal_encoder.fit_transform(housing_cat) # categorical dataframe housing_cat_encoded[:10] array([[1.], [4.], [1.], [4.], [0.], [3.], [0.], [0.], [0.], [0.]]) housing_cat_encoded.shape (16512, 1) # we can get the list of categories using the categories_ instance variable. It is a list containing a 1D array #of categories for each categorical attribute ordinal_encoder.categories_ [array([&#39;&lt;1H OCEAN&#39;, &#39;INLAND&#39;, &#39;ISLAND&#39;, &#39;NEAR BAY&#39;, &#39;NEAR OCEAN&#39;], dtype=object)] np.unique(housing_cat_encoded) array([0., 1., 2., 3., 4.]) . One issue with this representation is that ML algorithms ( OrdinaEncoder) will assume that two nearby values are more similar than two distant values. This may be fine in some cases (e.g., for ordered categories such as “bad,” “average,” “good,” and “excellent”) : ordinal encoding for categorical variables that have a natural rank ordering but it is obviously not the case for the ocean_proximity column (for example, categories 0 and 4 are clearly more similar than categories 0 and 1). . To fix this issue, a common solution is to create one binary attribute per category: one attribute equal to 1 when the category is “&lt;1H OCEAN” (and 0 otherwise), another attribute equal to 1 when the category is “INLAND” (and 0 otherwise), and so on. This is called one-hot encoding. The new attributes are sometimes called dummy attributes. Scikit-Learn provides a OneHotEncoder class to convert categorical values into one-hot vectors . See this blogpost : https://machinelearningmastery.com/one-hot-encoding-for-categorical-data/ . from sklearn.preprocessing import OneHotEncoder cat_encoder = OneHotEncoder() housing_cat_1hot = cat_encoder.fit_transform(housing_cat) housing_cat_1hot &lt;16512x5 sparse matrix of type &#39;&lt;class &#39;numpy.float64&#39;&gt;&#39; with 16512 stored elements in Compressed Sparse Row format&gt; . Notice that the output is a SciPy sparse matrix, instead of a NumPy array. This is very useful when you have categorical attributes with thousands of categories. After onehot encoding, we get a matrix with thousands of columns, and the matrix is full of 0s except for a single 1 per row. Using up tons of memory mostly to store zeros would be very wasteful, so instead a sparse matrix only stores the location of the nonzero elements. we can use it mostly like a normal 2D array,but if we really want to convert it to a (dense) NumPy array, we call the toarray() method: . housing_cat_1hot.toarray() array([[0., 1., 0., 0., 0.], [0., 0., 0., 0., 1.], [0., 1., 0., 0., 0.], ..., [1., 0., 0., 0., 0.], [1., 0., 0., 0., 0.], [0., 1., 0., 0., 0.]]) housing_cat.head(3) ocean_proximity 12655 INLAND 15502 NEAR OCEAN 2908 INLAND # to get the list of categories cat_encoder.categories_ [array([&#39;&lt;1H OCEAN&#39;, &#39;INLAND&#39;, &#39;ISLAND&#39;, &#39;NEAR BAY&#39;, &#39;NEAR OCEAN&#39;], dtype=object)] . If a categorical attribute has a large number of possible categories (e.g., country code, profession, species), then one-hot encoding will result in a large number of input features. This may slow down training and degrade performance. . If this happens, we may want to replace the categorical input with useful numerical features related to the categories: for example, we could replace the ocean_proximity feature with the distance to the ocean (similarly, a country code could be replaced with the country’s population and GDP per capita). Alternatively, we could replace each category with a learnable, low-dimensional vector called an embedding. . Each category’s representation would be learned during training. This is an example of representation learning. . Custom Transformers . retun back for more details ? see blogpost : https://towardsdatascience.com/pipelines-custom-transformers-in-scikit-learn-the-step-by-step-guide-with-python-code-4a7d9b068156 . and this : https://github.com/ageron/handson-ml2/blob/master/02_end_to_end_machine_learning_project.ipynb . housing.values[: ,4:] array([[797.0, 2237.0, 706.0, 2.1736, &#39;INLAND&#39;, 2], [855.0, 2015.0, 768.0, 6.3373, &#39;NEAR OCEAN&#39;, 5], [310.0, 667.0, 300.0, 2.875, &#39;INLAND&#39;, 2], ..., [166.0, 458.0, 172.0, 3.1797, &#39;&lt;1H OCEAN&#39;, 3], [580.0, 1208.0, 501.0, 4.1964, &#39;&lt;1H OCEAN&#39;, 3], [222.0, 625.0, 197.0, 3.1319, &#39;INLAND&#39;, 3]], dtype=object) housing.head(3) longitude latitude housing_median_age total_rooms total_bedrooms 12655 -121.46 38.52 29.0 3873.0 797.0 15502 -117.23 33.09 7.0 5320.0 855.0 2908 -119.04 35.37 44.0 1618.0 310.0 population households median_income ocean_proximity income_cat 12655 2237.0 706.0 2.1736 INLAND 2 15502 2015.0 768.0 6.3373 NEAR OCEAN 5 2908 667.0 300.0 2.8750 INLAND 2 from sklearn.base import BaseEstimator, TransformerMixin rooms_ix, bedrooms_ix, population_ix, households_ix = 3, 4, 5, 6 class CombinedAttributesAdder(BaseEstimator, TransformerMixin): def __init__(self, add_bedrooms_per_room = True): # no *args or **kargs self.add_bedrooms_per_room = add_bedrooms_per_room def fit(self, X, y=None): return self # nothing else to do def transform(self, X): rooms_per_household = X[:, rooms_ix] / X[:, households_ix] population_per_household = X[:, population_ix] / X[:, households_ix] if self.add_bedrooms_per_room: bedrooms_per_room = X[:, bedrooms_ix] / X[:, rooms_ix] return np.c_[X, rooms_per_household, population_per_household, bedrooms_per_room] else: return np.c_[X, rooms_per_household, population_per_household] attr_adder = CombinedAttributesAdder(add_bedrooms_per_room=False ) housing_extra_attribs = attr_adder.transform(housing.values) housing_extra_attribs array([[-121.46, 38.52, 29.0, ..., 2, 5.485835694050992, 3.168555240793201], [-117.23, 33.09, 7.0, ..., 5, 6.927083333333333, 2.6236979166666665], [-119.04, 35.37, 44.0, ..., 2, 5.3933333333333335, 2.223333333333333], ..., [-122.72, 38.44, 48.0, ..., 3, 4.1104651162790695, 2.6627906976744184], [-122.7, 38.31, 14.0, ..., 3, 6.297405189620759, 2.411177644710579], [-122.14, 39.97, 27.0, ..., 3, 5.477157360406092, 3.1725888324873095]], dtype=object) . Feature Scaling . One of the most important transformations you need to apply to your data is feature scaling. With few exceptions, Machine Learning algorithms don’t perform well when the input numerical attributes have very different scales . This is the case for the housing data: total_rooms ranges from about 6 to 39320, while median_income only range from 0 to 15 : . Note that scaling the target values is generally not required. . housing_num.describe().T count mean std min 25% longitude 16512.0 -119.575635 2.001828 -124.3500 -121.80000 latitude 16512.0 35.639314 2.137963 32.5400 33.94000 housing_median_age 16512.0 28.653404 12.574819 1.0000 18.00000 total_rooms 16512.0 2622.539789 2138.417080 6.0000 1443.00000 total_bedrooms 16512.0 533.939438 410.806260 2.0000 296.00000 population 16512.0 1419.687379 1115.663036 3.0000 784.00000 households 16512.0 497.011810 375.696156 2.0000 279.00000 median_income 16512.0 3.875884 1.904931 0.4999 2.56695 50% 75% max longitude -118.51000 -118.010000 -114.3100 latitude 34.26000 37.720000 41.9500 housing_median_age 29.00000 37.000000 52.0000 total_rooms 2119.00000 3141.000000 39320.0000 total_bedrooms 433.00000 641.000000 6210.0000 population 1164.00000 1719.000000 35682.0000 households 408.00000 602.000000 5358.0000 median_income 3.54155 4.745325 15.0001 . There are two common ways to get all attributes to have the same scale: . min-max scaling : ranging from 0 to 1. Scikit-Learn provides a transformer called MinMaxScaler for this. It has a feature_range hyperparameter that lets you change the range if, for some reason, you don’t want 0–1 : https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html . | Standardization : returns has unit variance and unlike min-max scaling, standardization does not bound values to a specific range, which may be a problem for some algorithms (e.g., neural networks often expect an input value ranging from 0 to 1).However, standardization is much less affected by outliers. Scikit-Learn provides a transformer called StandardScaler for standardization : https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html . | . Transformation Pipelines . As we can see, there are many data transformation steps that need to be executed in the right order. Fortunately, Scikit-Learn provides the Pipeline class to help with such sequences of transformations. . Here is a small pipeline for the numerical attributes: . from sklearn.pipeline import Pipeline from sklearn.impute import SimpleImputer from sklearn.preprocessing import StandardScaler,MinMaxScaler # plus the class add attribure that we created . The Pipeline constructor takes a list of name/estimator pairs defining a sequence of steps. All but the last estimator must be transformers (i.e., they must have a fit_transform() method). . # This pipeline is for numeric pipeline, we call is num_pipeline num_pipeline = Pipeline([ (&#39;imputer&#39;, SimpleImputer(strategy = &#39;median&#39;)), #(&#39;attribs_adder&#39;, CombinedAttributesAdder()), (&#39;std_scaler&#39;, StandardScaler()) ]) housing_num_tr = num_pipeline.fit_transform(housing_num) . So far, we have handled the categorical columns and the numerical columns separately. It would be more convenient to have a single transformer able to handle all columns, applying the appropriate transformations to each column. In version 0.20, Scikit-Learn introduced the ColumnTransformer for this purpose, and the good news is that it works great with pandas DataFrames. Let’s use it to apply all the transformations to the housing data: . from sklearn.compose import ColumnTransformer num_attribs = list(housing_num) cat_attribs = [&#39;ocean_proximity&#39;] NameError Traceback (most recent call last) &lt;ipython-input-2-344d101417c2&gt; in &lt;module&gt; -&gt; 1 num_attribs = list(housing_num) 2 cat_attribs = [&#39;ocean_proximity&#39;] NameError: name &#39;housing_num&#39; is not defined full_pipeline = ColumnTransformer([ (&#39;num&#39;, num_pipeline, num_attribs), (&#39;cat&#39;, OneHotEncoder(), cat_attribs) ] ) housing_prepared = full_pipeline.fit_transform(housing) housing_prepared array([[-0.94135046, 1.34743822, 0.02756357, ..., 0. , 0. , 0. ], [ 1.17178212, -1.19243966, -1.72201763, ..., 0. , 0. , 1. ], [ 0.26758118, -0.1259716 , 1.22045984, ..., 0. , 0. , 0. ], ..., [-1.5707942 , 1.31001828, 1.53856552, ..., 0. , 0. , 0. ], [-1.56080303, 1.2492109 , -1.1653327 , ..., 0. , 0. , 0. ], [-1.28105026, 2.02567448, -0.13148926, ..., 0. , 0. , 0. ]]) housing_prepared[0].shape (13,) housing_prepared.shape (16512, 13) . to read carefully for later : . First we import the ColumnTransformer class, next we get the list of numerical column . names and the list of categorical column names, and then we construct a Colum nTransformer. The constructor requires a list of tuples, where each tuple contains a name,22 a transformer, and a list of names (or indices) of columns that the transformer should be applied to. In this example, we specify that the numerical columns should be transformed using the num_pipeline that we defined earlier, and the categorical columns should be transformed using a OneHotEncoder. Finally, we apply this ColumnTransformer to the housing data: it applies each transformer to the appropriate columns and concatenates the outputs along the second axis (the transformers must return the same number of rows). Note that the OneHotEncoder returns a sparse matrix, while the num_pipeline returns a dense matrix. When there is such a mix of sparse and dense matrices, the Colum nTransformer estimates the density of the final matrix (i.e., the ratio of nonzero cells), and it returns a sparse matrix if the density is lower than a given threshold (by default, sparse_threshold=0.3). In this example, it returns a dense matrix. And that’s it! We have a preprocessing pipeline that takes the full housing data and applies the appropriate transformations to each column. Instead of using a transformer, you can specify the string “drop” if you want the columns to be dropped, or you can specify “pass through” if you want the columns to be left untouched. By default, the remaining columns (i.e., the ones that were not listed) will be dropped, but you can set the remainder hyperparameter to any transformer (or to “passthrough”) if you want these columns to be handled differently. If you are using Scikit-Learn 0.19 or earlier, you can use a third-party library such as sklearn-pandas, or you can roll out your own custom transformer to get the same functionality as the ColumnTransformer. Alternatively, you can use the FeatureUnion class, which can apply different transformers and concatenate their outputs. But you cannot specify different columns for each transformer; they all apply to the whole data. It is possible to work around this limitation using a custom transformer for column selection (see the Jupyter notebook for an example). . see link : https://github.com/ageron/handson-ml2/blob/master/02_end_to_end_machine_learning_project.ipynb . Select and Train a Model . we framed the problem | we got the data and explored it | we sampled a training set and a test set | we wrote transformation pipelines to clean up and prepare your data for Machine Learning algorithms automatically. | . Training and Evaluating on the Training Set . Let’s first train a Linear Regression model . housing_prepared.shape (16512, 13) from sklearn.linear_model import LinearRegression lin_reg = LinearRegression() lin_reg.fit(housing_prepared, housing_labels) LinearRegression() . Let’s try it out on a few instances from the training set: . # let&#39;s try the full preprocessing pipeline on a few training instances some_data = housing.iloc[:5] some_labels = housing_labels.iloc[:5] some_data_prepared = full_pipeline.transform(some_data) print(&quot;Predictions:&quot;, lin_reg.predict(some_data_prepared)) Predictions: [ 88983.14806384 305351.35385026 153334.71183453 184302.55162102 246840.18988841] print(&quot;Labels:&quot;, list(some_labels)) Labels: [72100.0, 279600.0, 82700.0, 112500.0, 238300.0] some_data_prepared.shape (5, 13) . Some remarks : If we encotered missing values are in the test set ? ( OneHotEncode() has a paramete : handle_unknown = ‘ignore’) | Indexing and selection data : if we want to modifiy a certain column in the dataframe, we should not proceed in this way : df[‘ocean_proxemity][0]= np.nan but rather copy the dataset first and then df.loc[0,’ocean_proxemity’]=np.nan. Read : https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy | . | . More on sklearn pipelines : https://scikit-learn.org/stable/auto_examples/compose/plot_column_transformer_mixed_types.html . The iloc indexer for Pandas Dataframe is used for integer-location based indexing / selection by position. | The Pandas loc indexer can be used with DataFrames for two different use cases: Selecting rows by label/index | Selecting rows with a boolean / conditional lookup | . | . Read : https://www.shanelynn.ie/pandas-iloc-loc-select-rows-and-columns-dataframe/ . Let’s measure this regression model’s RMSE on the whole training set using Scikit-Learn’s mean_squared_error() function : . from sklearn.metrics import mean_squared_error # housing_predictions = lin_reg.predict(housing_prepared) housing_predictions[:4] array([ 88983.14806384, 305351.35385026, 153334.71183453, 184302.55162102]) housing_labels 12655 72100.0 15502 279600.0 2908 82700.0 14053 112500.0 20496 238300.0 ... 15174 268500.0 12661 90400.0 19263 140400.0 19140 258100.0 19773 62700.0 Name: median_house_value, Length: 16512, dtype: float64 #RMSE lin_rmse = mean_squared_error(housing_predictions,housing_labels, squared= False) lin_rmse 69050.56219504567 . Most districts’ median_housing_values range between $120,000 and $265,000, so a typical prediction error of $69,050 is not very satisfying: . housing_labels.describe() count 16512.000000 mean 207005.322372 std 115701.297250 min 14999.000000 25% 119800.000000 50% 179500.000000 75% 263900.000000 max 500001.000000 Name: median_house_value, dtype: float64 . This is an example of a model underfitting the training data : When this happens it can mean that the features do not provide enough information to make good predictions, or that the model is not powerful enough. As we saw in the previous chapter, the main ways to fix underfitting are to select a more powerful model, to feed the training algorithm with better features, or to reduce the constraints on the model. . Let’s train a DecisionTreeRegressor. This is a powerful model, capable of finding complex nonlinear relationships in the data | . from sklearn.tree import DecisionTreeRegressor tree_reg = DecisionTreeRegressor() tree_reg.fit(housing_prepared, housing_labels) DecisionTreeRegressor() #Now that the model is trained, let’s evaluate it on the training set: housing_predictions = tree_reg.predict(housing_prepared) tree_rmse = mean_squared_error(housing_labels, housing_predictions, squared=False) tree_rmse 0.0 . As we saw earlier, we don’t want to touch the test set until we are ready to launch a model we are confident about, so we need to use part of the training set for training and part of it for model validation. . Better Evaluation Using Cross-Validation . One way to evaluate the Decision Tree model would be to use the train_test_split() function to split the training set into a smaller training set and a validation set, then train our models against the smaller training set and evaluate them against the validation set. . | A great alternative is to use Scikit-Learn’s K-fold cross-validation feature : The following code randomly splits the training set into 10 distinct subsets called folds, then it trains and evaluates the Decision Tree model 10 times, picking a different fold for evaluation every time and training on the other 9 folds. The result is an array containing the 10 evaluation scores: . | . from sklearn.model_selection import cross_val_score # to get the &#39;scoring&#39; options, use ssorted(sklearn.metrics.SCORERS.keys()) scores = - cross_val_score(tree_reg, housing_prepared, housing_labels, scoring=&#39;neg_root_mean_squared_error&#39;, cv=10) scores array([71270.15951523, 68888.32011559, 64997.85188763, 69263.03318422, 68197.14503697, 68963.98885461, 73536.17215975, 69183.4936482 , 66243.08004208, 71783.50940468]) . to get the ‘scoring’ options, use ssorted(sklearn.metrics.SCORERS.keys()) | Scikit-Learn’s cross-validation features expect a utility function (greater is better) rather than a cost function (lower is better), so the scoring function is actually the opposite of the MSE (i.e., a negative value) | . # display the results def display_scores(scores): print(&#39;Scores :&#39;,scores) print(&#39;Mean :&#39;,scores.mean()) print(&#39;Standard deviation :&#39;,scores.std()) display_scores(scores) Scores : [71270.15951523 68888.32011559 64997.85188763 69263.03318422 68197.14503697 68963.98885461 73536.17215975 69183.4936482 66243.08004208 71783.50940468] Mean : 69232.67538489516 Standard deviation : 2394.0765898258674 . Cross-validation allows you to get not only an estimate of the performance of your model, but also a measure of how precise this estimate is (i.e., its standard deviation). The Decision Tree has a score of approximately 69,232, generally ±2,394. We would not have this information if we just used one validation set. | Let’s compute the same scores for the Linear Regression model just to be sure: | . lin_scores = - cross_val_score(lin_reg,housing_prepared, housing_labels, scoring=&#39;neg_root_mean_squared_error&#39;, cv = 10) lin_scores array([72229.03469752, 65318.2240289 , 67706.39604745, 69368.53738998, 66767.61061621, 73003.75273869, 70522.24414582, 69440.77896541, 66930.32945876, 70756.31946074]) display_scores(lin_scores) Scores : [72229.03469752 65318.2240289 67706.39604745 69368.53738998 66767.61061621 73003.75273869 70522.24414582 69440.77896541 66930.32945876 70756.31946074] Mean : 69204.32275494766 Standard deviation : 2372.07079105592 . The Decision Tree model is overfitting so badly that it performs worse than the Linear Regression model. . Let’s try one last model now: the RandomForestRegressor : Random Forests work by training many Decision Trees on random subsets of the features, then averaging out their predictions. Building a model on top of many other models is called Ensemble Learning, and it is often a great way to push ML algorithms even further. . from sklearn.ensemble import RandomForestRegressor forest_reg = RandomForestRegressor() forest_reg.fit(housing_prepared, housing_labels) RandomForestRegressor() forest_reg_scores = - cross_val_score(forest_reg,housing_prepared, housing_labels, scoring=&#39;neg_root_mean_squared_error&#39;, cv = 10) forest_reg_scores array([50311.08798022, 49043.69572163, 46081.95238283, 50467.56214907, 47657.84153211, 49419.96274189, 51772.42197545, 49030.57976501, 47498.17482111, 53167.33074077]) display_scores(forest_reg_scores) Scores : [50311.08798022 49043.69572163 46081.95238283 50467.56214907 47657.84153211 49419.96274189 51772.42197545 49030.57976501 47498.17482111 53167.33074077] Mean : 49445.06098101039 Standard deviation : 1992.3842490271882 forest_predictions = forest_reg.predict(housing_prepared) forst_rmse = mean_squared_error(housing_labels, forest_predictions, squared=False) forst_rmse 18266.74368085342 . Note that the score on the training set(18,266) is still much lower than on the validation sets(49,445 +/-1992.38), meaning that the model is still overfitting the training set. . We should save every model we experiment with so that er can come back easily to any model you want. Make sure you save both the hyperparameters and the trained parameters, as well as the cross-validation scores and perhaps the actual predictions as well. This will allow you to easily compare scores across model types, and compare the types of errors they make. . | We can easily save Scikit-Learn models by using Python’s pickle module or by using the joblib library, which is more efficient at serializing large NumPy arrays (you can install this library using pip): . | . pip install joblib Requirement already satisfied: joblib in /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages (1.1.0) Note: you may need to restart the kernel to use updated packages. import joblib # random forest : joblib.dump(forest_reg,&#39;rmd_forest.pkl&#39;) # saving the model as pkl file and named &#39;rmd_forest.pkl model_reload = joblib.load(&#39;rmd_forest.pkl&#39;) # loading the model rmd_forest_prediction = model_reload.predict(housing_prepared) # saving the predictions rmd_forest_rmse = mean_squared_error(rmd_forest_prediction, housing_labels) # saving the rmse on the train test to check overfit rmd_forest_cross_validation = -cross_val_score(model_reload, housing_prepared, housing_labels, scoring=&#39;neg_root_mean_squared_error&#39;, cv = 10) # rmse on validation to check overfit display_scores(rmd_forest_cross_validation) # cross validation score Scores : [50992.51555592 49288.84220573 46237.11091931 50248.85062075 47806.3116179 49272.797347 51801.0468531 48800.83283468 47540.31917616 53091.63650718] Mean : 49508.0263637728 Standard deviation : 1972.8867918884973 . Fine-Tune Our Model . Grid Search . Using Scikit-Learn’s GridSearchCV, All we need to do is tell it which hyperparameters we want it to experiment with and what valuesto try out, and it will use cross-validation to evaluate all the possible combinations of hyperparameter values : . from sklearn.model_selection import GridSearchCV param_grid = [ {&#39;n_estimators&#39;: [3, 10, 30], &#39;max_features&#39;: [2, 4, 6, 8]}, {&#39;bootstrap&#39;: [False], &#39;n_estimators&#39;: [3, 10], &#39;max_features&#39;: [2, 3, 4]}, ] forest_reg = RandomForestRegressor() grid_search = GridSearchCV(forest_reg, param_grid, cv=5, scoring=&#39;neg_root_mean_squared_error&#39;, return_train_score = True) grid_search.fit(housing_prepared, housing_labels) GridSearchCV(cv=5, estimator=RandomForestRegressor(), param_grid=[{&#39;max_features&#39;: [2, 4, 6, 8], &#39;n_estimators&#39;: [3, 10, 30]}, {&#39;bootstrap&#39;: [False], &#39;max_features&#39;: [2, 3, 4], &#39;n_estimators&#39;: [3, 10]}], return_train_score=True, scoring=&#39;neg_root_mean_squared_error&#39;) ?GridSearchCV . This param_grid tells Scikit-Learn to first evaluate all 3 × 4 = 12 combinations of n_estimators and max_features hyperparameter values specified in the first dict. -Then try all 2 × 3 = 6 combinations of hyperparameter values in the second dict, but this time with the bootstrap hyperparameter set to False instead of True | The grid search will explore 12 + 6 = 18 combinations of `RandomForestRegressor hyperparameter values`` | And it will train each model 5 times (cv=5).In other words, all in all, there will be 18 × 5 = 90 rounds of training. | . We can get the best combination of parameters like this : . grid_search.best_params_ #we should probably try searching again with higher values; the score may continue to improve. {&#39;max_features&#39;: 8, &#39;n_estimators&#39;: 30} grid_search.best_estimator_ RandomForestRegressor(max_features=8, n_estimators=30) cvres = grid_search.cv_results_ for mean_score, params in zip(cvres[&quot;mean_test_score&quot;], cvres[&quot;params&quot;]): print(-mean_score, params) 64530.85647414619 {&#39;max_features&#39;: 2, &#39;n_estimators&#39;: 3} 55067.77832337284 {&#39;max_features&#39;: 2, &#39;n_estimators&#39;: 10} 52781.7167175866 {&#39;max_features&#39;: 2, &#39;n_estimators&#39;: 30} 60327.066875895776 {&#39;max_features&#39;: 4, &#39;n_estimators&#39;: 3} 52586.95798629394 {&#39;max_features&#39;: 4, &#39;n_estimators&#39;: 10} 50346.63948997191 {&#39;max_features&#39;: 4, &#39;n_estimators&#39;: 30} 58398.87657548992 {&#39;max_features&#39;: 6, &#39;n_estimators&#39;: 3} 52075.368300249116 {&#39;max_features&#39;: 6, &#39;n_estimators&#39;: 10} 50093.621910952315 {&#39;max_features&#39;: 6, &#39;n_estimators&#39;: 30} 58628.430409285626 {&#39;max_features&#39;: 8, &#39;n_estimators&#39;: 3} 51936.797178963665 {&#39;max_features&#39;: 8, &#39;n_estimators&#39;: 10} 50089.501357132904 {&#39;max_features&#39;: 8, &#39;n_estimators&#39;: 30} 62303.627420601595 {&#39;bootstrap&#39;: False, &#39;max_features&#39;: 2, &#39;n_estimators&#39;: 3} 54183.89357722118 {&#39;bootstrap&#39;: False, &#39;max_features&#39;: 2, &#39;n_estimators&#39;: 10} 60004.47703668058 {&#39;bootstrap&#39;: False, &#39;max_features&#39;: 3, &#39;n_estimators&#39;: 3} 52603.81684475204 {&#39;bootstrap&#39;: False, &#39;max_features&#39;: 3, &#39;n_estimators&#39;: 10} 58002.985249363366 {&#39;bootstrap&#39;: False, &#39;max_features&#39;: 4, &#39;n_estimators&#39;: 3} 52121.979634950054 {&#39;bootstrap&#39;: False, &#39;max_features&#39;: 4, &#39;n_estimators&#39;: 10} . Rq: Don’t forget that we can treat some of the data preparation steps as hyperparameters. The grid search will automatically find out whether or not to add a feature you were not sure about. Ex : using the add_bedrooms_per_room hyperparameter of your CombinedAttributesAdder transformer). . .",
            "url": "https://younesszaim.github.io/myportfolio/2021/11/15/Sickit-Learn-for-Machine-Learning.html",
            "relUrl": "/2021/11/15/Sickit-Learn-for-Machine-Learning.html",
            "date": " • Nov 15, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Scoring In Floa Bank",
            "content": ". Real-time credit scoring in FLOA Bank . by . Youness ZAIM . Under the guidance of . Anais LAPEYRE &amp; Christine THOMAS . Toulouse School of Economics . Master 2 Econometrics &amp; Statistics . Septembre 2021 . Toulouse School of Economics . Real-time credit scoring in FLOA Bank . Summary . Within the framework of the Master 2 Econometrics and Statistics of Toulouse School of Economics, this report aims to review and assess methods for constructing a customized credit scoring model for online credit. We examine the process-flow involved in the scorecard modeling and highlight the probable causes of weak model performance. Such examination includes theoretical and empirical assessment of data quality, variable transformation and selection based on automated binning algorithms, statistical tests for model performances to assess its validity and practicality for implementation. . Acknowledgements . With boundless excitement and appreciation, I would like to express my sincere gratitude to the people who helped for the successful completion of my apprentice report: . To my supervisor, Anais LAPEYRE, for hiring me as an intern and giving me the opportunity to do my apprenticeship in FLOA Bank, for the continuous support during my missions, for her generous advice, immense knowledge and encouragement. I could not have imagined having a better mentor and role model. . | To my academic tutor, Christine THOMAS, for her guidance throughout the year and all members of University of Toulouse for the quality of their teaching and their daily efforts in guiding us towards a successful professional insertion. . | To my manager, Virginie LANGE, for her instructions and recommendations. Her cheerful and vibrant personality has always been my inspiration. . | To the beloved FLOA’s data science team: . Joanna MORAIS for all her kind support, genuine advice, and patience. . | Vincent GALLMANN, for his generous support and guidance through the modelling process. His kindness will always be remembered and appreciated. . | Minh-Tri NGUYEN for all the fond memories and kind support. . | . | . I would like to express my warmest thanks to all those who have contributed in all ways to the accomplishment of this work. . Contents . # . I. Introduction 6 . 1- Presentation 6 . 2- Organization of FLOA Bank 8 . 3- Data platform 9 . 3.1 Data storage 9 . 3.2 Data ingestion 11 . 3.3 Exploitation and processing of the data 12 . II. Credit Scoring 15 . 1- Types of credit scores 15 . 2- Generic vs customized scoring models 16 . 3- Coup de pouce UPCYCLE 16 . 4- Monitoring machine learning models in production 21 . III. UPCYCLE credit score development 27 . 1- Goal definition 27 . 2- Data collection &amp; preparation 28 . 2.1 Data collection 28 . 2.2 Data analysis 33 . 3- Feature engineering 37 . 3.1 Binning and weights of evidence 37 . 3.2 Manuel binning 41 . 3.3 Cluster Binning for Categorical Variables 42 . 3.4 Time Features 43 . 3.5 Hamming distance 44 . 3.6 Impact coding 44 . 3.7 Standard normalization 45 . 3.8 Missing values 45 . IV. Modeling 46 . 1- Predictive power 46 . 2- Performance metrics 48 . 3- Probabilities and lift 49 . 4- Hyperparameter tuning 51 . 5- Machine learning algorithms 51 . 5.1 Random Forest 52 . 5.2 Gradient Boosted Trees 52 . 5.3 Logistic Regression 53 . 6- Experiment results 54 . 6.1 Confusion matrix and decision chart 54 . 6.2 Lift curve 57 . 6.3 ROC Curve 60 . 6.4 Detailed metrics 63 . V. Interpretation 66 . 1- Feature importance 66 . 2- Partial dependence 67 . 3- Subpopulation analysis 70 . 4- Individual analysis 72 . VI. Conclusion 73 . VII. Appendix 74 . VIII. Bibliography 75 . Introduction . Presentation . FLOA Bank is a French leader for web and mobile payment solutions by making consumers’ lives easier through payment facilities, instant loans and bank cards. FLOA is also partner to large e-tailers (Cdiscount, Oscaro, SFR, Vide dressing, etc.), key players in tourism (Selectour, Misterfly, Cdiscount Voyages, Pierre et Vacances, etc.), and fintechs (Lydia, Bankin’), for which it creates bespoke services. Its two retail banking brands are FLOA Bank for its B2C customers and FLOA Pay for its B2B partners. FLOA’s products and services stand out, as they are easy to use for customers and quick to integrate for partners. FLOA boasts more than 3 million customers and grants more than €2 billion in loans for goods and services every year. In France, FLOA was named Best Customer Service Company 2021 (Service Client de l’Année 2021). With operations in France, Spain and Belgium, FLOA aims to become one of Europe’s leading providers of payment solutions. . In an e-commerce market experiencing very strong growth, associated with changing consumer habits and customer expectations with regard to payment methods, BNP Paribas, Casino Group and Crédit Mutuel Alliance Fédérale signed an exclusivity agreement for the sale of FLOA to BNP Paribas by Casino Group and Crédit Mutuel Alliance Fédérale, setting up a strategic partnership between BNP Paribas and Casino Group. This will enable FLOA to initiate a new development cycle, capitalizing on BNP Paribas’ expertise and areas of business, particularly in view of European deployment. . FLOA Bank presents a range of innovative products divided into 4 main categories: . Online credit: Personal loan, revolving credit and credit insurance. More information on all the online credits: https://www.floabank.fr/credits/gamme-credits . | Bank cards: Casino bank card, Cdiscount MasterCard, gold card. More info on all bank cards: https://www.floabank.fr/mastercard/gamme-cartes . | Insurances: Credit and cards, family and leisure, health and providence. More info on all insurances: https://www.floabank.fr/assurances/gamme-assurances . | Payment facilities: Payment in several instalments (CB3X, CB4X and CB10X) on partner sites, instant mini-loans also called “Coup de pouce” (directly or via partnerships such as Lydia, Bankin, Cdiscount). . The “Coup de Pouce” credit allows Internet users to benefit from a loan repayable in less than 90 days in 3 or 4 times (3 or 4 monthly payments), with a minimum of administrative procedures and a minimum of supporting documents. This credit offer is characterized by the flexibility of managing payments with digressive fees according to the amount of the Helping Hand. They vary from 1.48% to a maximum of 3.13%. In practice: . For a ‘Coup de Pouce’ in 3 instalments: You pay the costs of your loan on the day of your application and then pay the total amount borrowed in 3 constant instalments over a period of 90 days. | . | . | . R1 = Credit fees on the day of the order D . R2 = 2nd instalment (1/3 of the amount requested) =&gt; D + 30 days . R3 = 3rd instalment (1/3 of the amount requested) =&gt; D + 60 days . R4 = 4th instalment (1/3 of the amount requested) =&gt; D + 90 days . For a ‘Coup de Pouce’ in 4 instalments: You pay the costs of your credit on the day of your request and then you pay the total amount borrowed in 4 constant instalments over a period of 90 days. | . R1 = Credit charges + 1st instalment (1/4 of the amount requested) on the day of the order D R2 = 2nd instalment (1/4 of the amount requested) =&gt; D + 30 days . R3 = 3rd instalment (1/4 of the amount requested) =&gt; D + 60 days . R4 = 4th instalment (1/4 of the amount requested) =&gt; D + 90 days . An example to illustrate how the “Coup de Pouce “ works: If your mini-loan is 600 €, you pay an amount of 18,78 €, the day of the order. Then, you pay a constant monthly payment of 200 € during the next 3 months. In total, your credit costs 618.78 €. More information on Coup de Pouce :https://www.moncoupdepouce.com/ . Payment in instalments (Le « paiement en plusieurs fois ») allows Internet users who are customers of FLOA’s partner merchant site to pay for their purchases in one, three or four instalments (one, three or four monthly instalments) with their bank card. The Customer’s repayment schedule will begin on the day the order is validated and will be spread out as follows: . For the “1 X deferred” and “1 X deferred FREE” Payment Service: | . | . R1 = 1st instalment on D + freely chosen period of time up to a maximum of 30 days. . For the “3 X” and “3 X FREE” payment service: | . R1 = 1st due date on D R2 = 2nd due date =&gt; D + 30 days R3 = 3rd due date =&gt; D+ 60 days . For the “3 X deferred” payment service: | . R1 = 1st due date on D + 30 days R2 = 2nd due date =&gt; D+ 60 days R3 = 3rd due date =&gt; D + 90 days . For the “4 X” and “4 X FREE” payment service: | . R1 = 1st due date on D R2 = 2nd due date =&gt; D + 30 days R3 = 3rd due date =&gt; D + 60 days R4 = 4th due date =&gt; D + 90 days . An example for a purchase on one of FLOA’s partner sites for an amount of 1500€. If you use the CB3X without fees proposed by FLOA Bank, a contribution corresponding to one third of the order will be requested on the day of the order. Two monthly payments corresponding to one third of the order will be taken 30 and 60 days later. No management or file fees: More information on all payment facilities: https://www.floabank.fr/solution-paiement/gamme-solution-paiement . Organization of FLOA Bank . ## . The Data Department in FLOA Bank is composed of four teams: . Data Factory under the responsibility of Benjamin Laurent: The team is in charge of feeding all data (partners, back &amp; front IS, repositories, etc.) on the data platform (Snowflake), industrializing data flows (Talend) and setting up an enterprise data model. . | BI &amp; Data Analytics under the responsibility of Zouheir Azahaf: The team is in charge of the industrialization of reporting in Tableau, BI analysis for the different departments of the company, and support for the business lines on their data challenges, especially on the specification of data marts. . | Data Science under the responsibility of Virginie Lange: The team is in charge of all predictive modeling for the company (risk, fraud, collection, finance, marketing, etc.) and in particular the implementation of real-time scoring on our customer paths. . | Strategy &amp; Data Projects under the responsibility of François Le Nouvel: The team isresponsible for the coordination of Data subjects and transversal operating modes as well as the internal/external visibility of data at FLOA. . | . The Data Department is also part of the FLOA governance with the use of Jira to help gain in efficiency, rigor, visibility, traceability and management. . The chart below illustrates the FLOA organization: . . Figure 1: Diagram of the organization of FLOA . Data platform . ## . The Data platform at FLOA brings together a set of tools and services to store (via Blob Storage, Datalake, Snowflake), process/prepare (via Talend or Dataiku), govern (via DataGalaxy), visualize (via Tableau), analyze (via Tableau or Dataiku) and model (via Dataiku) data. . Data storage . The Azure Blob Storage: serves as a buffer zone for storing raw data and as a gateway to the Data platform (SI FLOA, Euro Information, Eureka, etc.). Data arriving on the blob is not intended to remain there but to be automatically migrated to Snowflake (or the Datalake in certain cases) . | Data Lake: allows to store in file mode a very large quantity of data. These data can be stored in any format, in a raw way or after a reworking. They can be kept for an indefinite period of time (within the limits of RGPD). Historically, datalakes appeared in the so-called “Big Data” architectures in order to allow the analysis of huge volumes of data at a lower cost than storage on a classic database. In the meantime, databases have evolved and their cost has decreased. In addition, managing data in file mode on a Datalake has often appeared tedious and therefore costly for standard uses. This is why in FLOA, the Datalake, provided by Microsoft Azure, and is intended to be used for the storage and use of unstructured data (MongoDB) because they cannot be stored in database format (Snowflake). These data must be regularly purged to meet the RGPD rules. . | Snowflake (or SNF): is a SQL data storage and analysis service offered as a SaaS software (hosted in the Azure cloud in the case of FLOA). It is the first data warehouse designed for the cloud. Its multi-cluster architecture separates data storage into three distinct functions: storage; computation (virtual warehouses); services (user authentication, management, security, query optimization). . | . The tree structure in Snowflake is as follows: . Database . Schema(s) . Table(s) . | View(s) . | . | . | . In FLOA, the databases correspond to the different levels of storage (ODS, DWH, DTM) but also to environments dedicated to the business (BIDS, FRAUDE, etc.). Each database will have a production version (e.g. DWH_PRD), a test version (e.g. DWH_REC) and a development version (e.g. DWH_DEV). For the schemas, it will be a logical grouping allowing to target more quickly the stored data (Ex: MASTER, FILE, CASHBACK, SCORE_ERK). The table names correspond to those defined when modeling the ODS, DWH and DTM layers. . Snowflake is a tool whose use is intended for Data teams (DataOps, Data Science and Data Analysis): . Snowflake - OPERATIONAL DATA STORE (ODS): This storage phase concerns structured and semi-structured data. The ODS allows to centralize in our Snowflake database all the heterogeneous data entering the Data platform (AR, external partners, open data, etc.) This storage space allows to standardize the storage format (Snowflake) but also the format of some types of fields (Date, Number, etc.). For an input data source, a target data source is fed into the ODS. It is therefore a raw data storage with a first level of technical harmonization. This level of storage is intended to be fed by the DataOps team and used by the DataOps and potentially Data Science teams. . | Snowflake - DATA WAREHOUSE (DWH): The DATA WAREHOUSE succeeds the ODS layer in Snowflake. It allows to arrange, group, consolidate the data stored in the ODS in a model corresponding to the activity of the company. These tables contain information at the finest granularity with few calculated indicators. We can talk about a tabular view of data grouped by functional family (e.g.: Instruction table in the PnF universe, Stakeholder table in the Cards &amp;Credit universe, etc.) This level of storage is intended to be fed by the DataOps team and used by the DataOps and Data Science teams and all the company’s Data Analysts. . | Snowflake - DATAMARTS (DTM): The DATAMARTS succeed the DWH layer in Snowflake. Each DATAMART corresponds to a precise business need/use. There are many of them within the company. The DATAMART will allow to consolidate the DWH layer by gathering the relevant information present in several DWH tables and by storing them in a single table. DATAMART allows a more efficient and simpler use of the data available in the DWH. The business gets what it needs, stored the way it wants and with more efficient response time performance. This level of storage is intended to be fed by the DataOps teams and used by all DATA users within FLOA. . Data ingestion . Tools used for data ingestion . | | | . The data ingestion phase consists of retrieving data from the blob storage that is stored externally, either in the FLOA SI or at Euro Information or at our partners or via open data. The data can be retrieved . Either by push (the data provider pushes the data to the blob storage) . | Or by connecting directly to the database or to an intermediate SFTP (via Talend connectors) . | Or by using a dedicated query tool, such as the Focus / Webfocus tools with Euro Information . | . Different tools to support this data ingestion phase: . Talend: Talend is an ETL (Extract, Transform, Load) that collects data from several sources, structures it, centralizes it, and prepares it to make it available to users. Talend’s tool is used exclusively by the DataOps team to make prepared data flows available to the company . | Focus / Webfocus (outside the Data platform): Focus (older technology) and Webfocus (newer technology) are tools that code queries to extract BCA data hosted in the Euro Information environment (SIDU). Eventually, the goal is to get rid of these queries by directly deporting the data uses on the platform . | DataGalaxy: DataGalaxy is a web &amp; collaborative data mapping tool, essential to comply with the RGPD. The tool will eventually allow to have an exhaustive view of metadata and data processing on the Data platform, by offering search, impact analysis and data lineage functionalities. DataGalaxy also allows tagging some metadata and these tags can be used for example for the criticality level of personal data in the framework of the RGPD. DataGalaxy is a collaborative tool that will be open to all (not yet available - the opening will be done progressively) with 3 types of licenses depending on the use (steward, explorer, viewer) . | . To better understand the “journey of a data”, here is an example of lineage for an EI table arriving on the platform via the Blob which is ingested (via Talend) into Snowflake’s ODS and DWH layers, on which the DTM layer is based. . . Figure 2: Modeling the path of the data . Processing of the raw data during ingestion . The data preparation includes the different steps between the ingestion of the data and its availability to the user and can be summarized in the following steps: . Retrieve the data from the blob storage, . | Clean the data and control the quality of the data, . | Structure the data (according to the data model being developed), . | Enrich the data by crossing it with data from the same or another table, . | Publish the data in Snowflake or in the Datalake . | . The use of the data can be in real time: . In FLOA, our product subscription paths query production databases to store data (to retain information related to the customer’s request) and to retrieve data (e.g. customer recognition) in real time. . | Other usages include calling the credit score, fraud detection with the Threatmetrix tool (TMX), identity verification with Netheos, etc. . | . On the other hand, the use of data can be offline and corresponds to a usage of business monitoring or steering reports, data analysis, score modeling, marketing campaign creation, etc. . Exploitation and processing of the data . FLOA bank provides different tools to accompany this data exploitation phase: . Dataiku (or DSS): DSS is a Data Science tool that allows not only the analytical processing of data but also different types of modeling (prediction score, appetence, segmentations etc.) while offering a collaborative approach. The platform is composed of different environments and allows to prepare, model (Design Node environment), automate and historize the workflow (Automation Node environment), and deploy the production (API Deployer and API Node environments). For more information on the technical environment around Dataiku, please refer to: | . https://www.dataiku.com/ . The use cases of Dataiku DSS are numerous. The platform can be used for scoring, marketing / appetence analysis, fraud detection, data management, demand forecasting, etc. Dataiku brings us the possibility: . To work on more data (in terms of volume and variety), . | To do more “modern” modeling with the use of new methods like neural networks, . | To be more agile in the evolution of scores thanks to machine learning, the objective being always to play with the acceptance/risk trade-off. . | . Dataiku is a tool that is intended for use by data scientists and accounts for: . . Figure 3: Dataiku usage in FLOA . Tableau: Tableau is the tool that has been selected by FLOA bank in the data platform as a data visualization tool (DataViz). DataViz refers to all visual representations of data in order to help the user better understand complex and numerous information or data and to ensure a better understanding of the data and the issues related to it. Tableau is one of the world leaders in the market with Power BI (Microsoft) and QlikSense (QlikView). It uses a file structure of workbooks and sheets similar to Microsoft Excel. A workbook contains sheets. A sheet can be a spreadsheet (also called a view), a dashboard or a story. | . There are 3 types of licenses to access Tableau depending on the use - the tool is intended to be open to all: . Creator: for advanced uses; this status allows to connect to any data source outside the data platform . | Explorer: allows you to create your own workbook from enterprise data made available on the data platform . | Viewer: allows you to access the visualizations created by Creators &amp; Explorers and to customize the visualization according to your own needs (alerts, views, etc.) . | . . Figure 4: Data platform in FLOA . Credit Scoring . Types of credit scores . Scoring is one of FLOA’s main activities. Based on quantitative and qualitative data, such as a user’s behavior on a website or the frequency and value of his purchases, scoring consists of assigning a score to determine the probability of response to an offer or the risk of non-payment. . The Data Scientists use different types of scores: . Credit score or risk score: deciding from data derived from the client’s application and public data, whether FLOA accepts or rejects the order based on the risk estimate which the probability of default. These scores are all intended to be internalized in the Dataiku tool. . | Credit default recovery score: to evaluate whether it is more relevant to propose an automatic “self care” path or to go through an advisor for a person entering into a credit default recovery. The objective here is to optimize the contact strategy according to the past behavior of customers in the same group to restore their unpaid situation. The right treatment for each profile will allow FLOA to save money and focus its efforts on those most in need of further treatment. . | Fraud score: detect (a priori or a posteriori) if a request seems to correspond to a fraud. . | Appetence score: used in direct marketing and which reflects the probability that a prospect will become a consumer of the product or service being promoted or that he will respond favorably to the offer made to him. . | Attrition score: used in customer portfolio management and translates the probability that a customer or subscriber will switch to a competitor or terminate his contract. . | . A score is first modeled with a learning database population to answer a given problem and validated by the achievement of one or more KPIs (e.g.: acceptance rate and risk rate for a grant score). This score can then be used: . Real time scoring: Real-time scoring involves using the model to make predictions on individual rows of data via an API. . | Batch scoring: Batch scoring involves using the model to make predictions on multiple rows of data at a given frequency. . | . In addition to scoring activities, the automation of decisional reporting has many advantages in the face of the multiplication of needs and allows for significant time savings on a daily basis. At the request of the various Data Science/Finance/Risk and Fraud teams, the role of the analysts is to create dashboards with precise KPIs into which the data will be automatically integrated. These dashboards are then published on Tableau Server as activity monitoring. . Some examples: . Cards and Credits universe: Operational monitoring of collections, acceptance/refusal rates, KPI’s Granting, KPI’s Risk. . | PNF Universe: Monitoring of Pnf and Coup de Pouce scores in terms of acceptance and breakage (unpaid). . | Marketing and customer knowledge: Customer activity monitoring, campaign assessment. . | . Generic vs customized scoring models . In FLOA Bank, there are two types of credit scoring models: . Generic credit scoring model: based on an overall behavior of different customers. The goal is to have a general model that aims to rank order the risk appropriately and simultaneously define the borrower characteristics, for a specific product’s offering, that best predict future behavior in the repayment of the loan. . | Customized credit scoring model: based on a particular population and the outcome of the lending decisions that captures the behavioral and other characteristics of the subprime population targeted. . | . In order for the customized scoring to be meaningful and aligned to the needs, it is necessary that there is sufficient information on the credit lending product. . Coup de pouce UPCYCLE . Coup de Pouce UPCYCLE is one of several Coup de Pouce offers in FLOA. Based on Coup de Pouce production in first trimester of 2021, UPCYCLE presents 3.5% of total production behind Coup de Pouce Cdiscount (65.5%), Direct (19.5%) and Lydia (10.2%). In the other hand, UPCYCLE default cases presents 6.7% of the overall default’s cases across different Coup de Pouce providers. . It aims at proposing an alternative offer, which corresponds to 500€ or 1000€, to refusals of large amounts ranging from 5.000€ to 15.000€ (PPR / PPC). The UPCYCLE idea intent at testing the potential customers on a new product like Coup de Pouce, which is based on smaller amounts, and then possibly cross sell or potentially let them in for a large credit. Coup de Pouce UPCYCLE scoring model is in production since December 2019. It is a generic credit scoring model based on customer’s behavior from different Coup de Pouce partners. . Generally, generic credit scoring models do not accurately predict the level of the risk. Thus, banks that use generic models should not assume that their loss rates will be the same as those reflected in the industry odds charts. How accounts ultimately perform depends on a number of factors, including account management techniques used, the size of loan granted, and so forth. However, the development and implementation of scoring models and the review of these models present inherent challenges. . In that context, my first mission in FLOA Bank aimed at creating a new Tableau dashboard that allow the risk management unit as well as data scientists to monitor the production of the scoring models from a risk perspective and to simulate the future default rates. . Using different KPIs, we analyze the production behavior and the robustness of the score model based on: . The distribution of orders by profile (UNKNOWN/TRUSTED/WHITELIST detailed in following chapters) and offers (3X/4X) over time. . | The distribution of orders by scores over time. . | The average basket per score over time. . | The default rate by score: theoretical rate vs observed rate . | The simulation of default rate for the next 6 weeks. . | . From the Tableau dashboard, we illustrate the scoring model behavior on all customer’s profile of Coup de Pouce UPCYCLE. The chart below illustrates that the generic score model for Upcycle is not properly distinguishing the bad from the good payers. The default rates, as the percentage of unpaid loans i.e Coup de Pouce, varies widely across the risk scores and does not follow the theoretical trend of the model. On one hand, we notice higher default rate on good scores: default rate of 12% on score of 20 vs default rate of 10% on score of 14 and it reaches 15% on score of 4 but rises to 17% rate on score of 12. On the other hand, the default rate for each score should virtually be the same or close in accordance with the model in theory as the higher the score, the lower the credit risk. However, on score of 20, the model simulates 1.3% theoretical default rate whereas, in production, the default rate reaches 13%. Finally, regardless of the risk score, the average default rate remains very high close to 14% comparing to other Coup de Pouce partners with a customized scoring model. . . Figure 4: Default rate by score for UPCYCLE . In order to illustrate the performance of a customized scoring model, we take as an example the credit score model of Coup de Pouce CDISCOUNT, one of FLOA largest Coup de Pouce providers. . We notice that the risk level across the risk scores is decreasing alongside the theoretical default rate. The volume of orders is equally distributed according to the risk level: Higher volume on good scores beneath the average default rate at 5% and lower volume on bad scores with default rates slightly above the average but remain below the theoretical risk in each score. And regardless of the risk score, the average default rate remains close to 5%. . . Figure 5: Default rate by score for CDISCOUNT . Furthermore, the following chart displays the overall trend of the default rate varying over a wide range from 7% to 23%. The projection of the default rate on the last weeks of production, according to different empirical assumptions, indicates a continuous trend of the default. . . Figure 6: Historical and projected default rate for UPCYCLE . In addition, the following chart describes the proportion of Coup de Pouce Upcycle orders by score ranging from 1 (High risk) to 20 (Low risk). We see that most of the orders are on low score, from 1 to 5, meaning that the Upcycle population is risky. . . Figure 7: Distribution of Coup de Pouce orders by score for UPCYCLE . From this analysis, regarding the evaluation and the ongoing monitoring, we deduce that the population of Coup de Pouce Upcycle is risky and that we may identify concerns about the generic model performance of Coup de Pouce UPCYCLE. . Monitoring machine learning models in production . Determining the perfect time to upgrade a generic model can be challenging. The Tableau dashboard of risk monitoring can help to illustrate the decision making by comparing the theoretical model results and the risk level of the model in production. The default rate projection for the last weeks of production can also be a good indicator for the expected default rate. . In addition, once we reached more than 1.000 default cases on a product, the management can feel confident that it may be the time to challenge the generic model with a customized one. In our case, for Upcycle scoring model, until July first, the default reached 1567 cases which corresponds to 1M€ loss. The total orders attained 11.500 which corresponds to 7.6M€. . Switching to a specific model brings good benefits regarding having a closer look to the data population and automating the model lifecycle ensuring that the data as well as the models are up to date. . Monitoring Customized credit scoring models in production is an important, but often tedious task for data scientists. Conventional model retraining strategies are often based on monitoring model metrics. However, for many use cases, monitoring model metrics, such as AUC, may be insufficient. This may be particularly true in cases where long periods of time pass before receiving feedback on the accuracy of the model’s predictions. Over time, a machine Learning model gradually loses its predictive power. This is commonly known as “drift”. If this phenomenon is not detected quickly, it can have a significant impact on the performance of our models. It is therefore important to set up a loop that allows us to refresh our models when they degrade too much. . . Figure 6: Model quality of static models vs refreshed models . In general, such deviation may occur for two possible reasons: Concept Drift and Data Drift. . Concept drift corresponds to a change in the statistical properties of the target variable. . | Data drift, on the other hand, corresponds to a change in the statistical properties of the explanatory variables. . | . In FLOA Bank, data scientists define several possible strategies: . Re-train the customized model on more recent data and monitor the performance indicators (AUC, Lift, F1-score, …). If these indicators drop by a certain percentage, it is a sign that there is a drift in our data. Although this methodology may be correct, changing a model on production based on the performance metrics is risky. It may not go well as planned as we usually have to wait several days or even several months to see if the event has taken place. . | Monitor the evolution of the statistical properties of the variables can be more useful by looking at the recent data the model has had to score, and statistically compare it with the data on which the model was evaluated in the training phase. If these datasets are too different, the model may need to be retrained. . | . Dataiku developed a plugin used by the data science team called “model drift monitoring” which aims at providing views to analyze the potential drift of machine learning models. With this plugin, we can measure Drift by examining if new data waiting to be scored has diverged from the training data. If this new data is significantly different from the data used to train the model, it is likely that the model will no longer perform well, and may need to be retrained. . To detect drift between the original test dataset used by the model and the new data, the plugin stacks the two datasets and train a RandomForest (RF) classifier that aims at predicting data’s origin. This is called the domain classifier in the literature. . To have a balanced train set for this RF model, we take the first n samples from the original dataset and the new one, where n is the size of the smallest dataset. . If this model is successful in its task, it implies that data used at training time and new data can be distinguished, we can then say that the new data has drifted. . The plugin provides a new model view with the following visual elements: . Drift score: | . The plugin applies a binomial test on the accuracy of the domain classifier. The null hypothesis being there is no drift. There are two types of information that are computed: . The p-value which gives the probability of having at least the computed accuracy under the null hypothesis. . | The confidence interval of the computed accuracy given the sample size.  . | . The confidence interval gives us an index about the reliability of the drift score. If we get an accuracy score of 0.65 ± 0.2, the score is not very reliable as it overlaps with 0.5, which means no drift. . . Figure 7: Drift score . Fugacity: | . In addition to a drift score, the plugin provides a number of other insights. The Fugacity table expresses the difference between expected and observed samples. It compares the proportion of samples predicted in each class when applying the model on both the test and the input datasets. . . Figure 8: Fugacity table . Density chart: | . The density chart is the probability density estimation for a given prediction class when scoring both the test dataset and the new input dataset. Visually different probability density estimations indicate high data drift. The inverse is not true, as we can have a high drift score but a similar predicted probability density estimation. One such situation is when we have high drift of a feature that is not important in the original model. . . Figure 9: Density chart . Scatter plot: | . It is important to consider the data drift score in the context of the importance of certain features. For example, if the data drift score is high, but the features responsible for the drift are of low importance, the behavior of the model may remain the same. . The scatter plot shows feature importance for the original model on the x-axis versus feature importance for the (data classifying) drift model on the y-axis. Features in the top right quadrant of this scatter plot are highly drifted (i.e. they are powerful in distinguishing test data from new observations), but also of high importance for the original model. In this situation, you can expect the performance of the model to degrade as the model does not apply to your new observations. . . Figure 10: Scatter plot for feature drift . Second, by computing data drift between two dataset that have the same schema. The goal is to evaluate the same population, using the same KPIs, between different time stages. This recipe aims at data scientist who mostly build scoring models from data for a customized scoring model, each day when new data comes in they want to have a tool to assess whether or not the new data is “different”. It provides 2 types of information:  . Drift score: Data scientists can create a custom check with their own threshold and put these in a scenario for drift monitoring. . | Most drifted features: if the drift score is the threshold defined, we can define the features behind this drift. . | . Our thanks go to Vincent GALLMANN, senior data scientist in FLOA and one of Dataiku power users, for helping us to understand these technics as well as providing its different illustrations in Dataiku DSS as below: . . Figure 11: Dataiku flow for drift application . In conclusion, when properly designed, customized models are usually more reliable than generic subjective or judgmental methods. However, development and implementation of scoring models and review of these models present inherent challenges. These models will never be perfectly right and are only good if users understand them completely. Further, errors in model construction can lead to inaccurate scoring and consequently to booking riskier accounts than intended and/or to a failure to properly identify and address heightened credit risk within the loan portfolio. Errors in construction can range from basic formula errors to sample-bias to use of inaccurate and erroneous data. The impact on the score could be substantial. . UPCYCLE credit score development . Goal definition . ## . A credit scoring project starts with understanding the business objectives. With the assistance of Anais LAPEYRE, we had several exchanges with the risk and commercial teams to highlight a new plan for Upcycle scoring model. Alongside the risk team and after defining the new budget starting from June 2021, we set a 21% theoretical default and an approval rate of 70%. On the other hand, the marketing team brought insights about the user’s types, the origins of the orders and the business strategy. Hence, the new scoring model will cover: . Refusals of FLOA personal loans up to 15.000 € (PPRTOCDP): It covers the major part of Upcycle clients since its launch. . | Refusals of FLOA personal loans ranging from 7.000 € to 10.000 € (PPCTOCDP). . | Business providers (Refus PPR Leadin): LEADIN. . | FLOA credit card: The Upcycle model is set to score potential clients for the credit card Casino-Cdiscount starting late August 2021. . | . . Figure 12: Order’s origin of Coup de Pouce UPCYCLE . *Reconnexion modality refers following PPC or PPR request but whose origin has been lost . Once the scope of the business is defined, the machine learning engineering part starts. The figure below illustrates the following stages of a scoring project life cycle and consists of: 1) goal definition, 2) data collection &amp; preparation, 3) feature engineering, 4) model training, 5) model evaluation, 6) model deployment, 7) model serving, 8) model monitoring, 9) model maintenance. At some stages (indicated by dashed arrows), a decision can be made to go back in the process and either collect more data or collect different data and revise features. . . Figure 13: Machine learning life cycle . Data collection &amp; preparation . Data collection . | | Dataiku DSS allows data scientists in FLOA Bank to deploy predictive models for real-time scoring using its Dataiku API node. The API node provides all the necessary features for scoring in production as input and allows users to query predictions as output. . Example of input of features data in real-time for Upcycle scoring: . { “clt_address_1”: “string”, . “clt_address_2”: “string”, . “clt_address_3”: “string”, . “clt_address_4”: “string”, . “clt_birth_date”: “2019-10-25”, . “clt_birth_zip_code”: “string”, . “clt_cell_phone_number”: “string”, . “clt_city”: “string”, . “clt_civility”: “string”, . “clt_country”: “string”, . “clt_customer_ref”: “string”, . “clt_email”: “user@example.com”, . “clt_first_name”: “string”, . “clt_last_name”: “string”, . “clt_maiden_name”: “string”, . “clt_nationality”: “string”, . “clt_phone_number”: “string”, . “clt_zip_code”: “string”, . “nav_ip_address”: “string”, . “nav_origin”: “string”, . “nav_user_type”: “string”, . “nav_visit”: “string”, . “ppr_decision_bris_cd1”: “string”, . “ppr_decision_bris_cd2”: “string”, . “ppr_decision_bris_cd3”: “string”, . “ppr_decision_bris_cd4”: “string”, . “ppr_decision_cha_cod_reaf”: “string”, . “ppr_decision_cha_hn1”: “string”, . “ppr_decision_cha_hn2”: “string”, . “ppr_decision_cha_hn3”: “string”, . “ppr_decision_cha_hn4”: “string”, . “ppr_decision_cha_not”: “string”, . “ppr_decision_sco_note”: 0, . “ppr_journey_contract”: “string”, . “ppr_journey_revenue”: 0, . “ppr_journey_shelter”: “string”, . “request_date”: “2019-10-25T15:08:38.701000Z”, . “request_id”: “string”, . “request_partner_code”: “string”, . “request_score_type”: “string”, . “white_list”: “string” } . Example of output data predictions in real-time for Upcycle scoring: . {“__meta__”: . {“api_context”: . {“endpointId”: “traitement_output_cdp_refus_ppr”, . “serviceGeneration”: “RANDOM_FOREST_V10”, . “serviceId”: “SCORE_RISQUE_CDP_REFUS_PPR”}, . “dss_timing”: . {“execution”: 26350, . “functionInternal”: 25651, . “preProcessing”: 0, . “wait”: 24}}, . “decision”: “R”, . “input_score”: { . “request_date”: “2019-10-25T15:08:38.701000Z”, . “request_id”: “string”, . “request_partner_code”: “string”, . “request_score_type”: “string”, . “white_list”: “string” . }, . “request_date”: “2020-10-25T15:08:38.701000Z”, . “request_id”: “string”, . “score_called_api_context”: { . “endpointId”: “SCORE_RISQUE_CDP_REFUS_PPR”, . “serviceGeneration”: “RANDOM_FOREST_V10”, . “serviceId”: “SCORE_RISQUE_CDP_REFUS_PPR”}, . “score_called_result”: . {“ignored”: false, . “prediction”: “1”, . “probaPercentile”: 92, . “probas”: { . “0”: 0.6326911449614383, . “1”: 0.36730885503856175}}, . “score_called_timing”: . {“enrich”: 11, . “postProcessing”: 30, . “preProcessing”: 38, . “prediction”: 1095, . “preparation”: 19590, . “wait”: 17 }, . “score_note”: 2, . “score_point”: 0, . “score_proba”: 0.36730885503856175, . “seuil”: 0.2624 . } . The model is built from features of the input data which contains the following types of information: . Client information: Age, first name, last name, maiden name, city, address, zip code, email, country, nationality and phone number. . | Order information: origin of the demand, user type, visit type. . | Personal loan scoring information (PPR / PPC): decision score, revenue, shelter and contract. . | . In addition, open datasets can be used to enrich the input data to build predictive models. This process takes a column containing a French department code and outputs several columns with demographic data about this department such as: . Basic demography data (population, households, births, deaths, …) . | Housing data (households, second homes, …) . | Fiscal/Revenue data (number of ‘foyers fiscaux’, average revenue, …) . | Employment data (number of jobs, unemployment rate, …) . | Companies data (number of companies, breakdown by industry sector, breakdown by size, …) . | . For Coup de Pouce product, we define potential clients based on several modalities determined as follows: . Unknown: Clients signing up for a Coup de Pouce loan for the first time. . | Trusted prospect: Non-repeaters clients who had done at most only one coup de Pouce before without default or had been eligible for the loan but didn’t validate their order over the last 70 days. . | Trusted client: Repeaters clients who had done at least more than one Coup de Pouce before without default. . | Whitelist: VIPs and good clients. . | . In production, the score is used only for Unknown clients. Therefore, it is recommended to train a scoring model on at least 1.000 default cases of Unknown clients in order to build a robust model for future unknown clients. . In our case, our data set will cover the period starting from 2019/11/13 to 2021/05/23. The frequency distribution of Upcycle Coup de Pouce client ‘s modalities and default cases by modality is as follow: . [CHART] . [CHART] . Figure 14: Coup de Pouce UPCYCLE client’s modalities . Unfortunately, we have only 475 unknown clients with a default, representing 14% default rate in this client modality, which won’t be representative in our data set for the new model. As a result, we need to add another close modality to reach 1000 default cases: We add the trusted prospect clients. . To test whether the default rate of the two population (Unknown and Trusted prospect) is equal, we proceed by Student t-test at 90%, 95% and 99% confidence level: . Samples       . Client Modality |   | Count | Mean | . Unknown |   | 3384 | 14,04% | . Trusted Prospect |   | 6039 | 14,19% | .   |   |   |   | . Total |   | 9423 | 14,14% | . [CHART] . Figure 15: Proportion of Unknown vs Trusted prospect . Hypothesis   . Tested hypothesis | Default rate means are identical in both populations | . Significance level | 0.1 | . Significance level | 0.05 | . Significance level | 0.01 | . Results |   | . 𝘵-statistic | -0.206431921 | . 𝘱-value | 0.8364579992 | . At confidence level of 90%, 95% and 99%: We deduce non-significant difference in means among both populations. Therefore, our data set will cover Unknown and Trusted prospect clients, representing 9432 validated orders with 1332 (14.13%) default cases. . We donate our target variable “casse_r2” a binary variable representing the default event as 1 in the case of default and 0 otherwise. The prediction type is “Two-class classification” and the proportion of the target classes is as follow: . . Figure 16: Proportion of default cases on the data set . Data analysis . Starting with the proportion of validated orders and default cases by client’s modality. We notice a stable distribution of validated orders and default cases by modality, mostly from Trusted-Prospect clients, across the period selected for the modeling. . . . Figure 17: Proportion of validated and default loans by client’s modalities . The waterfall chart helps uderstanding the the gradual transition of Upcycle score path. From 448k request orders, only 13k are granted. The score refuted 215k request order where as 188K clients have abondenned their Coup de Pouce demand. . . Figure 18: Waterfall chart of UPCYCLE scoring process . Among the validated orders, we can see that their distribution is stable, ranging from 400 to 700 commands per month. The average default rate remains high and also stable ranging between 13% and 16% default rate. . Figure 19: Granted Coup de Pouce loans by average default . The variable ‘ppr_journey_contract’ describes the client’s employment contracts: CDI, CDD, Intermittent contract… Most of Upcycle clients don’t fill in their employment contract status. A large proportion of clients are on CDI and the rest on CDD, INT or others employment forms. . . Figure 20: Client’s employment contracts . The variable ‘ppr_journey_shelter’ describes the housing occupancy status of the client: Locataire, propriétaire, logé par la famille, etc.. As seen below, most of Upcycle clients also don’t fill their housing status. A large proportion of clients are Locataire. . . Figure 21: Client’s housing status . The variable ‘ppr_decision_sco_note’ describes the score given by the credit score loan PPR of the original client demand. Unfortunately, this variable is not well mapped in our logs as it is usually missing. . . Figure 22: Client’s credit score on larger loans PPR/PPC . Feature engineering . After data collection and preparation, feature engineering is the second most important activity in score modeling. We illustrate several techniques used for our scoring model: . Binning and weights of evidence . The reasons to discretize a real-valued numerical feature can be numerous. A successful discretization adds useful information to learning algorithm when the training is relatively small and may increase the performance of the model (Szepannek,2007) . Binning of numeric variables is often considered to be the most relevant part of score modeling. In general, cluster analysis centers around reducing the Euclidean-distance of within cluster variation of observations, while maximizing the segregation among the different cluster groups/bins. However, the Euclidean distance is the “ordinary” distance between two points. To solve the problem, we propose to cluster the factor observations based on their Weight of Evidence . [ text{WO}E_{i} = log left( frac{ text{ Dist. Good }{i}}{ text{ Dist. } text{ Bad }{i}} right)] . The Weight of Evidence (WoE) is widely used in credit scoring to separate good and bad clients. It compares the proportion of good accounts to bad accounts at each attribute level, and measure the strength of the attributes of an independent variable in separating good and bad accounts. As we cluster the factor attributes based on their corresponding WoE, the resulting bins would be analogous to default risk, regardless the fact that unsupervised learning methods don’t take any response variable into consideration. . Using the ‘smbinning’ package in R, this method uses Recursive Partitioning through conditional inference trees approach and can be summarized as follows: . Evaluate which variable is most significant by the following line code: | filter(smbinning.sumiv(df = mydataset, y = “good”),Process==”Numeric binning OK”) . Evaluate the best split of a continuous variable through the Chi-square decision tree approach: When binning a continuous variable, we are predicting the target variable using only our one continuous variable in the conditional inference tree. It evaluates if the variable is significant at predicting the target variable. If so, it finds the most significant statistical split using Chi-square tests in between each value of the continuous variable and then comparing the two groups formed by this split. After finding the most significant split we have two continuous variables - one below the split and one above. The process repeats itself until the algorithm can no longer find significant splits leading to the definition of the bins. | Count the number of default and non-default events for each attribute levels in the factor variable. . | Calculate WoE of each levels of attributes (WoE for the fine classes). . | Cluster the attributes based on their calculated WoE. . | Form the final bins according to the cluster groups. . | . The ‘smbinning’ function inside the smbinning package is the primary function to bin continuous variables and takes as the following arguments: . df : Data frame . | y : Binary response variable (0,1). In our case, the default variable ‘casse_r2’. . | x : Continuos characteristics . | p : Percentage of records per bin. We set the default 5%. . | . In our case study, the predictor variables defined by the function ‘smbinning’ set to be binned are: . clt_age: Client’s age. . | insee_owner_rate: Owner rate giving a postal code. . | insee_dss_ets_indus_2010: Number of industrial establishments giving a postal code in 2010 . | insee_dss_ets_construc_2010 : Number of establishments in the construction sector giving a postal code in 2010 . | clt_email_digits_length: Length of digit numbers used in mail address. . | insee_dss_ets_10plus_201 : Number establishments with 10 or more employees giving a postal code in 2010 . | insee_pop1564: Total population between 15 and 64 years old giving a postal code. . | insee_pop: Total population giving a postal code. . | insee_chom1564: Number of unemployed giving a postal code. . | insee_1564_rate: Unemployment rate giving a postal code. . | . In our report, we choose the variable ‘clt_age’ to illustrate this method. The ‘ivtable’ method in ‘smbinning’ package gives a summary of the splits as well as some information regarding each split. Working from left to right, the columns represent the number of observations in each bin, the number of goods (non-defaulters) and bads (defaulters) in each bin, as well as the cumulative versions of all of the above. Next comes the percentage of observations that are in the bin as well as percentage of observations in the bin that are both good and bad. Finally, the table lists the odds, natural log of the odds, weight of evidence (WoE), and information value component (which we are going to detail in following chapters). . . From the chart below, we can deduce higher proportion of default cases among young clients under 27 years old where the default rate is above the average 14.14%. Whereas, the proportion of good loans are among clients older than 39 years old. . Moreover, WoE summarizes the separation between the event of default and non-default (bad and good loans) as shown in the graph below. For WoE we are looking for large differences in WoE between bins. Ideally, we would like to see monotonic increases for variables that have ordered bins. This isn’t always required as long as the WoE pattern in the bins makes business sense. However, if a variable’s bins go back and forth between positive and negative WoE values across bins, then the variable typically has trouble separating goods and bad loans. WoE approximately zero, client’s age between 39 and 48 years old, implies percentages of non-default (good loans) are approximately equal to percentages of default (bad loans) so that bin doesn’t separate well the default and non-default events. WoE of positives values, client’s age above 48 years old, implies the bin identifies observations that are non-default (good loan), while WoE of negative values, client’s age below 27 years old, implies bin identifies observations that are events (bad loans). . . Figure 23: Smbining graphs . We then create a new categorical variable which contains age classes defined by the ‘smbinning’ and we call it ‘flg_age_risky2’: Class 1- Under 24, Class 2- between 24 and 27, Class 3- between 27 and 39, Class 4- between 39 and 48, Class 5- above 4_ years old. . In the other hand, based on the average rate of default cases in the ‘ivtable’ below, we can construct a new variable, called ‘flg_age_risky’ that takes two classes: Class 1- negative WoE and Class 2- positive WoE. . . Figure 24: Weight of evidence cut off . Another approach is to use the WoE for encoding where we can replace the classes with their associated WoE values. However, this method had several drawbacks and results to overfitting as we manipulate the effect of variables according to how categories are created. . Manuel binning . Manual binning offers detailed and tailored solution to classification within variables in line with business sense. For example, a variable can be binned according to the target group study by the firm, and such binning will make more business sense than some random classification based on equal size or equal weight binning. Especially for categorical variables, employing local expert knowledge has clear advantage over automated binning. We illustrate this method with the variable ‘nav_origin’. This variable describes business providers of Coup de Pouce Upcycle and contains multiple modalities that are not stable across the study period: . . Figure 25: Navigator origin’s modalities of Coup de Pouce UPCYCLE demand . The marketing team has more knowledge on characteristics within this variable and a proposal solution was to define whether the demand is from FLOA client or a new potential client based on origin that stars with . ‘CS’=cross sell, and ‘Bel’= banque en ligne : defines FLOA clients. . | ‘Comp’ : defines potentiel clients. . | ‘Email’ : could define either clients or potential clients. . | ‘CDP’ = coup de pouce et ‘Cdis’= cdiscount : could define either clients or potential clients. . | . Based on this, we can define a new variable ‘nav_origin_group’ that takes three modalities ‘Client’, ‘Potential client’ or ‘Potential client or Client’. . We proceed by Chi-square test to check the dependency between our new variable ‘nav_origin_group’ and the target variable ‘casse_r2’ to evaluate the binning proposal: . Hypothesis   . Tested hypothesis | nav_origin_group and casse_r2 are independent | . Significance level | 0.05 | . Results |   | . Chi-square statistic | 7.3692719671 | . Degrees of freedom | 2 | . 𝘱-value | 0.0251063122 | . From this test, we conclude that variables ‘nav_origin_group’ and ‘casse_r2’ are not independent. Hence this method results in precise binning of the variable by groups with similar characteristics. However, manual binning is a tedious and time-consuming process, especially when the number of predictor variables is large. . Cluster Binning for Categorical Variables . Using Python, we compute the target proportion in each modality of qualitative features and assess the significance of the difference from the average target proportion. The proportion of each modality must be representative (&gt;1% of total records) and the difference from the average using chi-square / ANOVA test has to be statistically significant at 5% confidence level. . We illustrate this method using email address variable ‘clt_email_domain_0’. The output is as follow: . . Based on that, we can define two dummy variables ‘clt_email_1’ which refers to less risky domain emails (default rate below the average 14% denoted by green dotes) and takes 1 if email is containing ‘waradoo, free, sfr, orange’ and ‘clt_email_2’ which refers to more risky domain emails (default rate above the average 14% denoted by red dotes) and takes 1 if email is containing ‘outlook, gmail’. . Time Features . From the date of the order ‘request_date’, we can extract valuable information such as: . Order month: Ranging from 1 to 12. . | Order day of the year: From 1 to 356. . | Order day of the week: From 1 to 7. . | Order day of the month: From 1 to 30. . | Order hour: From 1 to 24. . | Order date zone. . | If the order was on school holiday or on bank holiday, etc.. . | . The following charts illustrate the frequency distribution among the day of the week, hour of the day and the day of the month. We can see that most Coup de Pouce Upcycle demands are done mostly on mornings, during weekdays at the beginning of each month. . . . . Figure 26: Distribution of order by time modalities . Hamming distance . A distance between two strings of equal length is the number of positions at which the corresponding symbols are different. In other words, it measures the minimum number of substitutions required to change one string into the other, or the minimum number of errors that could have transformed one string into the other. . We calculate the distance between the username in the email and the first / last name of the client in order to detect similarity between them. . Impact coding . When the number of categories becomes very large, dummy encoding may become inconvenient. An alternative method to clustering or truncation of categories consists in characterizing the categories by the link they have with the target variable y: this is impact encoding. This method is also known as likelihood encoding, target coding, conditional-probability encoding. . For a target variable y and categorical variable X with K modalities (m_{1}),….( m_{k}), each modality (m_{k}) is encoded by its impact: . [ text{impact} left( m_{k} right) = mathbb{E} left lbrack y mid X = m_{k} right rbrack - mathbb{E} lbrack y rbrack . ] . Standard normalization . During the process of the feature engineering part, we used the standard normalization. It is the procedure during which the feature values are rescaled so that they have the properties of a standard normal distribution by subtracting the empirical mean and dividing by the standard deviation. It’s helpful to apply simple mathematical transformation to the feature values which includes taking the logarithm of the feature, squaring it, or extracting the square root of the feature. The idea is to obtain a distribution as close to a normal distribution as possible. . Missing values . There are a few choices for handling missing values in categorical features: . Treat as a regular value treats missing values as a distinct category. . | Replace missing values with the specified value and used for randomly missing data that are missing due to random noise by imputing the median value. . | . Modeling . Predictive power . As seen in previous chapters, Weight of Evidence summarizes the individual categories or bins of a variable. However, we need a measure of how well a variable do at separating the events from non-events. For that, we use the information value criteria (IV) as a measure of the predictive power of our variables. . In credit modeling, IV is used in some instances to actually select which variables belong in the model. Here are some typical IV ranges for determining the strength of a predictor variable at predicting the target variable: . 0 ≤ IV &lt;0.02 : Not predictor. . | 0.02≤ IV &lt;0.1 : Weak predictor. . | 0.1≤ IV &lt;0.25 : Moderate (medium) predictor. . | 0.25 ≤ IV : Strong predictor. . | . For categorical variables, IV uses the WOE from each category as a piece of its calculation: . [IV = sum_{i = 1}^{L} mspace{2mu} left( text{ Dist. } text{ Good }{i} - text{ Dist. } text{Ba}d{i} right) times log left( frac{ text{ Dist. Good }}{ text{ Dist. Bad }} right)] . To illustrate the predictive power of our categorical variables, we set an IV of 0.02 as a threshold as variables with information values less than 0.02 are typically not predictors in the scoring model. . [CHART] . Figure 27: Features by decreasing IV . As we can see, most of our variables are considered by the IV criteria as weak variables. The client department of residence and birth and client address are expected to have good predictive power in the model. In previous chapters, when defining the ‘smbinning’ package, we derive two variables from client’s age based on WoE: ‘clt_age_risky2’ taking the classes output of the function and ‘clt_age_risky’ where we define two classes based on the average default rate. From the chart above, we can see that classes defined by the ‘smbinning’ function have higher information value and thus more predictive power. As a result, we choose the following variable to integrate it in our model. . For numeric variables, we use Pearson correlation as a measure of their predictive power. We set a correlation threshold of 1.2%, in absolute value, with the target rate ‘casse_r2’ and we set correlation threshold between the dependent variables to 80%. . . Figure 28: Pearson correlation matrix . Performance metrics . The most common way to get a good model is to compare different models by calculation a performance metric on the holdout data. For classification, one way to assess the classification model performance is to use the “confusion matrix”, which summarizes how successful the classification model by comparing actual values (from the test dataset) to predicted values. . The confusion matrix is used to calculate the following performance metrics: . Precision is the ratio of true predictions to the overall number of positive predictions | . [ text{Precision } = frac{ text{tp}}{tp + fp}] . Recall is the ratio of true predictions to the overall number of positive examples | . [ text{Recall } = frac{ text{tp}}{tp + fn}] . In practice, we choose between high precision or high recall by varying the decision threshold for algorithms that return the prediction scores or by tuning the hyper parameters to maximize either precision or recall. This is called precision-recall tradeoff. . F1-score is the harmonic mean of precision and recall | . [F = 2 cdot frac{ text{ precision } cdot text{ recall }}{ text{ precision } + text{ recall }}] . Accuracy is given by the number of correctly classified examples, divided by the total number of classified examples. In terms of confusion matrix, it is given by: | . [ text{Accuracy } = frac{ text{tp} + text{tn}}{ text{tp} + text{tn} + text{fp} + text{fn}}] . Hamming loss is the fraction of labels that are incorrectly predicted | . [ text{Hamming Loss } = frac{1}{ text{nL}} sum_{i = 1}^{n} mspace{2mu} sum_{j = 1}^{L} mspace{2mu} I left( y_{i}^{j} neq { widehat{y}}_{i}^{j} right)] . Where (I) denotes the indicator function. Practically the smaller the value of hamming loss, the better the performance of the learning algorithm. . Roc curve: The Receiver Operating Characteristic (or ROC) curve shows the true positive rate TP/(TP+TN) versus the false positive rate FP/(FP+TN) resulting from different cutoffs in the predictive model. The “faster” the curve climbs, the better it is. On the contrary, a curve close to the diagonal line corresponds to a model with bad predictive power. | . Probabilities and lift . A binary classifier produces a probability that a given record is “positive” (here, that casse_r2 is 1). The lift is the ratio between the results of the chosen model and the results obtained with a random model. Lift curves are particularly useful for “targeting” kinds of problems which will be the bad clients in our case. In FLOA Bank, the lift is one of the main criteria in the validation process of a model as we can focus on the region of scores where the cut-off is expected and, at a given level of rejection, we can measure how the scoring model is better than random selection. In practice, the calculation is done for Lift corresponding to 10%, 20%,,100% of clients with the worst score. For Upcycle score, we set a lift corresponding to 30% of bad clients. . The paper of Martin and František ŘEZÁČ – “How to Measure the Quality of Credit Scoring Models” helps expressing Lift by means of the cumulative distribution functions of the scores of bad and all clients. The empirical cumulative distribution functions (CDFs) of the scores of good (bad) clients are given by the relationships . ( begin{matrix} F_{n. text{GOOD}}(a) = frac{1}{n} sum_{i = 1}^{n} mspace{2mu} I left( s_{i} leq a land D_{K} = 1 right) F_{m cdot text{BAD}}(a) = frac{1}{m} sum_{i = 1}^{m} mspace{2mu} I left( s_{i} leq a land D_{K} = 0 right)a in lbrack L,H rbrack end{matrix}) . where (s_{t}) is the score of the (i_{th}) client, n is the number of good clients, m is the number of bad clients, and (I) is the indicator function, where (I) (true) = 1 and (I) (false) = 0. (L) is the minimum value of a given score, (H) is the maximum value. . The empirical distribution function of the scores of all clients is given by . (F_{N, text{ALL}}(a) = frac{1}{N} sum_{i = 1}^{N} mspace{2mu} I left( s_{i} leq a right)a in lbrack L,H rbrack) . The lift is defined as: . ( text{Lift}(a) = frac{F_{n cdot text{BAD}}(a)}{F_{N cdot text{ALL}}(a)}a in lbrack L,H rbrack) . And the lift at a score level is defined as: . ( text{Li}f_{q} = frac{F_{n, text{BAD}} left( F_{N. text{ALL}}^{- 1}(q) right)}{F_{N cdot text{ALL}} left( F_{N. text{ALL}}^{- 1}(q) right)} = frac{1}{q}F_{n cdot text{BAD}} left( F_{N. text{ALL}}^{- 1}(q) right)) . where q represents the score level of 100q% of the worst scores and (F_{N. text{ALL}}^{- 1} left( q right) )can be computed as (F_{N. text{ALL}}^{- 1}(q) = min left{ a in lbrack L,H rbrack,F_{N, text{ALL}}(a) geq q right}). . Since the expected rejection rate is usually between 5% and 20%, (q) is typically assumed to be equal to 0.1 (10%), i.e., we are interested in the discriminatory power of a scoring model at the point of 10% of the worst scores. In our case, we set 30% lift, so we have ( text{Li}f_{30 %} = 30 cdot F_{n cdot text{BAD}} left( F_{N. text{ALL}}^{- 1}(0.3) right)) . We illustrate the results by two charts: . Cumulative Lift Curve chart | . The goal of this curve is to visualize the benefits of using a model for targeting a subset of the population. On the horizontal axis, we show the percentage of the population which is targeted and on the vertical axis the percentage of found positive records. A dotted diagonal illustrates a random model (i.e., targeting 40% of the population will find 40% of the positive records). . A wizard curve above shows a perfect model (there are 15% positive records in our test set, so a perfect model would target only this) . The actual percentage of actual positives found by this model. The steeper the curve, the better. . Per-bin lift chart | . This chart sorts the observations by deciles of decreasing predicted probability. It shows the lift in each of the bins. . If there is 20% of positives in our test set, but 60% in the first bin of probability then the lift of this first bin is 3. . This means that targeting only the observations in this bin would yield 3 times as many positive results as a random sampling (equally sized bars at the level of the dotted line). . The bars should decrease progressively from left to right, and the higher the bars on first probability decile, the better. . Hyperparameter tuning . Hyperparametrs is an important role on the model process. Some hyperparameters influence the speed of training. We use the Ramdom search and the metrics used to rank hyperparameter points are computed by cross-validation. In K-fold cross-validation the dataset is partitioned into k equally sized subsets. We select k=5 and then, 4 subsets are used as folded train sets while the remaining subset is retained to validate the model. This process is then repeated 5 times, once for each fold defined by the subset used as validation set. . . Figure 28: K-fold cross validation . Machine learning algorithms . We partition our dataset into 70% of the data for the training and 30% for testing. The training set is used by the machine learning algorithm to train the model and the test set is used for reporting the upcoming performance results: During the training phase, DSS “holds out” on the test set, and the model is only trained on the train set. Once the model is trained, DSS evaluates its performance on the test set. This ensures that the evaluation is done on data that the model has “never seen before”. . We optimize our model hyperparameters for the cumulative lift and we optimize the threshold for the F1 score. We compute the lift at 30% and we define a cost matrix, which evaluates a ‘gain’ brought by this model based on the following hypothesis: . If the model predicts that the target variable ‘casse_r2’ is true: . And it is indeed true, the gain is 1 . | But it is not true, the gain is -0.3 . | . | If the model predicts that ‘casse_r2’ is false: . And it is indeed false the gain is 0 . | But it is actually true, the gain is 0 . | . | . The algorithms selected for the training need to be in accordance with the following main properties: . Explainability: The model require explanation for a non-technical audience. The most accurate machine learning algorithms and models are called ‘black boxes’. They make very few prediction errors but it may be difficult to understand and harder to explain. Examples of such models are deep neural networks or Ensemble models. In contrast, linear regression or decision tree learning algorithms are easy to interpret by a non-expert. . | Number of features and examples: Some algorithms can handle a huge number of examples and millions of features. Others can be relatively modest in their capacity. . | Training speed: Simple algorithms like random forest and logistic regression are robust and faster in the training. Others are slow to train and may need retraining frequently. . | Prediction speed: Selected models will be used in production where very high throughput is required when generating predictions. . Random Forest . | | . Random Forest is made of many decision trees. Each tree in the forest predicts a record, and each tree “votes” for the final answer of the forest. The forest chooses the class having the most votes. It’s a simple algorithm which builds a decision tree. Each node of the decision tree includes a condition on one of the input features. When “growing” (ie, training) the forest: . for each tree, a random sample of the training set is used. We set number of trees between 100 and 700. The maximum depth of tree is between 1 and 3. And the minimum samples per leaf is set between 14 and 50. . | for each decision point in the tree, a random subset of the input features is considered. The algorithm default is set by selecting the square root of the number of features. . | . Random Forests generally provide good results, at the expense of explainability of the model. . Gradient Boosted Trees . Gradient boosting is a technique which produces a prediction model in the form of an ensemble of “weak” prediction models (small decision trees). The concept is to train a set of decision trees (weak learners) to create a final strong learner. This is an iterative method. After each tree is trained, the data is reweighted: samples that were misclassified gain weight while the correctly classified ones lose weight. This allows future weak learners to focus on the “difficult” examples that the previous weak learners missed. . Gradient Boosted Trees is a generalization of boosting to arbitrary differentiable loss functions. GBT is an accurate and effective off-the-shelf procedure that can be used for both regression and classification problems. We set the number of boosting stages between 80 and 300. The feature sampling strategy is set to automatic mode by selecting 30% of the features and the learning rate is between 0.05 and 0.5. The advantages of GBRT are: . Natural handling of data of mixed type (= heterogeneous features) . | Predictive power . | Robustness to outliers in output space (via robust deviance loss functions) and over-fitting . | . Due to the iterative nature of boosting, it is not very parallelizable and is less scalable than other algorithms. . Logistic Regression . Logistic Regression is a classification algorithm using a linear model (i.e., it computes the target feature as a linear combination of input features). Logistic Regression minimizes a specific cost function (called logit or sigmoid function), which makes it appropriate for classification. A simple Logistic regression algorithm is prone to overfitting and sensitive to errors in the input dataset. To address these issues, it is possible to use a penalty (or regularization term) to the weights. This implementation can use either ‘L1’ or ‘L2’ regularization terms. We choose the L2 regularization with the corresponding inverse of the regularization parameter between 0.01 and 100. . Experiment results . The obtained results are presented as follow: . Confusion matrix and decision chart . Random forest: | . . . Figure 29: Confusion matrix and decision chart – Random forest . The optimal thresold to achieve the highest F1 score is set at 0.150 where we achieve 71% accuracy and 29% F1-score. Based on the model, the bank will lose customers who were denied a Coup de Pouce loan based on the model’s prediction that they would be defaulters. . Gradiant boosting : | . . . Figure 30: Confusion matrix and decision chart – Gradiant boosting . The optimal thresold to achieve the highest F1 score is set at 0.125 where we achieve 52% accuracy and 27% F1-score. The model correctly identifying True Positives at 60% rate. The precision is low at 18%. . Logistic regression: | . . . Figure 31: Confusion matrix and decision chart – Logistic regression . The optimal thresold to achieve the highest F1 score is set at 0.075 where we achieve 33% accuracy and 27% F1-score. The model correctly identifies 83% of default cases. The precision is low at 18%. . Lift curve . Random forest: | . . Figure 32: Cumulative Lift Curve chart – Random forest . This chart describes the ‘gain’ in targeting a given percentage of the total number of customers using the highest modelled probabilities of default, rather than targeting them at random. This model suggests that the top 30% (decile 1, 2 and 3) of customers with the highest predicted probabilities of default contain approximately 45% of non-payers rather than capturing 30% of non-payers. . . Figure 33: Per-bin lift chart – Random forest . The lift curve of this model is extended highly into the first decile. At decile group 1, 2 and 3, we have a lift of 1.49 which corresponds with there being 1.49 the number of non-payers captured compared with the number we would expect at random. . Gradient boosting: | . . Figure 34: Cumulative Lift Curve chart – Gradient boosting . This model suggests that the top 30% (decile 1, 2 and 3) of customers with the highest predicted probabilities of default contain approximately 35% of non-payers rather than capturing 30% of non-payers. . . Figure 35: Per-bin lift chart – Gradient boosting . On the other hand, targeting 30% of population by decreasing probability would results at identifying only 1.238 of the number of non-payers compared by a random model. The lift curve is not decreasing by decile as a good classifier has to give us a high lift when we act on only a few cases and as we include more cases the lift has to be decreasing. Also, the lift curve for the best possible classifier has to overlap the existing curve at the start which is not the case at the third decile where the lift is at 0.97. . Logistic regression: | . . Figure 36: Cumulative Lift Curve chart – Logistic regression . This model suggests that the top 30% (decile 1, 2 and 3) of customers with the highest predicted probabilities of default contain approximately also 35% of non-payers rather than capturing 30% of non-payers. . . Figure 37: Per-bin lift chart – Logistic regression . On the other hand, targeting 30% of population by decreasing probability would results at identifying only 1.222 of the number of non-payers compared to a random model. Same as before, the lift curve is not decreasing by decile and overlay at the third decile with a lift of 0.99. . ROC Curve . Random forest: | . The AUC (Area Under the Curve) for this model is 0.612, which is fair. . . Figure 38: ROC Curve – Random forest . Gradient boosting trees: | . The AUC (Area Under the Curve) for this model is 0.571, which is not very good. . . Figure 39: ROC Curve – Gradient boosting . Logistic regression: | . The AUC (Area Under the Curve) for this model is 0.570, which is not very good. . . Figure 40: ROC Curve – Logistic regression . Detailed metrics . Random forest: | . Threshold-dependent (current threshold = 0.1500) . Accuracy Proportion of correct predictions (positive and negative) in the test set | 0.7094 | . Precision Proportion of positive predictions that were indeed positive (in the test set) | 0.2259 | . Recall Proportion of actual positive values found by the classifier | 0.3877 | . F1 Score Harmonic mean between Precision and Recall | 0.2855 | . Hamming loss Fraction of labels that are incorrectly predicted (the lower the better) | 0.2906 | . Cost matrix gain Average gain per record that the test set (2825 rows) would yield given the specified gain for each outcome. Specified gains: TP = 1, TN = 0, FP = -0.3, FN = 0. | -0.0016 | . Matthews Correlation Coefficient Correlation coefficient between actual and predicted values. +1 = perfect, 0 = no correlation, -1 = perfect anti-correlation | 0.1255 | . Threshold-independent | | . Log loss Error metric that takes into account the predicted probabilities (the lower the better) | 0.4175 | . ROC - AUC Score Area under the ROC; from 0.5 (random model) to 1 (perfect model) | 0.6118 | . Calibration loss Average distance between calibration curve and diagonal. From 0 (perfectly calibrated) up to 0.5. | 0.0118 | . Gradient boosting trees: | . Threshold-dependent (current threshold = 0.1250) . Accuracy Proportion of correct predictions (positive and negative) in the test set | 0.5242 | . Precision Proportion of positive predictions that were indeed positive (in the test set) | 0.1773 | . Recall Proportion of actual positive values found by the classifier | 0.5981 | . F1 Score Harmonic mean between Precision and Recall | 0.2735 | . Hamming loss Fraction of labels that are incorrectly predicted (the lower the better) | 0.4758 | . Cost matrix gain Average gain per record that the test set (2825 rows) would yield given the specified gain for each outcome. Specified gains: TP = 1, TN = 0, FP = -0.3, FN = 0. | -0.0351 | . Matthews Correlation Coefficient Correlation coefficient between actual and predicted values. +1 = perfect, 0 = no correlation, -1 = perfect anti-correlation | 0.0780 | . Threshold-independent | | . Log loss Error metric that takes into account the predicted probabilities (the lower the better) | 0.4292 | . ROC - AUC Score Area under the ROC; from 0.5 (random model) to 1 (perfect model) | 0.5706 | . Calibration loss Average distance between calibration curve and diagonal. From 0 (perfectly calibrated) up to 0.5. | 0.0285 | . Logistic regression: | . Threshold-dependent (current threshold = 0.0750) . Accuracy Proportion of correct predictions (positive and negative) in the test set | 0.3292 | . Precision Proportion of positive predictions that were indeed positive (in the test set) | 0.1621 | . Recall Proportion of actual positive values found by the classifier | 0.8345 | . F1 Score Harmonic mean between Precision and Recall | 0.2714 | . Hamming loss Fraction of labels that are incorrectly predicted (the lower the better) | 0.6708 | . Cost matrix gain Average gain per record that the test set (2825 rows) would yield given the specified gain for each outcome. Specified gains: TP = 1, TN = 0, FP = -0.3, FN = 0. | -0.0688 | . Matthews Correlation Coefficient Correlation coefficient between actual and predicted values. +1 = perfect, 0 = no correlation, -1 = perfect anti-correlation | 0.0635 | . Threshold-independent | | . Log loss Error metric that takes into account the predicted probabilities (the lower the better) | 0.4331 | . ROC - AUC Score Area under the ROC; from 0.5 (random model) to 1 (perfect model) | 0.5703 | . Calibration loss Average distance between calibration curve and diagonal. From 0 (perfectly calibrated) up to 0.5. | 0.0411 | . Based on this analysis, the best model is Random forest which the highest 30% lift of 1.49: We illustrate the performance metrics on the Train and Test set: . TEST_precision TEST_recall TEST_auc TEST_f1 TEST_accuracy TEST_lift . 0.2258 | 0.3877 | 0.6118 | 0.2854 | 0.7093 | 1.490 | . TRAIN_precision | TRAIN_recall | TRAIN_auc | TRAIN_f1 | TRAIN_accuracy | TRAIN_lift | . 0.2619 | 0.4884 | 0.6931 | 0.3410 | 0.7399 | 1.7788 | . We can identify if the model has overfit by first evaluating the model on the training dataset and then evaluating the same model on a holdout test dataset. If the performance of the model on the training dataset is significantly better than the performance on the test dataset, by computing the difference between the metrics obtained on Train and Test set, then the model may have overfit the training dataset. By looking at the difference ratio on AUC and Lift, we conclude that we model have a good performance and doesn’t overfit. . Diff_Lift Diff_AUC Diff_precision Diff_recal Diff_accuracy . 0.28 | 0.08 | 0.03 | 0.10 | 0.03 | . Interpretation . Feature importance . ## . Several measures are available for feature importance in Random Forests: . Gini Importance or Mean Decrease in Impurity (MDI) calculates each feature importance as the sum over the number of splits (across all tress) that include the feature, proportionally to the number of samples it splits. Dataiku DSS extract feature importance provided by the fitted attribute feature_importances_ of Scikit-Learn and outputs the following result: . . Figure 41: Feature importance . Partial dependence . A partial dependence plot shows the dependence of the predicted response on a single feature. The x axis displays the value of the selected feature, while the y axis displays the partial dependence. The value of the partial dependence is by how much the log-odds are higher or lower than those of the average probability. In other words, the partial dependence value shows how the probability of being in the default class changes across different values of the feature. . The log-odds for a probability p are defined as log(p / (1 - p)). They are strictly increasing, ie. higher log odds means higher probability of default. We illustrate this method with several variables: . clt_depart_str | . We represent 30 most frequent modalities of ‘clt_dep_str’, computed on 2825 rows (the full test set) . . Figure 42: Partial dependency of client’s department . The client department is considered to be the most important feature by the model. We notice that several departments increase the probability of default whereas other departments are less likeliy to do so. The department of Val-de-Marne (94) representing 3.2% of the data increases the probability of default. In the other hand, the department of Gironde (33) representing 2.5% of the data decreases the probability of default. . Insee_prop_pop25 | . We represent 50 bins for ‘insee_prop_pop25p’, computed on 2825 rows (the full test set) . . Figure 43: Partial dependency of the proportion of Insee population under 25 years old . The proportion of population under 25 years old ‘insee_prop_pop25’ is considered as an important feature in the model. Let’s investigate the following question: “To what extent does a client’s chances of defaulting depend on the proportion of the proportion of population under 25 years?” . Let’s first examine the trend line. We can see that as the proportion rate increases, the clients are steadily less likely to default. In addition to the trend line, we also have the distribution of the variable. We notice that the proportion of population under 25 years between 30% and 40% represents a major part of the data and we might be surprised that the probability of default goes up as this proportion exceed 50% but the small amount of data in this bracket could be a reason for the small dip. . request_date_day_str | . We represent 30 most frequent modalities of ‘request_date_day_str’, computed on 2825 rows (the full test set) . . Figure 44: Partial dependency of the request day . The day of the request is also considered to be an important feature by the model. We notice that orders done on first days of the month representing 16% of the data decreases the likelihood of default. On the other hand, orders done on the second part of the month, between 15 to 30, representing 15% of the data increases the likelihood of default. . Subpopulation analysis . In this part, we investigate whether our model behaves identically across different subpopulation using different metrics like the lift, AUC, precision and the recall. We illustrate this method with the following variables: . clt_sex: | . We have two modalities for ‘clt_sex’ (Femme,Homme), computed on 2825 rows (test set). . . For the two largest subgroups in this case, common metrics like lift, ROC AUC, accuracy, precision, and recall appear to be quite close. The female modality represents 55% of the data with 14% of default cases, 1% below the average default rate, among this modality. The model predicts 26% of unpaid loans among females which is 5% below the average predicated default rate. On the other hand, the male modality represents 45% of the data with 16% of default cases, 1% above the average default rate, among this modality. However, the model predicts 32% of unpaid loans among males which is 6% above the average predicated default rate. . clt_marie: | . We have three modalities for ‘clt_marie’ (Non marié, Marié, and Non renseigné), computed on 2825 rows (test set). . . As seen before, common metrics like lift, ROC AUC, accuracy, precision, and recall appear to be quite close. The model predicts more default cases for non-married clients whereas for married clients seem less likely to default (only 6% of predict default cases among this modality). . tracabilite: | . This variable describes the origin of the demand, detailed in previous chapters, computed on 2825 rows (test set). . . We notice that the model struggles to define the default from demands coming from ‘Refus PPR Leadin’ or where the origin is unknown. However, ‘PPRTOCDP’, which represents the large proportion of Coup de Pouce Upcucle demands, the model seems to better define the default event. . Individual analysis . Individual prediction explanations are feature importance specific to a given sample. When the model is linear (logistic regression, OLS…), the explanation for one feature is simply the impact of the feature on the prediction with the mean feature value as a baseline: coefficient * (feature value - mean feature value). As a generalization, the explanation is the difference between the prediction value and the average of prediction values obtained by replacing the feature value by values drawn from the test dataset. This method approximates Shapley values, trading off speed against both bias and variance. . For classification problems, the explanations are computed probability log-odd ratios: log(p / (1 - p)). . Using DSS Dataiku, we can compute the explanations for the most influential features for extreme probabilities using the method: Shapley Additive exPlanations. The goal of SHAP is to explain the prediction of an instance x by computing the contribution of each feature to the prediction. The SHAP explanation method computes Shapley values from coalitional game theory. . . Figure 45: Probability density and predicted probability . We illustrate 4 examples with the most influential features of below the average of default probability (non-payment loan event) and above the average of default probability (payment loan event) . . Conclusion . In conclusion, one limitation of a credit scoring model is that the model output is only as good as the input that is used. If data going into the scoring model is inaccurate (for instance, client’s score on PPR and PPC loans or the origin of Coup de Pouce UPCYCLE demand, etc..), the model’s score output will be erroneous. Depending on how the erroneous information is weighted in the scoring formula, the impact on the score could be substantial. . The effectiveness of the model score output can also be constrained by factors such as changing economic conditions and business environments. As illustrated in previous chapters, business providers, managed by the commercial team, can vary frequently and bring different client’s profiles. . A model is typically developed for a certain target population and may be difficult to adapt to other populations. In most cases, a credit scoring model should only be used for the product, range of loan size, and market that it was developed for. Granting small loans like 500€ or 1000€ to refusals whose original loan’s demand is ranging between 5.000€ and 15.000€ can be challenging. When a bank tries to adapt the model to a different population, performance of that population may likely deviate from expectation. . Moreover, models are calibrated using historical data (in our case, we had to add a different modality, which is not going to be evaluated by the model in the production environment, in our credit score modeling). So, if relevant un-modeled conditions change, the model can have trouble forecasting out of sample. . Appendix . . Figure 46: Dataiku flux of UPCYCLE score modeling . Bibliography . https://databricks.com/blog/2019/09/18/productionizing-machine-learning-from-deployment-to-drift-detection.html . G. Szepannek. On the practical relevance of modern machine learning algorithms for credit scoringapplications.WIAS Report Series, 29:88–96, 2017. . https://doi.org/10.20347/wias.report.29 . https://journal.fsv.cuni.cz/storage/1228_rezac.pdf . https://www.mwsug.org/proceedings/2013/AA/MWSUG-2013-AA14.pdf . https://sundarstyles89.medium.com/weight-of-evidence-and-information-value-using-python-6f05072e83eb . https://www2.deloitte.com/content/dam/Deloitte/global/Documents/Financial-Services/gx-be-aers-fsi-credit-scoring.pdf . https://medium.com/crim/manipuler-les-variables-cat%C3%A9goriques-dans-un-jeu-de-donn%C3%A9es-6973c54c9827 . https://cran.r-project.org/web/packages/smbinning/smbinning.pdf . https://www.ariclabarr.com/credit-modeling.html . http://papersjds14.sfds.asso.fr/submission_48.pdf . https://www.fdic.gov/regulations/examinations/credit_card/pdf_version/ch8.pdf . https://arxiv.org/pdf/2006.11835.pdf . https://www.researchgate.net/publication/309727706_Application_Scorecard_Modeling_Techniques_and_Performance . https://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_importances.html . http://eric.univ-lyon2.fr/~ricco/tanagra/fichiers/fr_Tanagra_R_Python_PDP.pdf . https://www.aquiladata.fr/insights/shap-mieux-comprendre-linterpretation-de-modeles/ . https://christophm.github.io/interpretable-ml-book/shap.html .",
            "url": "https://younesszaim.github.io/myportfolio/2021/09/15/Scoring-in-FLOA-BANK.html",
            "relUrl": "/2021/09/15/Scoring-in-FLOA-BANK.html",
            "date": " • Sep 15, 2021"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Hi ! I’m Youness ZAIM. . Please check my resume on Linkedin. . You can also email me on zaimyouness9797@gmail.com. .",
          "url": "https://younesszaim.github.io/myportfolio/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://younesszaim.github.io/myportfolio/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}