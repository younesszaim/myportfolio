{
  
    
        "post0": {
            "title": "Uscensus",
            "content": "US cencus income : Ensembles, Bagging and Shap Values . toc: true | badges: true | comments: true | categories: [fastpages, jupyter] | image: images/shap_values.png | . Problem Framework . Our task is to determine the income level for the person represented by the record. Incomes have been binned at the $50K level to present a binary classification problem. . The dataset used in this analysis was extracted from the census bureau database found at. The data was split into train/test in approximately 2/3, 1/3 proportions. . The following mappings of the data is as follow : . import pandas as pd from pandas.api.types import is_string_dtype, is_numeric_dtype, is_categorical_dtype import numpy as np df_labels = pd.read_csv(f&#39;{PATH}/census_income_metadata_column.csv&#39;, sep=&#39;;&#39;) df_labels.head(5) . In any sort of data science work, it’s important to look at our data directly to make sure we understand the format, how it’s stored, what types of values it holds, etc. Even if we’ve read a description of the data, the actual data may not be what we expect. We’ll start by reading the training set into a Pandas DataFrame : . # Loading the train data df = pd.read_csv(f&#39;{PATH}/census_income_learn.csv&#39;, names = df_labels[&#39;column_name&#39;]) df.shape . Let’s have a look at the columns, their types defined by Pandas and compared it to their actual mapping types : . # Chekcing the mapping of the data d1 = df.dtypes.apply(lambda x: x.name).to_dict() d2 = {c: d for c,d in zip(df_labels[&#39;column_name&#39;],df_labels[&#39;dtype&#39;])} mapping = [d1, d2] d = {} for k in d1.keys(): d[k] = tuple(d[k] for d in mapping) d . We can see that detailed_industry_recode, detailed_occupation_recode, own_business_or_self_employed, veterans_benefits and year is set by default as a continuos category. . Let’s redifined their types : . # Correcting data types d1[&#39;detailed_industry_recode&#39;]=&#39;object&#39; d1[&#39;detailed_occupation_recode&#39;]=&#39;object&#39; d1[&#39;own_business_or_self_employed&#39;]=&#39;object&#39; d1[&#39;veterans_benefits&#39;]=&#39;object&#39; d1[&#39;year&#39;]=&#39;object&#39; . Let’s reload the data with its correspind feature’s mapping : . # reload data with coorexted types df = pd.read_csv(f&#39;{PATH}/census_income_learn.csv&#39;, names =df_labels[&#39;column_name&#39;], dtype= d1) . The info() method is useful to get a quick description of the data, in particular the total number of rows, each attribute’s type, and the number of nonnull values : . df.info() # dispplay first rows with pd.option_context(&#39;display.max_rows&#39;, None, &#39;display.max_columns&#39;, None): display(df.head(3)) # drop &#39;ignore&#39; column df.drop(&#39;ignore&#39;, axis=1,inplace=True) # list columns df.columns . We load the test set with the same training data types : . # loading the test set test = pd.read_csv(f&#39;{PATH}/census_income_test.csv&#39;, names =df_labels[&#39;column_name&#39;], dtype= d1 ) test.info() . We verify if we got the same columns both on the train and the test set : . # checking columns on test set which not in train set(test.columns).difference(set(df.columns)) # dropping &#39;ignore&#39; columns test.drop(&#39;ignore&#39;, inplace=True, axis=1) test.columns # display first rows of the test set with pd.option_context(&#39;display.max_rows&#39;, None, &#39;display.max_columns&#39;, None): display(test.head(3)) df.shape, test.shape . Looking at the data . The most important data column is the dependent variable—that is, the one we want to predict which is income_level : . dep_var = &#39;income_level&#39; . Let’s see its distribution : . print(df[dep_var].value_counts(normalize = True)) df[dep_var].value_counts(normalize = True).plot(kind=&#39;bar&#39;, edgecolor=&#39;black&#39;, color=&#39;blue&#39;, title=&#39;income_level&#39;) . We have an imbalanced dataset where the income level of -50k is representing more than 93% of the total records. . Next, we automatically handle which columns are continuous and which are categorical : . # get categorical and numerical variables def cont_cat_split(df, dep_var=None): &quot;Helper function that returns column names of cont and cat variables from given `df`.&quot; cont_names, cat_names = [], [] for label in df: if label in [dep_var]: continue if (pd.api.types.is_integer_dtype(df[label].dtype) or pd.api.types.is_float_dtype(df[label].dtype)): cont_names.append(label) else: cat_names.append(label) return cont_names, cat_names cont, cat = cont_cat_split(df, dep_var= dep_var) cont , cat . Let’s start by checking the modalties of our categorical variables : . Some categorical features are purely nominal-having multiple modalities (with modality ? for nan values) and others are ordinal columns like education and year: . # Ediucation modalities df[&#39;education&#39;].unique(), df[&#39;education&#39;].nunique() # Year modalities df[&#39;year&#39;].unique(), df[&#39;year&#39;].nunique() . We can tell Pandas about a suitable ordering of these levels like so: . # Setting the order of education variable education = &#39; Children&#39;,&#39; Less than 1st grade&#39;,&#39; 1st 2nd 3rd or 4th grade&#39;,&#39; 5th or 6th grade&#39;, &#39; 7th and 8th grade&#39;,&#39; 9th grade&#39;,&#39; 10th grade&#39;,&#39; 11th grade&#39;, &#39; 12th grade no diploma&#39;, &#39; High school graduate&#39;, &#39; Associates degree-academic program&#39;,&#39; Associates degree-occup /vocational&#39;, &#39; Prof school degree (MD DDS DVM LLB JD)&#39;,&#39; Some college but no degree&#39;,&#39; Bachelors degree(BA AB BS)&#39;, &#39; Masters degree(MA MS MEng MEd MSW MBA)&#39;,&#39; Doctorate degree(PhD EdD)&#39; len(education) # Setting the order of year variaable year = &#39;94&#39;, &#39;95&#39; # apply the defined ordering fot our data : df[&#39;education&#39;] = df[&#39;education&#39;].astype(&#39;category&#39;) df[&#39;education&#39;].cat.set_categories(education, ordered=True, inplace=True) df[&#39;year&#39;] = df[&#39;year&#39;].astype(&#39;category&#39;) df[&#39;year&#39;].cat.set_categories(year, ordered=True, inplace=True) #Same for test set : test[&#39;education&#39;] = test[&#39;education&#39;].astype(&#39;category&#39;) test[&#39;education&#39;].cat.set_categories(education, ordered=True, inplace=True) test[&#39;year&#39;] = test[&#39;year&#39;].astype(&#39;category&#39;) test[&#39;year&#39;].cat.set_categories(year, ordered=True, inplace=True) . Lets check our continous features: The describe() method shows a summary of the numerical attributes . df[cont].describe() . The count, mean, min, and max rows are self-explanatory.The std row shows the standard deviation, which measures how dispersed the values are. The 25%, 50%, and 75% rows show the corresponding percentiles. . We plot a histogram for each numerical attribute : . %matplotlib inline import matplotlib.pyplot as plt df[cont].hist(bins=50, figsize=(20,15)) plt.show() . We can see that these attributes have very different scales. . | Some numerical varaibles are countinous like age and others are discrete and finite like weeks_worked_in_year or infinete num_persons_worked_for_employer. . | Some features as wage_per_hour,capital_gains,capital_losses,dividends_from_stocks are tail-heavy: they extend much farther to the median right with high coefficient of variation : . | . df[cont].boxplot(column=[&#39;wage_per_hour&#39;,&#39;capital_gains&#39;,&#39;capital_losses&#39;,&#39;dividends_from_stocks&#39;], figsize=(10,5)) . We can see the presence of extreme values for those features. . Using the skewness value, which explains the extent to which the data is normally distributed, in order to confirm that. Ideally, the skewness value should be between -1 and +1, and any major deviation from this range indicates the presence of extreme values. . We can calculate the skwenss value : . # skewness value df[[&#39;wage_per_hour&#39;,&#39;capital_gains&#39;,&#39;capital_losses&#39;,&#39;dividends_from_stocks&#39;]].skew() . Using the IQR score, let’s see the number of obseravtions that are not in the (Q1 - 1.5 IQR) and (Q3 + 1.5 IQR) range : . # IQR score Q1 = df[[&#39;wage_per_hour&#39;,&#39;capital_gains&#39;,&#39;capital_losses&#39;,&#39;dividends_from_stocks&#39;]].quantile(0.25) Q3 = df[[&#39;wage_per_hour&#39;,&#39;capital_gains&#39;,&#39;capital_losses&#39;,&#39;dividends_from_stocks&#39;]].quantile(0.75) IQR = Q3 - Q1 print(IQR) # number of observation out of the definied range out = df[[&#39;wage_per_hour&#39;,&#39;capital_gains&#39;,&#39;capital_losses&#39;,&#39;dividends_from_stocks&#39;]] df_out = out[((out &lt; (Q1 - 1.5 * IQR)) |(out &gt; (Q3 + 1.5 * IQR))).any(axis=1)] out.shape, df_out.shape, df_out.shape[0]/out.shape[0] . From 199.523 observation of the selcted features, 38.859 records (19%) represent extrem values. . For weeks_worked_in_year : . df[&#39;weeks_worked_in_year&#39;].plot( kind=&#39;hist&#39;, bins=53, edgecolor=&#39;black&#39;, color=&#39;blue&#39;, title=&#39;weeks worked in a year&#39;, figsize=(10,5)) . For num_persons_worked_for_employer : . df[&#39;num_persons_worked_for_employer&#39;].plot( kind=&#39;hist&#39;, edgecolor=&#39;black&#39;, color=&#39;blue&#39;, title=&#39;num_persons_worked_for_employer&#39;, figsize=(7,5)) . We notice an increase in the 7th bins num_persons_worked_for_employer=6. Check if this variable is capped ? . Exploratory data analysis . Starting with numerical variables : | . import seaborn as sns data_dia = df[dep_var] data = df[[&#39;age&#39;]] data = pd.concat([data_dia,data],axis=1) data = pd.melt(data,id_vars=&quot;income_level&quot;, var_name=&quot;features&quot;, value_name=&#39;value&#39;) plt.figure(figsize=(6,5)) sns.violinplot(x=&quot;features&quot;, y=&quot;value&quot;, hue=&quot;income_level&quot;, data=data,split=True, inner=&quot;quartile&quot;) plt.xticks(rotation=90) . For the age feature, we can see that the medians of the income levels +/- 50k look separated. The income level of +50k with a median of 50 years old has a lower interquntile range (IQR) with value spread of 10 years. Whereas The income level of -50k has a median of 30 years old has and interquantile range (IQR) of 40 years. So, age can be good for classification. . Let’s look at the weeks_worked_in_year feature : . # get the number of income class in each week weeks_worked_in_year = df.groupby([&quot;weeks_worked_in_year&quot;, &quot;income_level&quot;]) .size() .groupby(level=0).apply(lambda x: 100*x/x.sum()).unstack() # print the percentage class for the first and last weeks weeks_worked_in_year.iloc[[0,1,2, -3,-2,-1]] weeks_worked_in_year.plot(kind=&#39;bar&#39;, stacked=True, edgecolor=&#39;black&#39;, figsize=(12,5)) . We can see that the propotion of people making more than 50k a year is increasing with the number of working weeks in a given year where it can reach more than 14% for those working 52 weeks . However, the -50k level of income is representing the higher propotion regardless of the number of working weeks. We notice that among those how don’t work at all, 0.6% still make more than 50k a year. . Let’s look at num_persons_worked_for_employer : . num_persons_worked_for_employer = df.groupby([&quot;num_persons_worked_for_employer&quot;, &quot;income_level&quot;]) .size() .groupby(level=0).apply(lambda x: 100*x/x.sum()).unstack() num_persons_worked_for_employer num_persons_worked_for_employer.plot(kind=&#39;bar&#39;, stacked=True, edgecolor=&#39;black&#39;, figsize=(12,5)) . The proportion of +50k income level increases with the number of the num_preson_worked_for_employer where it reaches 16% for num_preson_worked_for_employer= 6. . Let’s see the average of wage_per_hour,capital_gains,capital_losses,dividends_from_stocks across the income levels : . avg = df[[&#39;wage_per_hour&#39;,&#39;capital_gains&#39;,&#39;capital_losses&#39;,&#39;dividends_from_stocks&#39;,&#39;income_level&#39;]] .groupby(&#39;income_level&#39;) .mean() avg.plot(kind=&#39;bar&#39;, title = &#39;average across income levels&#39;, figsize=(10,5)) avg . We can see that people making more than 50k a year, have on average, higher wage per hour,higher return on capital asset and dividends from stock options. . Next, let’s analyse some categorical variables : | . # Education variable pd.crosstab(df[&#39;income_level&#39;], df[&#39;education&#39;], margins = True, normalize = &#39;columns&#39;).style.format(&#39;{:.2%}&#39;) pd.crosstab(df[&#39;income_level&#39;], df[&#39;education&#39;], margins = True, normalize = &#39;columns&#39;).plot(kind=&#39;bar&#39;,stacked=True, edgecolor=&#39;black&#39;, figsize=(12,10)) plt.legend(bbox_to_anchor=(1.5, 1.0)) . We can see the effect of education on income level where more than 50% of Prof school degree and Doctorate degree earn more than 50k a year. On the other hand, the majority of people (more than 90%) with no degree earn less than 50k a year. . Let’s further this analysis and see the effect of education and the number of working weeks : . pd.crosstab(df[&#39;income_level&#39;], df[&#39;education&#39;], values = df[&#39;weeks_worked_in_year&#39;], aggfunc = &#39;mean&#39;).round(2) import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns plt.figure(figsize=(20, 4)) sns.heatmap( pd.crosstab(df[&#39;income_level&#39;], df[&#39;education&#39;], values = df[&#39;weeks_worked_in_year&#39;], aggfunc = &#39;mean&#39;).round(1) ,annot = True ,linewidths=.5 ,cmap=&quot;YlGnBu&quot; ) plt.show() . We can see that earning more than 50k a year demands high level of education but also lot of hard work ! . Let’s analyse the effect of sex and marital_stat on income level : . pd.crosstab(df[&#39;income_level&#39;], [df[&#39;sex&#39;],df[&#39;marital_stat&#39;]], margins = True, normalize = &#39;columns&#39;).style.format(&#39;{:.2%}&#39;) pd.crosstab(df[&#39;income_level&#39;], [df[&#39;sex&#39;],df[&#39;marital_stat&#39;]]).plot(kind=&#39;bar&#39;,stacked=True, edgecolor=&#39;black&#39;, figsize=(12,10)) . We can see that the highest proportion of people earning less than 50k a year are mostly female Married-A F spouse present or never married and seperated male. On the other hand, male Married-civilian spouse present represent the highest propotion on the +50k income level. . We can further the analysis more as we have got many interesting features with several modalities but for now let’s see how machine learning models can help us understanding more our data. . Data preparation . We set the feature vector and the target variable : . # setting feature vector and target variable for the train set X = df.drop([&#39;income_level&#39;], axis = 1) y = df[&#39;income_level&#39;] # setting feature vector and target variable for the test set test_x = test.drop([&#39;income_level&#39;], axis = 1) test_y = test[&#39;income_level&#39;] # Cheching the result df.shape, X.shape, y.shape . We will keep the provided test set hidden and will use it as a realtime dataset when we make our model on production in order to avoid the risk of data snooping. . For that, we will be using a validation set derived from our training set (30%). Scikit-Learn provides a few functions to split datasets into multiple subsets in various ways. The simplest function is train_test_split(). . Since we have an imbalanced dataset, we can’t considered purely random sampling methods. For that, we do stratified sampling based on the income level. . from sklearn.model_selection import train_test_split X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.3, random_state=12, stratify=y) # Checking the train and validation set X_train.shape, X_val.shape # Checking the income level proportion y_train.value_counts(normalize = True), y_val.value_counts(normalize = True) . What if we didn’t stratify with respect to income level ? . We can compare the income level proportions in the overall dataset, in the test set generated with stratified sampling, and in a test set generated using purely random sampling. . def income_cat_proportions(data): return data[&quot;income_level&quot;].value_counts() / len(data) train_set, test_set = train_test_split(df, test_size=0.3, random_state=12) train_set, test_set_strat = train_test_split(df, test_size=0.3, random_state=12,stratify=df[&#39;income_level&#39;]) compare_props = pd.DataFrame({ &quot;Overall&quot;: income_cat_proportions(df), &quot;Stratified&quot;: income_cat_proportions(test_set_strat), &quot;Random&quot;: income_cat_proportions(test_set), }).sort_index() compare_props[&quot;Rand. %error&quot;] = 100 * compare_props[&quot;Random&quot;] / compare_props[&quot;Overall&quot;] - 100 compare_props[&quot;Strat. %error&quot;] = 100 * compare_props[&quot;Stratified&quot;] / compare_props[&quot;Overall&quot;] - 100 . As we can see, the test set generated using stratified sampling has income level proportions almost identical to those in the full dataset, whereas the test set generated using purely random sampling is skewed. . compare_props . Now that we defined our training set, It’s time to prepare the data for our machine Learning algorithms. . Data cleaning : | . We have seen previously that we don’t have any missing values. For some cataegorical features, we assumed that the ? modality is encoded for NaN values. . Handling Text and Categorical Attributes : | . Strating with the target variable income_level, we use LabelEncoder() to encode target labels with value between 0 and n_classes-1 = 1. . We have seen also that we have some ordinal variable as education and year, so we use OrdinalEncoder to encode the categorical features as an integer array. The results in a single column of integers (0 to n_categories - 1) per feature. . Since the remaining categorical features have several modalities per feature, we use also OrdinalEncoder instead of OneHotEncoder. . Working with OneHotEncoder leads, in our case, to high memory consumption. We can combine OneHotEncoder and PCA : The benefit in PCA is that combination of N attributes is better than any individual attribute. And the disadvantage is in harder explanation what exactly that PCA component means. Therefore, for this work, we will sacrifice a bit of predictive power to get more understandable model. . # categorical variables encoding from sklearn.preprocessing import LabelEncoder, OrdinalEncoder # For the traget varaible le = LabelEncoder() y_train = le.fit_transform(y_train) #fit on training set y_val = le.transform(y_val) test_y = le.transform(test_y) # For categorical features : Or = OrdinalEncoder(handle_unknown=&#39;use_encoded_value&#39;, unknown_value = -1) for c in cat : X_train[c] = Or.fit_transform(np.array(X_train[c]).reshape(-1,1).astype(str)) #fit on training set X_val[c] = Or.transform(np.array(X_val[c]).reshape(-1,1).astype(str)) test_x[c] = Or.transform(np.array(test_x[c]).reshape(-1,1).astype(str)) # Cheking categorical features encoding X_train[cat].head(3) # Cheking target feature encoding set(y_train) . Feature Scaling : | . We saw previously that out numerical inputs have different scales like the weeks_worked_in_year and capital_gains. We will be using StandardScaler since standardization is much less affected by outliers. . from sklearn.preprocessing import StandardScaler scaler = StandardScaler() for c in cont: X_train[c] = scaler.fit_transform(np.array(X_train[c]).reshape(-1,1)) # fir on the train set X_val[c] = scaler.transform(np.array(X_val[c]).reshape(-1,1)) test_x[c] = scaler.transform(np.array(test_x[c]).reshape(-1,1)) #checking the standardization X_train[cont].head(3) . So far, we have handled the categorical columns and the numerical columns : . # Checking the training set with pd.option_context(&#39;display.max_rows&#39;, None, &#39;display.max_columns&#39;, None): display(X_train.head(3)) X_train.shape, X_val.shape, test_x.shape . Data modeling . Selecting a Performance Measure : | . Accuracy is the simplest way to measure the effectiveness of a classification task, and it’s the percentage of correct predictions over all predictions. In other words, in a binary classification task, you can calculate this by adding the number of True Positives (TPs) and True Negatives (TNs) and dividing them by a tally of all predictions made. As with regression metrics, you can measure accuracy for both train and test to gauge overfitting. . But, we can get an accuracy of 94%, which sounds pretty good, but it turns out we are always predicting -50k! In other words, even if we get high accuracy, it is meaningless unless we are predicting accurately for the least represented class, +50k. . For this reasing, we will be using F1-score. The F1-score is also called the harmonic average of precision and recall because it’s calculated like this: 2TP / 2TP + FP + FN. Since it includes both precision and recall metrics, which pertain to the proportion of true positives, it’s a good metric choice to use when the dataset is imbalanced, and we don’t prefer either precision or recall. . Base model : | . Let’s start with Decision tree ensembles. . A decision tree asks a series of binary (that is, yes or no) questions about the data. After each question the data at that part of the tree is split between a “yes” and a “no” branch. After one or more questions, either a prediction can be made on the basis of all previous answers or another question is required. . We illustarte a tree classification using 4 leaf nodes. . from sklearn.tree import DecisionTreeClassifier, plot_tree m = DecisionTreeClassifier(max_leaf_nodes=4, random_state=14) # to plot the tree classification m.fit(X_train, y_train) # to get the class output m.classes_ !pip install pydotplus !pip install graphviz . The top node represents the initial model before any splits have been done, when all the data is in the initial income levels. This is the simplest possible model. It is the result of asking zero questions and will always predict the more represented class which is -50k. We use the Gini method to create split points. The strategy is to select each pair of adjacent values as a possible split-point and the point with smaller gini index chosen as the splitting point. In our case, the capital gains at 1.47 was choosen first. . Moving down and to the left, this node shows us that there were 130,999 records for income level of -50k where capital gains was less than 1.47. The class predicted is -50k in this case. Moving down and to the right from the initial model takes us to the records where capital gains was greater than 1.47. The class predicted is +50k in this case where 1370 records have an income of +50k and capital gains &gt;0.4 . The bottom row contains our leaf nodes: the nodes with no answers coming out of them, because there are no more questions to be answered. . Returning back to the top node after the first decision point, we can see that a second binary decision split has been made, based on asking whether weeks_worked_per_year is less than or equal to 0.9. For the group where this is true, the class predicted is -50k with a gini of 0.019 and there are 85,411 records. For the records where this decision is false, the class predicted is -50k with a gini of 0.019, and there are 52,361 records. So again, we can see that the decision tree algorithm has successfully split out more records into two more groups which differ in gini value significantly. . Now, let’s run our base model : . m = DecisionTreeClassifier(random_state=14) m.fit(X_train, y_train) . We evaluate the model on our validation set using accuracy, recall and f1 score : . from sklearn.metrics import accuracy_score, f1_score, recall_score # on the train set accuracy_score(y_train,m.predict(X_train)) , recall_score(y_train,m.predict(X_train)), f1_score(y_train,m.predict(X_train), average=&#39;binary&#39;, pos_label=1) # on the valid set accuracy_score(y_val,m.predict(X_val)) , recall_score(y_val,m.predict(X_val)), f1_score(y_val,m.predict(X_val), average=&#39;binary&#39;, pos_label=1) . It’s seems that we are doing badly on the validation set. Let’s see houw many leaf nodes we got : . m.get_n_leaves(), len(X_train) . Sklearn’s default settings allow it to continue splitting nodes until there is only one item in each leaf node. Let’s change the stopping rule to tell sklearn to ensure every leaf node contains at least 25 records: . m = DecisionTreeClassifier(min_samples_leaf=25,random_state=14) m.fit(X_train, y_train) # on the train set accuracy_score(y_train,m.predict(X_train)) , recall_score(y_train,m.predict(X_train)), f1_score(y_train,m.predict(X_train), average=&#39;binary&#39;, pos_label=1) # on the valid set accuracy_score(y_val,m.predict(X_val)) , recall_score(y_val,m.predict(X_val)), f1_score(y_val,m.predict(X_val), average=&#39;binary&#39;, pos_label=1) . That looks much better. Let’s check the number of leaves again: . m.get_n_leaves(), len(X_train) . We got less leaf nodes than before. So, the more we increase the number of leaf nodes, the more is the possibility of overfitting. . Building a decision tree is a good way to create a model of our data. It is very flexible, since it can clearly handle nonlinear relationships and interactions between variables. But we can see there is a fundamental compromise between how well it generalizes (which we can achieve by creating small trees) and how accurate it is on the training set (which we can achieve by using large trees). . So how do we get the best of both worlds? . Ensembling : | . An an example of an Ensemble method is Random Forest : we can train a group of Decision Tree classifiers, each on a different random subset of the training set. The process of subseting the data is called bagging done with max_samples hyperparameter ( we set it at 100.00 samples) and the ramdom selection process this called bootsraping done by setting bootstrap = True. . With bagging, some instances may be sampled several times for any given predictor, while others may not be sampled at all. The remaining sampled are called out-of-bag (oob) instances used as validation set in the training process and done by setting oob_score=True. . We train a Random Forest classifier with 50 trees (each limited to minimum 5 samples per leaf). and instead of searching for the very best feature when splitting a node, we searches for the best feature among a random subset of 50% of our initial features. . from sklearn.ensemble import RandomForestClassifier rf = RandomForestClassifier(n_estimators = 50, max_samples=100_000, max_features=0.5, min_samples_leaf= 5, bootstrap= True,oob_score = True,random_state=14) %%time rf.fit(X_train,y_train) # on the train set accuracy_score(y_train,rf.predict(X_train)) , recall_score(y_train,rf.predict(X_train)), f1_score(y_train,rf.predict(X_train), average=&#39;binary&#39;, pos_label=1) # on the valid set accuracy_score(y_val,rf.predict(X_val)) , recall_score(y_val,rf.predict(X_val)), f1_score(y_val,rf.predict(X_val), average=&#39;binary&#39;, pos_label=1) . Looking at what happens to the oob error rate as we add more and more trees, we you can see that the improvement levels off quite a bit after around 40 trees: . scores =[] for k in range(1, 50): rfc = RandomForestClassifier(n_estimators = k, max_samples=100_000, max_features=0.5, min_samples_leaf= 5, bootstrap= True,oob_score = True,random_state=14) rfc.fit(X_train, y_train) #y_pred = rfc.predict(X_val) #scores.append(accuracy_score(y_test, y_pred)) oob_score_ oob_error = 1 - rfc.oob_score_ scores.append(oob_error) import matplotlib.pyplot as plt %matplotlib inline # plot the relationship between K and testing accuracy # plt.plot(x_axis, y_axis) plt.plot(range(1, 50), scores) plt.xlabel(&#39;Value of n_estimators for Random Forest Classifier&#39;) #plt.ylabel(&#39;Testing Accuracy&#39;) plt.ylabel(&#39;OOB error rate&#39;) . Let’s try to improve our model : . We may ask which columns are the strongest predictors, which can we ignore? . It’s not normally enough just to know that a model can make accurate predictions—we also want to know how it’s making predictions. Feature importance gives us insight into this. We can get these directly from sklearn’s random forest by looking in the feature_importances_ attribute. Here’s a simple function we can use to pop them into a DataFrame and sort them: . def rf_feat_importance(m, df): return pd.DataFrame({&#39;cols&#39;:df.columns, &#39;imp&#39;:m.feature_importances_} ).sort_values(&#39;imp&#39;, ascending=False) fi = rf_feat_importance(rf, X_train) fi[:14] . The feature importances for our model show that the first few most important columns have much higher importance scores than the rest, with (not surprisingly) capital_gains and dividends_from_stocks being at the top of the list. . A plot of the feature importances shows the relative importances more clearly: . def plot_fi(fi): return fi.plot(&#39;cols&#39;, &#39;imp&#39;, &#39;barh&#39;, figsize=(12,7), legend=False) plot_fi(fi[:30]); . The way these importances are calculated is quite simple yet elegant. The feature importance algorithm loops through each tree, and then recursively explores each branch. At each branch, it looks to see what feature was used for that split, and how much the model improves as a result of that split. The improvement (weighted by the number of rows in that group) is added to the importance score for that feature. This is summed across all branches of all trees, and finally the scores are normalized such that they add to 1. . It seems likely that we could use just a subset of the columns by removing the variables of low importance and still get good results. Let’s try just keeping those with a feature importance greater than 0.005: . to_keep = fi[fi.imp&gt;0.005].cols len(to_keep) . We can retrain our model using just this subset of the columns: . X_train_imp = X_train[to_keep] X_val_imp = X_val[to_keep] m = RandomForestClassifier(n_estimators = 50, max_samples=100_000, max_features=0.5, min_samples_leaf= 5, bootstrap= True,oob_score = True,random_state=14) m.fit(X_train_imp,y_train) # on the train set accuracy_score(y_train,m.predict(X_train_imp)) , recall_score(y_train,m.predict(X_train_imp)), f1_score(y_train,m.predict(X_train_imp), average=&#39;binary&#39;, pos_label=1) # on the valid set accuracy_score(y_val,m.predict(X_val_imp)) , recall_score(y_val,m.predict(X_val_imp)), f1_score(y_val,m.predict(X_val_imp), average=&#39;binary&#39;, pos_label=1) . Our accuracy is about the same, but we have far fewer columns to study: . len(X_train.columns), len(X_train_imp.columns) . We’ve found that generally the first step to improving a model is simplifying it—48 columns was too many for us to study them all in depth! Furthermore, in practice often a simpler, more interpretable model is easier to roll out and maintain. . This also makes our feature importance plot easier to interpret. Let’s look at it again: . plot_fi(rf_feat_importance(m, X_train_imp)); . Let’s see if we have redundent feature in our model by determining their similarities : . Determining Similarity: The most similar pairs are found by calculating the rank correlation, which means that all the values are replaced with their rank (i.e., first, second, third, etc. within the column), and then the correlation is calculated. . import scipy from scipy.cluster import hierarchy as hc def cluster_columns(df, figsize=(10,6), font_size=12): corr = np.round(scipy.stats.spearmanr(df).correlation, 4) corr_condensed = hc.distance.squareform(1-corr) z = hc.linkage(corr_condensed, method=&#39;average&#39;) fig = plt.figure(figsize=figsize) hc.dendrogram(z, labels=df.columns, orientation=&#39;left&#39;, leaf_font_size=font_size) plt.show() cluster_columns(X_train_imp) . Looking good! This is really not much worse than the model with all the fields. Let’s create DataFrames without these columns, and save them: . X_train_final = X_train_imp # train X_val_final = X_val_imp # valid test_x_final = test_x[to_keep] # test set X_train_final.shape , X_val_final.shape, test_x_final.shape . Model Assesment : . We have seen the DecisionTreeClassifier as our basemodel, then we tried RandomForestClassifier and finaly we tried to optimize so we can have less features for better interpretation. . Here is the model metrics on our validation set : . models_metrics = {&#39;DTC&#39;: [0.95, 0.42, 0.51], &#39;RF&#39;: [0.96, 0.42, 0.53], &#39;RF_less_feat&#39;: [0.95, 0.41, 0.53] } df = pd.DataFrame(data = models_metrics) df.rename(index={0:&#39;Accuracy&#39;,1:&#39;Recall&#39;, 2: &#39;F1 score&#39;}, inplace=True) ax = df.plot(kind=&#39;bar&#39;, figsize = (10,5), color = [&#39;gold&#39;, &#39;lightgreen&#39;,&#39;lightcoral&#39;], rot = 0, title =&#39;Models performance&#39;, edgecolor = &#39;grey&#39;, alpha = 0.5) for p in ax.patches: ax.annotate(str(p.get_height()), (p.get_x() * 1.01, p.get_height() * 1.0005)) plt.show() . Based on F1 score, we select the RandomForestClassifier with 25 features as our best model. . Let’s see the experiment results* of this model : . The precision_recall_curve and roc_curve are useful tools to visualize the sensitivity-specificty tradeoff in the classifier. They help inform a data scientist where to set the decision threshold of the model to maximize either sensitivity or specificity. This is called the operating point of the model. . from sklearn.metrics import roc_curve, precision_recall_curve, auc, make_scorer, recall_score, accuracy_score, precision_score, confusion_matrix # We create an array of the class probabilites called y_scores y_scores = m.predict_proba(X_val_imp)[:, 1] # we enerate the precision-recall curve for the classifier: p, r, thresholds = precision_recall_curve(y_val, y_scores) # We calculate the F1 scores f1_scores = 2*r*p/(r+p) . Let’s plot the decision chart of our model : . def plot_precision_recall_vs_threshold(precisions, recalls, thresholds): plt.figure(figsize=(8, 8)) plt.title(&quot;Precision, Recall Scores and F1 scores as a function of the decision threshold&quot;) plt.plot(thresholds, precisions[:-1], &quot;b--&quot;, label=&quot;Precision&quot;) plt.plot(thresholds, recalls[:-1], &quot;g-&quot;, label=&quot;Recall&quot;) plt.plot(thresholds, f1_scores[:-1], &quot;r-&quot;, label=&quot;F1-score&quot;) plt.ylabel(&quot;Score&quot;) plt.xlabel(&quot;Decision Threshold&quot;) plt.legend(loc=&#39;best&#39;) plot_precision_recall_vs_threshold(p, r, thresholds) . We can see that the the optimal threshold to achieve the highest F1 score is set at 0.30 with 59% F1-score. . print(&#39;Best threshold: &#39;, thresholds[np.argmax(f1_scores)]) print(&#39;Best F1-Score: &#39;, np.max(f1_scores)) . Let’s creat an animated confusion matrix where the users get to choose the threesholds and we dislpay the confusion matrix and recall vs precision curve : . pip install ipywidgets import ipywidgets as widgets import itertools def plot_confusion_matrix(cm, classes, normalize = False, title = &#39;Confusion matrix&quot;&#39;, cmap = plt.cm.Blues) : plt.imshow(cm, interpolation = &#39;nearest&#39;, cmap = cmap) plt.title(title) plt.colorbar() tick_marks = np.arange(len(classes)) plt.xticks(tick_marks, classes, rotation = 0) plt.yticks(tick_marks, classes) thresh = cm.max() / 2. for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])) : plt.text(j, i, cm[i, j], horizontalalignment = &#39;center&#39;, color = &#39;white&#39; if cm[i, j] &gt; thresh else &#39;black&#39;) plt.tight_layout() plt.ylabel(&#39;True label&#39;) plt.xlabel(&#39;Predicted label&#39;) def adjusted_classes(y_scores, t): &quot;&quot;&quot; This function adjusts class predictions based on the prediction threshold (t). Will only work for binary classification problems. &quot;&quot;&quot; return [1 if y &gt;= t else 0 for y in y_scores] def precision_recall_threshold(p, r, thresholds, t=0.5): &quot;&quot;&quot; plots the precision recall curve and shows the current value for each by identifying the classifier&#39;s threshold (t). &quot;&quot;&quot; # generate new class predictions based on the adjusted_classes # function above and view the resulting confusion matrix. y_pred_adj = adjusted_classes(y_scores, t) cm = confusion_matrix(y_val, y_pred_adj) class_names = [0,1] plt.figure() plot_confusion_matrix(cm, classes=class_names, title=&#39;RF Confusion matrix&#39;) plt.figure(figsize=(8,8)) plt.title(&quot;Precision and Recall curve ^ = current threshold&quot;) plt.step(r, p, color=&#39;b&#39;, alpha=0.2, where=&#39;post&#39;) plt.fill_between(r, p, step=&#39;post&#39;, alpha=0.2, color=&#39;b&#39;) plt.xlabel(&#39;Recall&#39;); plt.ylabel(&#39;Precision&#39;); # plot the current threshold on the line close_default_clf = np.argmin(np.abs(thresholds - t)) plt.plot(r[close_default_clf], p[close_default_clf], &#39;^&#39;, c=&#39;k&#39;, markersize=15) slider = widgets.IntSlider( min=0, max=10, step=1, description=&#39;Slider:&#39;, value=3 # The best threshhold for our model ) display(slider) print(f&#39;For this threshold : {slider.value/10}, the confusion matrix is as follow :&#39;) precision_recall_threshold(p, r, thresholds, slider.value/10) def plot_roc_curve(fpr, tpr, label=None): plt.figure(figsize=(8,8)) plt.title(&#39;ROC Curve&#39;) plt.plot(fpr, tpr, linewidth=2, label=label) plt.plot([0, 1], [0, 1], &#39;k--&#39;) plt.axis([-0.005, 1, 0, 1.005]) plt.xticks(np.arange(0,1, 0.05), rotation=90) plt.xlabel(&quot;False Positive Rate&quot;) plt.ylabel(&quot;True Positive Rate (Recall)&quot;) plt.legend(loc=&#39;best&#39;) fpr, tpr, auc_thresholds = roc_curve(y_val, y_scores) print(f&#39;AUC : {auc(fpr, tpr)}&#39;) # AUC of ROC plot_roc_curve(fpr, tpr, &#39;recall_optimized&#39;) . Now, let’s test this model on our test set : . accuracy_score(test_y,m.predict(test_x_final)) , recall_score(test_y,m.predict(test_x_final)) , f1_score(test_y,m.predict(test_x_final), average=&#39;binary&#39;, pos_label=1) . Results : Partial dependency and SHAP values . Let’s look at partial dependence plots. . Partial dependence plots try to answer the question: if a row varied on nothing other than the feature in question, how would it impact the dependent variable? . For instance, how does capital_gains and dividends_from_stocks impact probability of belonging to the +50k income levl, all other things being equal? . from sklearn.inspection import plot_partial_dependence fig,ax = plt.subplots(figsize=(20, 8)) plot_partial_dependence(m, X_val_final, [&#39;capital_gains&#39;,&#39;dividends_from_stocks&#39;], percentiles=(0,1), grid_resolution=15, ax=ax); . Looking first at the dividends_from_stocks plot, we can see a nearly linear relationship between capital dividends_from_stocks and the probabillity of income level. Same for capital_gains at 5 standad deviation from the mean after reaching a steady state above that. . !pip install shap import shap row_to_show = 20 data_for_prediction = test_x_final.iloc[row_to_show] # use 1 row of data here. Could use multiple rows if desired # Create object that can calculate shap values explainer = shap.TreeExplainer(m) # Calculate Shap values shap_values = explainer.shap_values(data_for_prediction) . The above explanation shows features each contributing to push the model output from the base value (the average model output over the training dataset we passed) to the model output. Features pushing the prediction higher are shown in red, those pushing the prediction lower are in blue . The base_value here is 0.062 while our predicted value is 0.0. | sex = 1 has the biggest impact on increasing the prediction, while | Weeks_worked_im_year (below the average) and Age (below the average) feature has the biggest effect in decreasing the prediction. | . explainer = shap.TreeExplainer(m) # calculate shap values. This is what we will plot. # Calculate shap_values for all of val_X rather than a single row, to have more data for plot. shap_values = explainer.shap_values(test_x_final.iloc[:1000,]) # Make plot. Index of [1] is explained in text below. shap.summary_plot(shap_values[1],test_x_final.iloc[:1000,]) . For every dot: . Vertical location shows what feature it is depicting | Color shows whether that feature was high or low for that row of the dataset | Horizontal location shows whether the effect of that value caused a higher or lower prediction. | . For the age variable, the point in the upper left was depicts a person whose age level is less thereby reducing the prediction of income level +50k class by 0.2. . Conclusion : . In this work, we presented some techniques for dealing with a machine learning project : . We used Decision Tree ensembles : Random Forest are the easiest to train, because they are extremely resilient to hyperparameter choices and require very little preprocessing. They are very fast to train, and should not overfit if we have enough trees. . | we used the model for feature selection and partial dependence analysis and Shap values, to get a better understanding of our data. . | . For futur improvements : . We can try Gradient Boosting machines as in theory are just as fast to train as random forests, but in practice we will have to try lots of different hyperparameters. They can overfit, but they are often a little more accurate than random forests. . | We can try OneHotEncoder with PCA to deal with the multiple modalities on our categorical variables. . | We can creat new features to challenge the model performance. . | . .",
            "url": "https://younesszaim.github.io/myportfolio/2021/11/20/uscensus.html",
            "relUrl": "/2021/11/20/uscensus.html",
            "date": " • Nov 20, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Sickit Learn For Machine Learning",
            "content": "Sickit-Learn for Machine Learning . In this notebook we will work through an example project end to end using Sickit Learn library. . About . In this chapter we’ll use the California Housing Prices dataset from the StatLib repository This dataset is based on data from the 1990 California census. . This data includes metrics such as the population, median income, and median housing price for each block group in California. Block groups are the smallest geographical unit for which the US Census Bureau publishes sample data (a block group typical). . Your model should learn from this data and be able to predict the median housing price in any district, given all the other metrics. . Goal : Your boss answers that your model’s output (a prediction of a district’s median housing price) will be fed to another Machine Learning system , along with many other signals. This downstream system will determine whether it is worth investing in a given area or not. Getting this right is critical, as it directly affects revenue. . First, you need to frame the problem: is it supervised, unsupervised, or Reinforcement Learning? Is it a classification task, a regression task, or something else? Should you use batch learning or online learning techniques? . Let’s see: it is clearly a typical supervised learning task, since you are given labeled training examples. . It is also a typical regression task, since you are asked to predict a value. More specifically, this is a multiple regression problem, since the system will use multiple features to make a prediction. . It is also a univariate regression problem, since we are only trying to predict a single value for each district. If we were trying to predict multiple values per district, it would be a multivariate regression problem. . Finally, there is no continuous flow of data coming into the system, there is no particular need to adjust to changing data rapidly, and the data is small enough to fit in memory, so plain batch learning should do just fine. . Select a Performance Measure . Performance Measures for our univariate regression problem . Typical performance measure for regression problems : . Root Mean Square Error (RMSE): it gives an idea of how much error the system typically makes in its predictions, with a higher weight for large errors : Since the errors are squared before they are averaged, the RMSE gives a relatively high weight to large errors. This means the RMSE should be more useful when large errors are particularly undesirable. | . RMSE is sensitive to outliers : If we make a single very bad prediction, taking the square will make the error even worse and it may skew the metric towards overestimating the model’s badness. Actually, it’s hard to realize if our model is good or not by looking at the absolute values of MSE or MSE : We would probably want to measure how much our model is better than the constant baseline : A model should at least perform better than the RMSE score constant baseline. . RMSE has the benefit of penalizing large errors more so can be more appropriate in some cases, for example, if being off by 10 is more than twice as bad as being off by 5. But if being off by 10 is just twice as bad as being off by 5, then MAE is more appropriate. . | . Root Mean Square Log Error (RMSLE): It is an extension on root Mean Squared Error (RMSE) that is mainly used when predictions have large deviations | . RMSLE is preferable when : . targets having exponential growth, such as population counts, average sales of a commodity over a span of years etc . | we care about percentage errors rather than the absolute value of errors : The reason we use log is because generally, you care not so much about missing by €10 but missing by 10%. So if it was €1000,000 item and you are €100,000 off or if it was a 10,000 item and you are €1,000 off — we would consider those equivalent scale issues. . | There is a wide range in the target variables and we don’t want to penalize big differences when both the predicted and the actual are big numbers. . | We want to penalize under estimates more than over estimates. . | Let’s imagine two cases of predictions, . | . Case-1: our model makes a prediction of 30 when the actual number is 40 Case-2: our model makes a prediction of 300 when the actual number is 400 . With RMSE the second result is scored as 10 times more than the first result Conversely, with RMSLogE two results are scored the same. RMSLogE takes into account just the ratio of change Lets have a look at the below example . Case-3 : Prediction = 600, Actual = 1000 (the absolute difference is 400) . RMSE = 400, RMSLogE = 0.5108 . Case-4 : Prediction = 1400, Actual = 1000 (the absolute difference is 400) . RMSE = 400, RMSLogE = 0.3365 . When the differences are the same between actual and predicted in both cases. RMSE treated them equally, however RMSLogE penalized the under estimate more than over estimate (under estimated prediction score is higher than over estimated prediction score). Often, penalizing the under estimate more than over estimate is important for prediction of sales and inventory demands. . | . | . Mean Absolute Error (MAE): also called the average absolute deviation : MAE measures the average magnitude of the errors in a set of predictions, without considering their direction. . | R-Squared (R2) : proportional improvement in prediction of the regression model, compared to the mean model (model predicting all given samples as mean value) : - If we were exactly as effective as just predicting the mean, SSres/SStot = 1 and R² = 0 - If we were perfect (i.e. yi = fi for all cases), SSres/SStot = 0 and R² = 1 However, it does not take into consideration of overfitting problem. . Interpreted as the proportion of total variance that is explained by the model. | R² is the ratio between how good your model is (RMSE)vs. how good is the naïve mean model (RMSE). | . | . RMSE vs RMSLE vs MAE . See links below : . RMSE vs MAE : https://medium.com/human-in-a-machine-world/mae-and-rmse-which-metric-is-better-e60ac3bde13d | RMSLE metric and defining baseline : https://www.kaggle.com/carlolepelaars/understanding-the-metric-rmsle/notebook | Model fot metrics : https://www.kaggle.com/residentmario/model-fit-metrics | . Scikit-learn implementation . ### R-square : # sklean from sklearn.metrics import r2_score # hand implemetation import numpy as np def r2_score(y, y_pred): rss_adj = np.sum((y - y_pred)**2) n = len(y) y_bar_adj = (1 / n) * np.sum(y) ess_adj = np.sum((y - y_bar_adj)**2) return 1 - rss_adj / ess_adj r2_score(y, y_pred) ### Root Mean Squared Error (RMSE) from sklearn.metrics import mean_squared_error mean_squared_error(y,y_pred, squared = False) # hand implemetation import math def rmse(y, y_pred): return math.sqrt( ((y-y_pred)**2).mean() ) root_mean_squared_error(y, y_pred) ### Root Mean log Squared Error (RMLSE) from sklearn.metrics import mean_squared_log_error mean_squared_error(y,y_pred, squared = False) # or import numpy as np y = np.log(df.y) RMSLE = rmse(y,y_pred) ### Mean Absolute Error (MAE) from sklearn.metrics import mean_absolute_error # hand implemetation import numpy as np def mae(y,y_pred): return (np.abs(y-y_pred)).mean() NameError Traceback (most recent call last) &lt;ipython-input-1061-7e2a74a2dd02&gt; in &lt;module&gt; 14 return 1 - rss_adj / ess_adj 15 &gt; 16 r2_score(y, y_pred) 17 18 NameError: name &#39;y_pred&#39; is not defined . Download the Data . PATH = &#39;/Users/rmbp/handson-ml2/datasets/&#39; !ls {PATH} housing inception jsb_chorales lifesat titanic import pandas as pd housing = pd.read_csv(f&#39;{PATH}/housing/housing.csv&#39;) housing.head() longitude latitude housing_median_age total_rooms total_bedrooms 0 -122.23 37.88 41.0 880.0 129.0 1 -122.22 37.86 21.0 7099.0 1106.0 2 -122.24 37.85 52.0 1467.0 190.0 3 -122.25 37.85 52.0 1274.0 235.0 4 -122.25 37.85 52.0 1627.0 280.0 population households median_income median_house_value ocean_proximity 0 322.0 126.0 8.3252 452600.0 NEAR BAY 1 2401.0 1138.0 8.3014 358500.0 NEAR BAY 2 496.0 177.0 7.2574 352100.0 NEAR BAY 3 558.0 219.0 5.6431 341300.0 NEAR BAY 4 565.0 259.0 3.8462 342200.0 NEAR BAY . Automating the process of fetching and loading the data . import os import tarfile import urllib DOWNLOAD_ROOT = &quot;https://raw.githubusercontent.com/ageron/handson-ml2/master/&quot; HOUSING_PATH = os.path.join(&quot;/Users/rmbp/Desktop&quot;, &quot;housing&quot;) HOUSING_URL = DOWNLOAD_ROOT + &quot;datasets/housing/housing.tgz&quot; def fetch_housing_data(housing_url=HOUSING_URL, housing_path=HOUSING_PATH): os.makedirs(housing_path, exist_ok=True) tgz_path = os.path.join(housing_path, &quot;housing.tgz&quot;) urllib.request.urlretrieve(housing_url, tgz_path) housing_tgz = tarfile.open(tgz_path) housing_tgz.extractall(path=HOUSING_PATH) housing_tgz.close() fetch_housing_data(HOUSING_URL,HOUSING_PATH) import pandas as pd def load_housing_data(housing_path=HOUSING_PATH): csv_path = os.path.join(housing_path, &quot;housing.csv&quot;) return pd.read_csv(csv_path) load_housing_data().head() longitude latitude housing_median_age total_rooms total_bedrooms 0 -122.23 37.88 41.0 880.0 129.0 1 -122.22 37.86 21.0 7099.0 1106.0 2 -122.24 37.85 52.0 1467.0 190.0 3 -122.25 37.85 52.0 1274.0 235.0 4 -122.25 37.85 52.0 1627.0 280.0 population households median_income median_house_value ocean_proximity 0 322.0 126.0 8.3252 452600.0 NEAR BAY 1 2401.0 1138.0 8.3014 358500.0 NEAR BAY 2 496.0 177.0 7.2574 352100.0 NEAR BAY 3 558.0 219.0 5.6431 341300.0 NEAR BAY 4 565.0 259.0 3.8462 342200.0 NEAR BAY . Take a Quick Look at the Data Structure . housing.head() longitude latitude housing_median_age total_rooms total_bedrooms 0 -122.23 37.88 41.0 880.0 129.0 1 -122.22 37.86 21.0 7099.0 1106.0 2 -122.24 37.85 52.0 1467.0 190.0 3 -122.25 37.85 52.0 1274.0 235.0 4 -122.25 37.85 52.0 1627.0 280.0 population households median_income median_house_value ocean_proximity 0 322.0 126.0 8.3252 452600.0 NEAR BAY 1 2401.0 1138.0 8.3014 358500.0 NEAR BAY 2 496.0 177.0 7.2574 352100.0 NEAR BAY 3 558.0 219.0 5.6431 341300.0 NEAR BAY 4 565.0 259.0 3.8462 342200.0 NEAR BAY housing.info() &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 20640 entries, 0 to 20639 Data columns (total 10 columns): # Column Non-Null Count Dtype -- -- 0 longitude 20640 non-null float64 1 latitude 20640 non-null float64 2 housing_median_age 20640 non-null float64 3 total_rooms 20640 non-null float64 4 total_bedrooms 20433 non-null float64 5 population 20640 non-null float64 6 households 20640 non-null float64 7 median_income 20640 non-null float64 8 median_house_value 20640 non-null float64 9 ocean_proximity 20640 non-null object dtypes: float64(9), object(1) memory usage: 1.6+ MB . There are 20,640 instances in the dataset. Notice that the total_bedrooms attribute has only 20,433 nonnull values, meaning that 207 districts are missing this feature. All attributes are numerical, except the ocean_proximity field. Its type is object. Since we loaded this data from a CSV file, it must be a text attribute. : the values in the ocean_proximity column were repetitive, which means that it is probably a categorical attribute. . housing[&#39;ocean_proximity&#39;].value_counts() &lt;1H OCEAN 9136 INLAND 6551 NEAR OCEAN 2658 NEAR BAY 2290 ISLAND 5 Name: ocean_proximity, dtype: int64 housing.describe(include=&#39;all&#39;).T count unique top freq mean longitude 20640.0 NaN NaN NaN -119.569704 latitude 20640.0 NaN NaN NaN 35.631861 housing_median_age 20640.0 NaN NaN NaN 28.639486 total_rooms 20640.0 NaN NaN NaN 2635.763081 total_bedrooms 20433.0 NaN NaN NaN 537.870553 population 20640.0 NaN NaN NaN 1425.476744 households 20640.0 NaN NaN NaN 499.53968 median_income 20640.0 NaN NaN NaN 3.870671 median_house_value 20640.0 NaN NaN NaN 206855.816909 ocean_proximity 20640 5 &lt;1H OCEAN 9136 NaN std min 25% 50% 75% longitude 2.003532 -124.35 -121.8 -118.49 -118.01 latitude 2.135952 32.54 33.93 34.26 37.71 housing_median_age 12.585558 1.0 18.0 29.0 37.0 total_rooms 2181.615252 2.0 1447.75 2127.0 3148.0 total_bedrooms 421.38507 1.0 296.0 435.0 647.0 population 1132.462122 3.0 787.0 1166.0 1725.0 households 382.329753 1.0 280.0 409.0 605.0 median_income 1.899822 0.4999 2.5634 3.5348 4.74325 median_house_value 115395.615874 14999.0 119600.0 179700.0 264725.0 ocean_proximity NaN NaN NaN NaN NaN max longitude -114.31 latitude 41.95 housing_median_age 52.0 total_rooms 39320.0 total_bedrooms 6445.0 population 35682.0 households 6082.0 median_income 15.0001 median_house_value 500001.0 ocean_proximity NaN . hist() method on the whole dataset will plot a histogram for each numerical attribute . A histogram is used for continuous data, where the bins represent ranges of data, counts the data points in each bin, and shows the bins on the x-axis and the counts on the y-axis. : https://towardsdatascience.com/histograms-and-density-plots-in-python-f6bda88f5ac0 | A bar chart is a plot of categorical variables. | . #This tells Jupyter to set up Matplotlib so it uses Jupyter’s own backend. %matplotlib inline import matplotlib.pyplot as plt housing.hist(bins=60, figsize=(15,10)) plt.show() . . # plot &#39;median_house_value&#39; housing[&#39;median_house_value&#39;].plot(kind=&#39;hist&#39;, bins= 60) &lt;AxesSubplot:ylabel=&#39;Frequency&#39;&gt; . . housing[&#39;ocean_proximity&#39;].value_counts().plot(kind= &#39;barh&#39;) &lt;AxesSubplot:&gt; . . pd.DataFrame(housing[&#39;median_income&#39;].describe()).T count mean std min 25% 50% 75% median_income 20640.0 3.870671 1.899822 0.4999 2.5634 3.5348 4.74325 max median_income 15.0001 n, bins, patches = plt.hist(housing.median_income, bins = int((15.000100 - 0.499900)/0.1),edgecolor = &#39;black&#39; ,color = &#39;blue&#39;) # bins = int((15.000100 - 0.499900)/0.1) : We choose the number of bins with an interval lenght of 100€ . . pd.DataFrame(housing[&#39;housing_median_age&#39;].describe()).T count mean std min 25% 50% 75% max housing_median_age 20640.0 28.639486 12.585558 1.0 18.0 29.0 37.0 52.0 n, bins, patches = plt.hist(housing.housing_median_age, bins = int((52.000000 - 1.000000)/1) , color = &#39;blue&#39; , edgecolor = &#39;black&#39;) bins array([ 1., 2., 3., 4., 5., 6., 7., 8., 9., 10., 11., 12., 13., 14., 15., 16., 17., 18., 19., 20., 21., 22., 23., 24., 25., 26., 27., 28., 29., 30., 31., 32., 33., 34., 35., 36., 37., 38., 39., 40., 41., 42., 43., 44., 45., 46., 47., 48., 49., 50., 51., 52.]) . . # Target pd.DataFrame(housing[&#39;median_house_value&#39;].describe()).T count mean std min 25% median_house_value 20640.0 206855.816909 115395.615874 14999.0 119600.0 50% 75% max median_house_value 179700.0 264725.0 500001.0 n, bins, patches = plt.hist(housing.median_house_value , bins = int((500001.000000 - 14999.000000)/10000) , color = &#39;blue&#39; ,edgecolor = &#39;black&#39;) bins array([ 14999. , 25103.20833333, 35207.41666667, 45311.625 , 55415.83333333, 65520.04166667, 75624.25 , 85728.45833333, 95832.66666667, 105936.875 , 116041.08333333, 126145.29166667, 136249.5 , 146353.70833333, 156457.91666667, 166562.125 , 176666.33333333, 186770.54166667, 196874.75 , 206978.95833333, 217083.16666667, 227187.375 , 237291.58333333, 247395.79166667, 257500. , 267604.20833333, 277708.41666667, 287812.625 , 297916.83333333, 308021.04166667, 318125.25 , 328229.45833333, 338333.66666667, 348437.875 , 358542.08333333, 368646.29166667, 378750.5 , 388854.70833333, 398958.91666667, 409063.125 , 419167.33333333, 429271.54166667, 439375.75 , 449479.95833333, 459584.16666667, 469688.375 , 479792.58333333, 489896.79166667, 500001. ]) . . From the figure below, we can see how the data was computed : . We can see that median_income was scaled and capped at 15 (actually, 15.0001) for higher median incomes, and at 0.5 (actually, 0.4999) for lower median incomes. The numbers represent roughly tens of thousands of dollars. | The housing median age and the median house value were also capped. The latter may be a serious problem since it is our target attribute. In this caseour Machine Learning algorithms may learn that prices never go beyond that limit (€500,000). We need to check with our team to see if this is a problem or not. If the team needs precise predictions even beyond €500,000, then you have two options: Collect proper labels for the districts whose labels were capped. | Remove those districts from the training set (and also from the test set, since your system should not be evaluated poorly if it predicts values beyond €500,000). | . | . We can also see that : . These attributes have very different scales. | Many histograms are tail-heavy : they extend much farther to the right of the median than to the left. | . Create a Test Set . We ahve only taken a quick glance at the data : numeric / categorical features, missing values, scale of attributes, distribution, how values are computed, distribution of the target variable. It’s enough. Why ? . if you look at the test set, you may stumble upon some seemingly interesting pattern in the test data that leads you to select a particular kind of Machine Learning model. When you estimate the generalization error using the test set, your estimate will be too optimistic, and you will launch a system that will not perform as well as expected. This is called data snooping bias. | . Train and test set stability . Creating a test set is theoretically simple: pick some instances randomly, typically 20% of the dataset . import numpy as np def split_train_test(data, test_ratio): shuffled_indices = np.random.permutation(len(data)) test_set_size = int(len(data) * test_ratio) test_indices = shuffled_indices[:test_set_size] train_indices = shuffled_indices[test_set_size:] return data.iloc[train_indices], data.iloc[test_indices] train_set, test_set = split_train_test(housing,0.2) print(len(train_set)) print(len(test_set)) 16512 4128 . Well, this works, but it is not perfect: if you run the program again, it will generate a different test set! . Over time, you (or your Machine Learning algorithms) will get to see the whole dataset, which is what you want to avoid. | . # first run test set split_train_test(housing,0.2)[1].head(5) longitude latitude housing_median_age total_rooms total_bedrooms 12003 -117.57 33.90 7.0 3797.0 850.0 13304 -117.63 34.09 19.0 3490.0 816.0 19037 -121.99 38.36 35.0 2728.0 451.0 9871 -121.82 36.61 24.0 2437.0 438.0 16526 -121.20 37.80 37.0 311.0 61.0 population households median_income median_house_value 12003 2369.0 720.0 3.5525 137600.0 13304 2818.0 688.0 2.8977 126200.0 19037 1290.0 452.0 3.2768 117600.0 9871 1430.0 444.0 3.8015 169100.0 16526 171.0 54.0 4.0972 101800.0 ocean_proximity 12003 INLAND 13304 INLAND 19037 INLAND 9871 &lt;1H OCEAN 16526 INLAND # second run test set split_train_test(housing,0.2)[1].head(5) longitude latitude housing_median_age total_rooms total_bedrooms 5709 -118.23 34.21 32.0 1464.0 406.0 16381 -121.30 38.02 4.0 1515.0 384.0 16458 -121.30 38.13 26.0 2256.0 360.0 8613 -118.37 33.87 23.0 1829.0 331.0 2738 -115.56 32.78 35.0 1185.0 202.0 population households median_income median_house_value 5709 693.0 380.0 2.5463 200000.0 16381 491.0 348.0 2.8523 87500.0 16458 937.0 372.0 5.0528 153700.0 8613 891.0 356.0 6.5755 359900.0 2738 615.0 191.0 4.6154 86200.0 ocean_proximity 5709 &lt;1H OCEAN 16381 INLAND 16458 INLAND 8613 &lt;1H OCEAN 2738 INLAND . Solution : . One solution is to save the test set on the first run and then load it in subsequent runs. | Another option is to set the random number generator’s seed (e.g., with np.ran dom.seed(42))14 before calling np.random.permutation() so that it always generates the same shuffled indices : | . import numpy as np def split_train_test(data, test_ratio): np.random.seed(1997) shuffled_indices = np.random.permutation(len(data)) test_set_size = int(len(data) * test_ratio) test_indices = shuffled_indices[:test_set_size] train_indices = shuffled_indices[test_set_size:] return data.iloc[train_indices], data.iloc[test_indices] # first run test set split_train_test(housing,0.2)[1].head(5) longitude latitude housing_median_age total_rooms total_bedrooms 9009 -118.60 34.07 16.0 319.0 59.0 17779 -121.83 37.38 15.0 4430.0 992.0 20209 -119.21 34.28 27.0 2219.0 312.0 3170 -119.69 36.41 38.0 1016.0 202.0 2200 -119.85 36.83 15.0 2563.0 335.0 population households median_income median_house_value 9009 149.0 64.0 4.6250 433300.0 17779 3278.0 1018.0 4.5533 209900.0 20209 937.0 315.0 5.7601 281100.0 3170 540.0 187.0 2.2885 75000.0 2200 1080.0 356.0 6.7181 160300.0 ocean_proximity 9009 &lt;1H OCEAN 17779 &lt;1H OCEAN 20209 NEAR OCEAN 3170 INLAND 2200 INLAND # second run test set split_train_test(housing,0.2)[1].head(5) longitude latitude housing_median_age total_rooms total_bedrooms 9009 -118.60 34.07 16.0 319.0 59.0 17779 -121.83 37.38 15.0 4430.0 992.0 20209 -119.21 34.28 27.0 2219.0 312.0 3170 -119.69 36.41 38.0 1016.0 202.0 2200 -119.85 36.83 15.0 2563.0 335.0 population households median_income median_house_value 9009 149.0 64.0 4.6250 433300.0 17779 3278.0 1018.0 4.5533 209900.0 20209 937.0 315.0 5.7601 281100.0 3170 540.0 187.0 2.2885 75000.0 2200 1080.0 356.0 6.7181 160300.0 ocean_proximity 9009 &lt;1H OCEAN 17779 &lt;1H OCEAN 20209 NEAR OCEAN 3170 INLAND 2200 INLAND . But both these solutions will break the next time you fetch an updated dataset. . If the dataset is updated, we want to ensure that the test set will remain consistent across multiple runs, even if you refresh the dataset : The new test set will contain 20% of the new instances, but it will not contain any instance that was previously in the training set. . To have a stable train/test split even after updating the dataset, a common solution is to use each instance’s identifier to decide whether or not it should go in the test set (assuming instances have a unique and immutable identifier). For example, we could compute a hash of each instance’s identifier and put that instance in the test set if the hash is lower than or equal to 20% of the maximum hash value. . from zlib import crc32 def test_set_check(identifier, test_ratio): return crc32(np.int64(identifier)) &amp; 0xffffffff &lt; test_ratio * 2**32 # crc32(np.int64(identifier)) = create a hash from a given value # crc32(np.int64(identifier)) &amp; 0xffffffff = make sure the hash value does not exceed 2^32 (or 4294967296). # crc32(np.int64(identifier)) &amp; 0xffffffff &lt; test_ratio * 2**32. # crc32(np.int64(identifier)) &amp; 0xffffffff &lt; test_ratio * 2**32 # This line returns True or False. Let test_ratio be 0.2. # Then, any hash value less than 0.2 * 4294967296 returns True and will be # added to the test set; otherwise, it returns False and will be added to the training set. */ def split_train_test_by_id(data, test_ratio, id_column): ids = data[id_column] # compute a hash of each instance’s identifier in_test_set = ids.apply(lambda id_: test_set_check(id_, test_ratio)) # if hash is lower than or equal to 20% of the maximum hash value return data.loc[~in_test_set], data.loc[in_test_set] . Unfortunately, the housing dataset does not have an identifier column. The simplest solution is to use the row index as the ID: . housing_with_id = housing.reset_index() # adds an `index` column train_set, test_set = split_train_test_by_id(housing_with_id, 0.2, &quot;index&quot;) . If we use the row index as a unique identifier, you need to make sure that new data gets appended to the end of the dataset and that no row ever gets deleted. If this is not possible, then we can try to use the most stable features to build a unique identifier. . For example, a district’s latitude and longitude are guaranteed to be stable for a few million years, so you could combine them into an ID like so: . housing_with_id[&quot;id&quot;] = housing[&quot;longitude&quot;] * 1000 + housing[&quot;latitude&quot;] train_set, test_set = split_train_test_by_id(housing_with_id, 0.2, &quot;id&quot;) . See explanation of this method in : https://ichi.pro/fr/ameliorez-la-repartition-des-tests-de-train-avec-la-fonction-de-hachage-267796356735483 and https://datascience.stackexchange.com/questions/51348/splitting-train-test-sets-by-an-identifier . Train / Test split using Sckit-learn . Scikit-Learn provides a few functions to split datasets into multiple subsets in various ways. The simplest function is train_test_split(), which does pretty much the same thing as the function split_train_test(), with a couple of additional features : . First, there is a random_state parameter random_state that allows you to set the random generator seed and a test size test_size. | Second, we can pass it multiple datasets with an identical number of rows, and it will split them on the same indices (this is very useful, for example, if you have a separate DataFrame for labels): | . from sklearn.model_selection import train_test_split train_set, test_set = train_test_split(housing, test_size=0.2, random_state = 1997) . Sampling bias in Test set . Using train_test_splitmethod, we using purely random sampling methods to generate our test set. This is generally fine if our dataset is large enough (especially relative to the number of attributes), but if it is not, we run the risk of introducing a significant sampling bias. . If an attribute (continues or categorical) is important (after discussing with experts for exemple) : We may want to ensure that the test set is representative of the various categories of that variable in the whole dataset. . Suppose that the median income is a very important attribute to predict median housing prices. . housing[&#39;median_income&#39;].describe() count 20640.000000 mean 3.870671 std 1.899822 min 0.499900 25% 2.563400 50% 3.534800 75% 4.743250 max 15.000100 Name: median_income, dtype: float64 plt.hist(housing[&#39;median_income&#39;] #, bins = int( (housing[&#39;median_income&#39;].max() - housing[&#39;median_income&#39;].min()) / 0.5) , bins = 60 , color = &#39;blue&#39; ,edgecolor = &#39;black&#39; ) plt.show() . . housing[&quot;income_cat&quot;] = pd.cut(housing[&quot;median_income&quot;], bins=[0., 1.5, 3.0, 4.5, 6., np.inf], labels=[1, 2, 3, 4, 5]) housing[&#39;income_cat&#39;] = pd.cut( housing[&#39;median_income&#39;] , bins = [0., 1.5, 3.0, 4.5, 6., np.inf] , labels = [1, 2, 3, 4, 5] ) housing[&quot;income_cat&quot;].hist() &lt;AxesSubplot:&gt; . . housing[&quot;income_cat&quot;].value_counts() / len(housing) 3 0.350581 2 0.318847 4 0.176308 5 0.114438 1 0.039826 Name: income_cat, dtype: float64 from sklearn.model_selection import StratifiedShuffleSplit split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42) for train_index, test_index in split.split(housing, housing[&quot;income_cat&quot;]): strat_train_set = housing.loc[train_index] strat_test_set = housing.loc[test_index] strat_train_set[&quot;income_cat&quot;].value_counts() / len(strat_train_set) 3 0.350594 2 0.318859 4 0.176296 5 0.114462 1 0.039789 Name: income_cat, dtype: float64 strat_test_set[&quot;income_cat&quot;].value_counts() / len(strat_test_set) 3 0.350533 2 0.318798 4 0.176357 5 0.114341 1 0.039971 Name: income_cat, dtype: float64 def income_cat_proportions(data): return data[&quot;income_cat&quot;].value_counts() / len(data) train_set, test_set = train_test_split(housing, test_size=0.2, random_state=42) compare_props = pd.DataFrame({ &quot;Overall&quot;: income_cat_proportions(housing), &quot;Stratified&quot;: income_cat_proportions(strat_test_set), &quot;Random&quot;: income_cat_proportions(test_set), }).sort_index() compare_props[&quot;Rand. %error&quot;] = 100 * compare_props[&quot;Random&quot;] / compare_props[&quot;Overall&quot;] - 100 compare_props[&quot;Strat. %error&quot;] = 100 * compare_props[&quot;Stratified&quot;] / compare_props[&quot;Overall&quot;] - 100 compare_props Overall Stratified Random Rand. %error Strat. %error 1 0.039826 0.039971 0.040213 0.973236 0.364964 2 0.318847 0.318798 0.324370 1.732260 -0.015195 3 0.350581 0.350533 0.358527 2.266446 -0.013820 4 0.176308 0.176357 0.167393 -5.056334 0.027480 5 0.114438 0.114341 0.109496 -4.318374 -0.084674 . Further analysis later | . for set_ in (strat_train_set, strat_test_set): set_.drop(&quot;income_cat&quot;, axis=1, inplace=True) . Discover and Visualize the Data to Gain Insights . First, we make sure that we have put the test set aside and we are only exploring the training set. Also, if the training set is very large, you may want to sample an exploration set, to make manipulations easy and fast. In our case, the set is quite small, so we can just work directly on the full set. . Let’s create a copy so that you can play with it without harming the training set: | . housing = strat_train_set.copy() housing.head(5) longitude latitude housing_median_age total_rooms total_bedrooms 12655 -121.46 38.52 29.0 3873.0 797.0 15502 -117.23 33.09 7.0 5320.0 855.0 2908 -119.04 35.37 44.0 1618.0 310.0 14053 -117.13 32.75 24.0 1877.0 519.0 20496 -118.70 34.28 27.0 3536.0 646.0 population households median_income median_house_value 12655 2237.0 706.0 2.1736 72100.0 15502 2015.0 768.0 6.3373 279600.0 2908 667.0 300.0 2.8750 82700.0 14053 898.0 483.0 2.2264 112500.0 20496 1837.0 580.0 4.4964 238300.0 ocean_proximity income_cat 12655 INLAND 2 15502 NEAR OCEAN 5 2908 INLAND 2 14053 NEAR OCEAN 2 20496 &lt;1H OCEAN 3 . Since we have geographic information (lon / lat), let’s create a scatterplot of all districts to visualize the data : doc of a scatterplot parameter https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.scatter.html | . housing.plot(kind = &#39;scatter&#39;, x = &#39;longitude&#39;, y = &#39;latitude&#39;) &lt;AxesSubplot:xlabel=&#39;longitude&#39;, ylabel=&#39;latitude&#39;&gt; . . Scatter plots work well for hundreds of observations but overplotting becomes an issue once the number of observations gets into tens of thousands. . We can see that in some areas, there are vast numbers of dots, so it is hard to see any particular pattern. . Simple options to address overplotting : . reducing the point size : usisng the s parameter - This parameter indicates the marker size. | alpha blending : using alpha parameter This option indicates the blending value, between 0 (transparent) and 1 (opaque). | . housing.plot(kind = &#39;scatter&#39; ,x = &#39;longitude&#39; ,y = &#39;latitude&#39; , s= 0.2) &lt;AxesSubplot:xlabel=&#39;longitude&#39;, ylabel=&#39;latitude&#39;&gt; . . housing.plot(kind = &#39;scatter&#39; ,x = &#39;longitude&#39; ,y = &#39;latitude&#39; , alpha= 0.1) &lt;AxesSubplot:xlabel=&#39;longitude&#39;, ylabel=&#39;latitude&#39;&gt; . . We can get the names of the cities in the map and conclude which have the highest density - Many article covers this subject - we will do it later . # To save a picture in our folder project : IMAGES_PATH = &quot;/Users/rmbp/Desktop/housing&quot; def save_fig(fig_id, tight_layout=True, fig_extension=&quot;png&quot;, resolution=300): path = os.path.join(IMAGES_PATH, fig_id + &quot;.&quot; + fig_extension) print(&quot;Saving figure&quot;, fig_id) if tight_layout: plt.tight_layout() plt.savefig(path, format=fig_extension, dpi=resolution) housing.plot(kind = &#39;scatter&#39; ,x = &#39;longitude&#39; ,y = &#39;latitude&#39; , alpha= 0.1, c=&#39;black&#39;) save_fig(&quot;better_visualization_plot&quot;) Saving figure better_visualization_plot . . We can see the houses price crossing with the population on the map below : . the parameter s re presenting the radius of each circle will represents the `district’s population`` | the paramter c representing the color will represents the price. | We will use a predefined color map (option cmap) called jet, which ranges from blue (low values) to red (high prices): | . housing.plot(kind=&quot;scatter&quot;, x=&quot;longitude&quot;, y=&quot;latitude&quot;, alpha=0.4, s=housing[&quot;population&quot;]/100, label=&quot;population&quot;, figsize=(10,7), c=&quot;median_house_value&quot;, cmap=plt.get_cmap(&quot;jet&quot;), colorbar=True, ) plt.legend() &lt;matplotlib.legend.Legend at 0x12f4d1650&gt; . . This image tells us that the housing prices are very much related to the location (e.g., close to the ocean) and to the population density. | A clustering algorithm should be useful for detecting the main cluster and for adding new features that measure the proximity to the cluster centers. See later.. Check this blog : https://dev.to/travelleroncode/analyzing-a-dataset-with-unsupervised-learning-31ld | The ocean proximity attribute may be useful as well, although in Northern California the housing prices in coastal districts are not too high, so it is not a simple rule. | . Looking for correlations . If we want to explore our data it is good to compute correlation between numeric variable : Spearman S and Pearon P. W can compute them both since the relation between the Spearman (S) and Pearson (P) correlations will give some good information : . Briefly, S is computed on ranks and so depicts monotonic relationships while P is on true values and depicts linear relationships. . | We the corr method : By default, method = ‘Pearson’ . | . s = {} for x in range(1,100): s[x] = math.exp(x) s = pd.DataFrame(s.items()) s.corr(&#39;pearson&#39;) 0 1 0 1.000000 0.253274 1 0.253274 1.000000 s.corr(&#39;spearman&#39;) 0 1 0 1.0 1.0 1 1.0 1.0 . This is because 𝑦 increases monotonically with 𝑥 so the Spearman correlation is perfect, but not linearly, so the Pearson correlation is imperfect. . Doing both is interesting because if we have S &gt; P, that means that we have a correlation that is monotonic but not linear. Since it is good to have linearity in statistics (it is easier) we can try to apply a transformation on 𝑦(such a log). . corr_matrix = housing.corr() corr_matrix longitude latitude housing_median_age total_rooms longitude 1.000000 -0.924478 -0.105823 0.048909 latitude -0.924478 1.000000 0.005737 -0.039245 housing_median_age -0.105823 0.005737 1.000000 -0.364535 total_rooms 0.048909 -0.039245 -0.364535 1.000000 total_bedrooms 0.076686 -0.072550 -0.325101 0.929391 population 0.108071 -0.115290 -0.298737 0.855103 households 0.063146 -0.077765 -0.306473 0.918396 median_income -0.019615 -0.075146 -0.111315 0.200133 median_house_value -0.047466 -0.142673 0.114146 0.135140 total_bedrooms population households median_income longitude 0.076686 0.108071 0.063146 -0.019615 latitude -0.072550 -0.115290 -0.077765 -0.075146 housing_median_age -0.325101 -0.298737 -0.306473 -0.111315 total_rooms 0.929391 0.855103 0.918396 0.200133 total_bedrooms 1.000000 0.876324 0.980167 -0.009643 population 0.876324 1.000000 0.904639 0.002421 households 0.980167 0.904639 1.000000 0.010869 median_income -0.009643 0.002421 0.010869 1.000000 median_house_value 0.047781 -0.026882 0.064590 0.687151 median_house_value longitude -0.047466 latitude -0.142673 housing_median_age 0.114146 total_rooms 0.135140 total_bedrooms 0.047781 population -0.026882 households 0.064590 median_income 0.687151 median_house_value 1.000000 . Now let’s look at how much each attribute correlates with the median house value: . corr_matrix[&#39;median_house_value&#39;].sort_values(ascending = False) median_house_value 1.000000 median_income 0.687151 total_rooms 0.135140 housing_median_age 0.114146 households 0.064590 total_bedrooms 0.047781 population -0.026882 longitude -0.047466 latitude -0.142673 Name: median_house_value, dtype: float64 corr_matrix = housing.corr(&#39;spearman&#39;) corr_matrix[&#39;median_house_value&#39;].sort_values(ascending = False) median_house_value 1.000000 median_income 0.675714 total_rooms 0.204476 households 0.110722 total_bedrooms 0.084284 housing_median_age 0.083301 population 0.001309 longitude -0.071562 latitude -0.162283 Name: median_house_value, dtype: float64 . Another way to check for correlation between attributes is to use the pandas scatter_matrix() function, which plots every numerical attribute against every other numerical attribute. ( if we have 11 attribiute, we will plot 11**2 plots ) . From the pearson coefficient below, we focus on a few promising attributes that seem most correlated with the median housing value : . from pandas.plotting import scatter_matrix attributes = [&#39;median_house_value&#39;, &#39;median_income&#39;, &#39;total_rooms&#39;, &#39;housing_median_age&#39; ] scatter_matrix(housing[attributes], figsize=(10,6), ) array([[&lt;AxesSubplot:xlabel=&#39;median_house_value&#39;, ylabel=&#39;median_house_value&#39;&gt;, &lt;AxesSubplot:xlabel=&#39;median_income&#39;, ylabel=&#39;median_house_value&#39;&gt;, &lt;AxesSubplot:xlabel=&#39;total_rooms&#39;, ylabel=&#39;median_house_value&#39;&gt;, &lt;AxesSubplot:xlabel=&#39;housing_median_age&#39;, ylabel=&#39;median_house_value&#39;&gt;], [&lt;AxesSubplot:xlabel=&#39;median_house_value&#39;, ylabel=&#39;median_income&#39;&gt;, &lt;AxesSubplot:xlabel=&#39;median_income&#39;, ylabel=&#39;median_income&#39;&gt;, &lt;AxesSubplot:xlabel=&#39;total_rooms&#39;, ylabel=&#39;median_income&#39;&gt;, &lt;AxesSubplot:xlabel=&#39;housing_median_age&#39;, ylabel=&#39;median_income&#39;&gt;], [&lt;AxesSubplot:xlabel=&#39;median_house_value&#39;, ylabel=&#39;total_rooms&#39;&gt;, &lt;AxesSubplot:xlabel=&#39;median_income&#39;, ylabel=&#39;total_rooms&#39;&gt;, &lt;AxesSubplot:xlabel=&#39;total_rooms&#39;, ylabel=&#39;total_rooms&#39;&gt;, &lt;AxesSubplot:xlabel=&#39;housing_median_age&#39;, ylabel=&#39;total_rooms&#39;&gt;], [&lt;AxesSubplot:xlabel=&#39;median_house_value&#39;, ylabel=&#39;housing_median_age&#39;&gt;, &lt;AxesSubplot:xlabel=&#39;median_income&#39;, ylabel=&#39;housing_median_age&#39;&gt;, &lt;AxesSubplot:xlabel=&#39;total_rooms&#39;, ylabel=&#39;housing_median_age&#39;&gt;, &lt;AxesSubplot:xlabel=&#39;housing_median_age&#39;, ylabel=&#39;housing_median_age&#39;&gt;]], dtype=object) . . The main diagonal (top left to bottom right) would be full of straight lines if pandas plotted each variable against itself, which would not be very useful. So instead pandas displays a histogram of each attribute. The diagonal option in scatter_matrix pick between ‘kde’ and ‘hist’ for either Kernel Density Estimation or Histogram plot in the diagonal. . The most promising attribute to predict the median house value is the median income( Pearson and Spearman correlation coefficient = 0.67 ), so let’s zoom in on their correlation scatterplot : . housing.plot( kind = &#39;scatter&#39; ,x = &#39;median_income&#39; ,y = &#39;median_house_value&#39; ,alpha = 0.2) &lt;AxesSubplot:xlabel=&#39;median_income&#39;, ylabel=&#39;median_house_value&#39;&gt; . . This plot reveals a few things : . First, the correlation is indeed very strong; we can clearly see the upward trend, and the points are not too dispersed. | Second, the price cap that we noticed earlier is clearly visible as a horizontal line at $500,000. But this plot reveals other less obvious straight lines: a horizontal line around $450,000, another around $350,000, perhaps one around $280,000, and a few more below that, we can see this picks in the histogram above: As result, we may want to try removing the corresponding districts to prevent our algorithms from learning to reproduce these data quirks. | . housing[&#39;median_house_value&#39;].describe() count 16512.000000 mean 207005.322372 std 115701.297250 min 14999.000000 25% 119800.000000 50% 179500.000000 75% 263900.000000 max 500001.000000 Name: median_house_value, dtype: float64 # Data picks in the target variable plt.hist(housing[&#39;median_house_value&#39;] #, bins = int( (housing[&#39;median_income&#39;].max() - housing[&#39;median_income&#39;].min()) / 0.5) , bins = int ((500001.000000 - 14999.000000)/1000) , color = &#39;blue&#39; ,edgecolor = &#39;black&#39; ) (array([ 3., 0., 0., 0., 0., 0., 0., 3., 0., 0., 0., 2., 1., 1., 0., 1., 0., 4., 1., 3., 1., 2., 4., 2., 3., 4., 5., 10., 13., 11., 14., 11., 19., 17., 21., 27., 24., 36., 33., 39., 59., 30., 53., 50., 38., 43., 39., 45., 45., 35., 54., 48., 74., 59., 62., 54., 47., 61., 51., 39., 51., 41., 33., 47., 37., 39., 64., 43., 54., 59., 63., 55., 106., 73., 55., 86., 53., 84., 74., 73., 81., 70., 77., 71., 64., 79., 59., 45., 73., 50., 55., 49., 55., 68., 65., 62., 72., 110., 78., 54., 56., 66., 55., 79., 53., 57., 55., 60., 59., 43., 83., 61., 54., 45., 59., 52., 61., 51., 55., 65., 59., 68., 144., 51., 75., 72., 65., 75., 74., 64., 68., 79., 57., 52., 38., 111., 76., 64., 67., 77., 63., 94., 75., 83., 76., 96., 74., 145., 73., 74., 92., 88., 55., 61., 61., 79., 68., 52., 64., 53., 91., 50., 57., 63., 68., 52., 69., 59., 84., 74., 65., 70., 114., 44., 56., 63., 70., 69., 57., 51., 64., 52., 37., 49., 40., 59., 41., 35., 32., 52., 47., 45., 34., 47., 51., 41., 37., 51., 53., 50., 51., 44., 49., 66., 44., 55., 50., 49., 46., 38., 107., 56., 50., 48., 48., 52., 60., 51., 44., 52., 40., 41., 49., 44., 43., 53., 49., 27., 51., 39., 43., 30., 47., 37., 22., 50., 33., 36., 42., 43., 35., 20., 34., 40., 29., 29., 37., 44., 41., 39., 38., 40., 40., 39., 35., 30., 43., 34., 34., 65., 21., 33., 25., 29., 38., 22., 23., 26., 30., 20., 22., 24., 36., 22., 25., 28., 26., 26., 21., 24., 26., 16., 15., 9., 31., 12., 17., 19., 19., 18., 17., 18., 14., 15., 19., 11., 22., 14., 18., 21., 23., 15., 9., 24., 16., 17., 18., 23., 20., 28., 12., 12., 21., 11., 22., 17., 22., 19., 22., 19., 25., 21., 15., 14., 20., 25., 22., 20., 18., 22., 22., 16., 13., 22., 75., 14., 19., 19., 19., 15., 22., 13., 18., 16., 21., 16., 19., 24., 11., 13., 16., 17., 13., 11., 15., 4., 18., 9., 8., 26., 8., 14., 6., 8., 12., 12., 11., 10., 12., 14., 5., 13., 16., 7., 7., 11., 10., 12., 14., 15., 9., 11., 10., 10., 22., 4., 2., 12., 2., 10., 12., 11., 2., 4., 14., 9., 10., 10., 5., 13., 5., 13., 8., 13., 7., 9., 3., 8., 8., 12., 5., 5., 5., 5., 2., 11., 7., 6., 9., 11., 7., 7., 7., 9., 7., 7., 6., 7., 8., 9., 6., 7., 5., 2., 28., 6., 5., 7., 6., 5., 3., 6., 10., 6., 6., 1., 6., 4., 4., 3., 3., 6., 5., 3., 5., 3., 4., 6., 3., 10., 1., 2., 8., 4., 1., 3., 1., 3., 10., 7., 2., 4., 4., 3., 3., 4., 3., 4., 4., 2., 6., 2., 2., 5., 810.]), array([ 14999. , 15999.00412371, 16999.00824742, 17999.01237113, 18999.01649485, 19999.02061856, 20999.02474227, 21999.02886598, 22999.03298969, 23999.0371134 , 24999.04123711, 25999.04536082, 26999.04948454, 27999.05360825, 28999.05773196, 29999.06185567, 30999.06597938, 31999.07010309, 32999.0742268 , 33999.07835052, 34999.08247423, 35999.08659794, 36999.09072165, 37999.09484536, 38999.09896907, 39999.10309278, 40999.10721649, 41999.11134021, 42999.11546392, 43999.11958763, 44999.12371134, 45999.12783505, 46999.13195876, 47999.13608247, 48999.14020619, 49999.1443299 , 50999.14845361, 51999.15257732, 52999.15670103, 53999.16082474, 54999.16494845, 55999.16907216, 56999.17319588, 57999.17731959, 58999.1814433 , 59999.18556701, 60999.18969072, 61999.19381443, 62999.19793814, 63999.20206186, 64999.20618557, 65999.21030928, 66999.21443299, 67999.2185567 , 68999.22268041, 69999.22680412, 70999.23092784, 71999.23505155, 72999.23917526, 73999.24329897, 74999.24742268, 75999.25154639, 76999.2556701 , 77999.25979381, 78999.26391753, 79999.26804124, 80999.27216495, 81999.27628866, 82999.28041237, 83999.28453608, 84999.28865979, 85999.29278351, 86999.29690722, 87999.30103093, 88999.30515464, 89999.30927835, 90999.31340206, 91999.31752577, 92999.32164948, 93999.3257732 , 94999.32989691, 95999.33402062, 96999.33814433, 97999.34226804, 98999.34639175, 99999.35051546, 100999.35463918, 101999.35876289, 102999.3628866 , 103999.36701031, 104999.37113402, 105999.37525773, 106999.37938144, 107999.38350515, 108999.38762887, 109999.39175258, 110999.39587629, 111999.4 , 112999.40412371, 113999.40824742, 114999.41237113, 115999.41649485, 116999.42061856, 117999.42474227, 118999.42886598, 119999.43298969, 120999.4371134 , 121999.44123711, 122999.44536082, 123999.44948454, 124999.45360825, 125999.45773196, 126999.46185567, 127999.46597938, 128999.47010309, 129999.4742268 , 130999.47835052, 131999.48247423, 132999.48659794, 133999.49072165, 134999.49484536, 135999.49896907, 136999.50309278, 137999.50721649, 138999.51134021, 139999.51546392, 140999.51958763, 141999.52371134, 142999.52783505, 143999.53195876, 144999.53608247, 145999.54020619, 146999.5443299 , 147999.54845361, 148999.55257732, 149999.55670103, 150999.56082474, 151999.56494845, 152999.56907216, 153999.57319588, 154999.57731959, 155999.5814433 , 156999.58556701, 157999.58969072, 158999.59381443, 159999.59793814, 160999.60206186, 161999.60618557, 162999.61030928, 163999.61443299, 164999.6185567 , 165999.62268041, 166999.62680412, 167999.63092784, 168999.63505155, 169999.63917526, 170999.64329897, 171999.64742268, 172999.65154639, 173999.6556701 , 174999.65979381, 175999.66391753, 176999.66804124, 177999.67216495, 178999.67628866, 179999.68041237, 180999.68453608, 181999.68865979, 182999.69278351, 183999.69690722, 184999.70103093, 185999.70515464, 186999.70927835, 187999.71340206, 188999.71752577, 189999.72164948, 190999.7257732 , 191999.72989691, 192999.73402062, 193999.73814433, 194999.74226804, 195999.74639175, 196999.75051546, 197999.75463918, 198999.75876289, 199999.7628866 , 200999.76701031, 201999.77113402, 202999.77525773, 203999.77938144, 204999.78350515, 205999.78762887, 206999.79175258, 207999.79587629, 208999.8 , 209999.80412371, 210999.80824742, 211999.81237113, 212999.81649485, 213999.82061856, 214999.82474227, 215999.82886598, 216999.83298969, 217999.8371134 , 218999.84123711, 219999.84536082, 220999.84948454, 221999.85360825, 222999.85773196, 223999.86185567, 224999.86597938, 225999.87010309, 226999.8742268 , 227999.87835052, 228999.88247423, 229999.88659794, 230999.89072165, 231999.89484536, 232999.89896907, 233999.90309278, 234999.90721649, 235999.91134021, 236999.91546392, 237999.91958763, 238999.92371134, 239999.92783505, 240999.93195876, 241999.93608247, 242999.94020619, 243999.9443299 , 244999.94845361, 245999.95257732, 246999.95670103, 247999.96082474, 248999.96494845, 249999.96907216, 250999.97319588, 251999.97731959, 252999.9814433 , 253999.98556701, 254999.98969072, 255999.99381443, 256999.99793814, 258000.00206186, 259000.00618557, 260000.01030928, 261000.01443299, 262000.0185567 , 263000.02268041, 264000.02680412, 265000.03092784, 266000.03505155, 267000.03917526, 268000.04329897, 269000.04742268, 270000.05154639, 271000.0556701 , 272000.05979381, 273000.06391753, 274000.06804124, 275000.07216495, 276000.07628866, 277000.08041237, 278000.08453608, 279000.08865979, 280000.09278351, 281000.09690722, 282000.10103093, 283000.10515464, 284000.10927835, 285000.11340206, 286000.11752577, 287000.12164948, 288000.1257732 , 289000.12989691, 290000.13402062, 291000.13814433, 292000.14226804, 293000.14639175, 294000.15051546, 295000.15463918, 296000.15876289, 297000.1628866 , 298000.16701031, 299000.17113402, 300000.17525773, 301000.17938144, 302000.18350515, 303000.18762887, 304000.19175258, 305000.19587629, 306000.2 , 307000.20412371, 308000.20824742, 309000.21237113, 310000.21649485, 311000.22061856, 312000.22474227, 313000.22886598, 314000.23298969, 315000.2371134 , 316000.24123711, 317000.24536082, 318000.24948454, 319000.25360825, 320000.25773196, 321000.26185567, 322000.26597938, 323000.27010309, 324000.2742268 , 325000.27835052, 326000.28247423, 327000.28659794, 328000.29072165, 329000.29484536, 330000.29896907, 331000.30309278, 332000.30721649, 333000.31134021, 334000.31546392, 335000.31958763, 336000.32371134, 337000.32783505, 338000.33195876, 339000.33608247, 340000.34020619, 341000.3443299 , 342000.34845361, 343000.35257732, 344000.35670103, 345000.36082474, 346000.36494845, 347000.36907216, 348000.37319588, 349000.37731959, 350000.3814433 , 351000.38556701, 352000.38969072, 353000.39381443, 354000.39793814, 355000.40206186, 356000.40618557, 357000.41030928, 358000.41443299, 359000.4185567 , 360000.42268041, 361000.42680412, 362000.43092784, 363000.43505155, 364000.43917526, 365000.44329897, 366000.44742268, 367000.45154639, 368000.4556701 , 369000.45979381, 370000.46391753, 371000.46804124, 372000.47216495, 373000.47628866, 374000.48041237, 375000.48453608, 376000.48865979, 377000.49278351, 378000.49690722, 379000.50103093, 380000.50515464, 381000.50927835, 382000.51340206, 383000.51752577, 384000.52164948, 385000.5257732 , 386000.52989691, 387000.53402062, 388000.53814433, 389000.54226804, 390000.54639175, 391000.55051546, 392000.55463918, 393000.55876289, 394000.5628866 , 395000.56701031, 396000.57113402, 397000.57525773, 398000.57938144, 399000.58350515, 400000.58762887, 401000.59175258, 402000.59587629, 403000.6 , 404000.60412371, 405000.60824742, 406000.61237113, 407000.61649485, 408000.62061856, 409000.62474227, 410000.62886598, 411000.63298969, 412000.6371134 , 413000.64123711, 414000.64536082, 415000.64948454, 416000.65360825, 417000.65773196, 418000.66185567, 419000.66597938, 420000.67010309, 421000.6742268 , 422000.67835052, 423000.68247423, 424000.68659794, 425000.69072165, 426000.69484536, 427000.69896907, 428000.70309278, 429000.70721649, 430000.71134021, 431000.71546392, 432000.71958763, 433000.72371134, 434000.72783505, 435000.73195876, 436000.73608247, 437000.74020619, 438000.7443299 , 439000.74845361, 440000.75257732, 441000.75670103, 442000.76082474, 443000.76494845, 444000.76907216, 445000.77319588, 446000.77731959, 447000.7814433 , 448000.78556701, 449000.78969072, 450000.79381443, 451000.79793814, 452000.80206186, 453000.80618557, 454000.81030928, 455000.81443299, 456000.8185567 , 457000.82268041, 458000.82680412, 459000.83092784, 460000.83505155, 461000.83917526, 462000.84329897, 463000.84742268, 464000.85154639, 465000.8556701 , 466000.85979381, 467000.86391753, 468000.86804124, 469000.87216495, 470000.87628866, 471000.88041237, 472000.88453608, 473000.88865979, 474000.89278351, 475000.89690722, 476000.90103093, 477000.90515464, 478000.90927835, 479000.91340206, 480000.91752577, 481000.92164948, 482000.9257732 , 483000.92989691, 484000.93402062, 485000.93814433, 486000.94226804, 487000.94639175, 488000.95051546, 489000.95463918, 490000.95876289, 491000.9628866 , 492000.96701031, 493000.97113402, 494000.97525773, 495000.97938144, 496000.98350515, 497000.98762887, 498000.99175258, 499000.99587629, 500001. ]), &lt;BarContainer object of 485 artists&gt;) . . Check docs on how to detect picks : . Finding peaks in the histograms of the variables : https://www.kaggle.com/simongrest/finding-peaks-in-the-histograms-of-the-variables . | Peak-finding algorithm for Python/SciPy : https://stackoverflow.com/questions/1713335/peak-finding-algorithm-for-python-scipy . | . Experimenting with Attribute Combinations . We identified a few data quirks that we may want to clean up before feeding the data to a Machine Learning algorithm, | We found interesting correlations between attributes, in particular with the target attribute. | We also noticed that some attributes have a tail-heavy distribution, so you may want to transform them (e.g., by computing their logarithm). | One last thing we may want to do before preparing the data for Machine Learning algorithms is to try out various attribute combinations : For example, the total number of rooms in a district is not very useful if we don’t know how many households there are. What we really want is the number of rooms per household. Similarly, the total number of bedrooms by itself is not very useful: you probably want to compare it to the number of rooms. And the population per household also seems like an interesting attribute combination to look at. Let’s create these new attributes: | . # We can see that some attributes are very linked to each others housing[[&#39;total_rooms&#39;,&#39;total_bedrooms&#39;,&#39;households&#39;,&#39;population&#39; ]].corr() total_rooms total_bedrooms households population total_rooms 1.000000 0.930380 0.918484 0.857126 total_bedrooms 0.930380 1.000000 0.979728 0.877747 households 0.918484 0.979728 1.000000 0.907222 population 0.857126 0.877747 0.907222 1.000000 . To highlight the matrix correlation, we can use heatmap from seaborn: . import seaborn as sns cor= housing[[&#39;total_rooms&#39;,&#39;total_bedrooms&#39;,&#39;households&#39;,&#39;population&#39; ]].corr() sns.heatmap(cor, cmap=&#39;Blues&#39;, annot= True) &lt;AxesSubplot:&gt; . . housing[&quot;rooms_per_household&quot;] = housing[&quot;total_rooms&quot;]/housing[&quot;households&quot;] housing[&quot;bedrooms_per_room&quot;] = housing[&quot;total_bedrooms&quot;]/housing[&quot;total_rooms&quot;] housing[&quot;population_per_household&quot;]=housing[&quot;population&quot;]/housing[&quot;households&quot;] # nbre of person per houshold housing[&quot;bedrooms_per_room&quot;].describe() count 20433.000000 mean 0.213039 std 0.057983 min 0.100000 25% 0.175427 50% 0.203162 75% 0.239821 max 1.000000 Name: bedrooms_per_room, dtype: float64 # on average, we have 21 bedrooms for 100 rooms from fractions import Fraction z = Fraction(0.21).limit_denominator() z Fraction(21, 100) corr_matrix = housing.corr() corr_matrix[&#39;median_house_value&#39;].sort_values(ascending = False) median_house_value 1.000000 median_income 0.688075 rooms_per_household 0.151948 total_rooms 0.134153 housing_median_age 0.105623 households 0.065843 total_bedrooms 0.049686 population_per_household -0.023737 population -0.024650 longitude -0.045967 latitude -0.144160 bedrooms_per_room -0.255880 Name: median_house_value, dtype: float64 . The new bedrooms_per_room attribute is much more correlated (0.25)with the median house value than the total number of rooms(0.13) or bedrooms (0.04) : . Apparently houses with a lower bedroom/room ratio tend to be more expensive. | The number of rooms per household is also more informative than the total number of rooms in a district—obviously the larger the houses, the more expensive they are. | . Prepare the Data for Machine Learning Algorithms . let’s revert to a clean training set (by copying strat_train_set once again). | Let’s also separate the predictors and the labels, since we don’t necessarily want to apply the same transformations to the predictors and the target values. | . # drop() creates a copy of the data and does not affect strat_train_set housing = strat_train_set.drop(&quot;median_house_value&quot;, axis=1) housing_labels = strat_train_set[&quot;median_house_value&quot;].copy() housing = strat_train_set.drop(&#39;median_house_value&#39;, axis = 1) housing_labels = strat_train_set[&#39;median_house_value&#39;].copy() . Data Cleaning . Formissing values (like for total_bedrooms), we have three options: . Get rid of the corresponding districts. | Get rid of the whole attribute. | Set the values to some value (zero, the mean, the median, etc.) | We can accomplish these easily using DataFrame’s dropna(), drop(), and fillna() . housing[&#39;total_bedrooms&#39;].describe() count 16354.000000 mean 534.914639 std 412.665649 min 2.000000 25% 295.000000 50% 433.000000 75% 644.000000 max 6210.000000 Name: total_bedrooms, dtype: float64 # in bar chart, NanN&#39;s are filled with 0&#39;s plt.hist(housing[&#39;total_bedrooms&#39;] , bins = int ((6210.000000 - 2.000000 )/500) , color = &#39;blue&#39; , edgecolor = &#39;black&#39; ) (array([1.0221e+04, 4.7670e+03, 9.1400e+02, 2.6100e+02, 9.9000e+01, 5.1000e+01, 1.7000e+01, 1.1000e+01, 7.0000e+00, 3.0000e+00, 2.0000e+00, 1.0000e+00]), array([2.00000000e+00, 5.19333333e+02, 1.03666667e+03, 1.55400000e+03, 2.07133333e+03, 2.58866667e+03, 3.10600000e+03, 3.62333333e+03, 4.14066667e+03, 4.65800000e+03, 5.17533333e+03, 5.69266667e+03, 6.21000000e+03]), &lt;BarContainer object of 12 artists&gt;) . . # to count the number of Nan&#39;s housing[&#39;total_bedrooms&#39;].isna().sum() 158 housing.isna().sum() longitude 0 latitude 0 housing_median_age 0 total_rooms 0 total_bedrooms 158 population 0 households 0 median_income 0 ocean_proximity 0 income_cat 0 dtype: int64 # option 1 : Get rid of the corresponding districts housing.dropna(subset = [&#39;total_bedrooms&#39;]) # drop from 16512 to 16354 using len() # option 2 : Get rid of the whole attribute housing.drop(&#39;total_bedrooms&#39;, axis = 1) # option 3 : Set the values to some value (zero, the mean, the median, etc.) median = housing[&#39;total_bedrooms&#39;].median() housing[&#39;total_bedrooms&#39;].fillna(median, inplace = True) longitude latitude housing_median_age total_rooms population 12655 -121.46 38.52 29.0 3873.0 2237.0 15502 -117.23 33.09 7.0 5320.0 2015.0 2908 -119.04 35.37 44.0 1618.0 667.0 14053 -117.13 32.75 24.0 1877.0 898.0 20496 -118.70 34.28 27.0 3536.0 1837.0 ... ... ... ... ... ... 15174 -117.07 33.03 14.0 6665.0 2026.0 12661 -121.42 38.51 15.0 7901.0 4769.0 19263 -122.72 38.44 48.0 707.0 458.0 19140 -122.70 38.31 14.0 3155.0 1208.0 19773 -122.14 39.97 27.0 1079.0 625.0 households median_income ocean_proximity income_cat 12655 706.0 2.1736 INLAND 2 15502 768.0 6.3373 NEAR OCEAN 5 2908 300.0 2.8750 INLAND 2 14053 483.0 2.2264 NEAR OCEAN 2 20496 580.0 4.4964 &lt;1H OCEAN 3 ... ... ... ... ... 15174 1001.0 5.0900 &lt;1H OCEAN 4 12661 1418.0 2.8139 INLAND 2 19263 172.0 3.1797 &lt;1H OCEAN 3 19140 501.0 4.1964 &lt;1H OCEAN 3 19773 197.0 3.1319 INLAND 3 [16512 rows x 9 columns] . For ‘option’ 3 : fill missings with some value, the median for example, we should : . Compute the median value on the training and use it to fill the missing values in the training set. | Save the median value that you have computed. | Using later for to replace missing values in the test set when we want to evaluate our system | Using it once the system goes live to replace missing values in new data. | . Scikit-Learn provides a handy class to take care of missing values: SimpleImputer. . from sklearn.impute import SimpleImputer . First, you need to create a SimpleImputer instance, specifying that we want to replace each numeric attribute’s missing values with the median of that attribute : . imputer = SimpleImputer(strategy = &#39;median&#39;) housing.dtypes longitude float64 latitude float64 housing_median_age float64 total_rooms float64 total_bedrooms float64 population float64 households float64 median_income float64 ocean_proximity object dtype: object # We drop the categorical variables since the median can only be computed on numerical attributes housing_num = housing.drop([&#39;ocean_proximity&#39;], axis = 1) . Now you can fit the imputer instance to the training data using the fit() method . imputer.fit(housing_num) SimpleImputer(strategy=&#39;median&#39;) . The imputer has simply computed the median of each attribute and stored the result in its statistics_ instance variable. We apply the imputer to all the numerical attributes : . imputer.statistics_ array([-118.51 , 34.26 , 29. , 2119. , 433. , 1164. , 408. , 3.54155]) housing_num.columns Index([&#39;longitude&#39;, &#39;latitude&#39;, &#39;housing_median_age&#39;, &#39;total_rooms&#39;, &#39;total_bedrooms&#39;, &#39;population&#39;, &#39;households&#39;, &#39;median_income&#39;], dtype=&#39;object&#39;) # equivalant to imputer.statistics_ : we have the median for each numeric variable housing_num.median().values array([-118.51 , 34.26 , 29. , 2119. , 433. , 1164. , 408. , 3.54155]) . Now we can use this trained imputer to transform the training set by replacing missing values with the learned medians: . # The result is a plain NumPy array containing the transformed features. X = imputer.transform(housing_num) #If you want to put it back into a pandas DataFrame, it’s simple: housing_tr = pd.DataFrame(X, columns=housing_num.columns, index=housing_num.index) . Alternative method to fit() and transform() method is using directy fit_transform() method . X = imputer.fit_transform(housing_num) housing_tr = pd.DataFrame( X, columns = housing_num.columns , index = housing_num.index) housing_tr longitude latitude housing_median_age total_rooms total_bedrooms 12655 -121.46 38.52 29.0 3873.0 797.0 15502 -117.23 33.09 7.0 5320.0 855.0 2908 -119.04 35.37 44.0 1618.0 310.0 14053 -117.13 32.75 24.0 1877.0 519.0 20496 -118.70 34.28 27.0 3536.0 646.0 ... ... ... ... ... ... 15174 -117.07 33.03 14.0 6665.0 1231.0 12661 -121.42 38.51 15.0 7901.0 1422.0 19263 -122.72 38.44 48.0 707.0 166.0 19140 -122.70 38.31 14.0 3155.0 580.0 19773 -122.14 39.97 27.0 1079.0 222.0 population households median_income 12655 2237.0 706.0 2.1736 15502 2015.0 768.0 6.3373 2908 667.0 300.0 2.8750 14053 898.0 483.0 2.2264 20496 1837.0 580.0 4.4964 ... ... ... ... 15174 2026.0 1001.0 5.0900 12661 4769.0 1418.0 2.8139 19263 458.0 172.0 3.1797 19140 1208.0 501.0 4.1964 19773 625.0 197.0 3.1319 [16512 rows x 8 columns] . Handling Text and Categorical Attributes . So far we have only dealt with numerical attributes, but now let’s look at text attributes. In this dataset, there is just one: the ocean_proximity attribute. Let’s look at its value for the first 10 instances: . housing_cat = housing[[&#39;ocean_proximity&#39;]] housing_cat.head(10) housing_cat.nunique() ocean_proximity 5 dtype: int64 housing[&#39;ocean_proximity&#39;].unique() array([&#39;INLAND&#39;, &#39;NEAR OCEAN&#39;, &#39;&lt;1H OCEAN&#39;, &#39;NEAR BAY&#39;, &#39;ISLAND&#39;], dtype=object) . It’s not arbitrary text: there are a limited number of possible values, each of which represents a category. So this attribute is a categorical attribute. Most Machine Learning algorithms prefer to work with numbers, so let’s convert these categories from text to numbers. For this, we can use Scikit-Learn’s OrdinalEncoder class : . from sklearn.preprocessing import OrdinalEncoder ordinal_encoder = OrdinalEncoder() housing_cat_encoded = ordinal_encoder.fit_transform(housing_cat) # categorical dataframe housing_cat_encoded[:10] array([[1.], [4.], [1.], [4.], [0.], [3.], [0.], [0.], [0.], [0.]]) housing_cat_encoded.shape (16512, 1) # we can get the list of categories using the categories_ instance variable. It is a list containing a 1D array #of categories for each categorical attribute ordinal_encoder.categories_ [array([&#39;&lt;1H OCEAN&#39;, &#39;INLAND&#39;, &#39;ISLAND&#39;, &#39;NEAR BAY&#39;, &#39;NEAR OCEAN&#39;], dtype=object)] np.unique(housing_cat_encoded) array([0., 1., 2., 3., 4.]) . One issue with this representation is that ML algorithms ( OrdinaEncoder) will assume that two nearby values are more similar than two distant values. This may be fine in some cases (e.g., for ordered categories such as “bad,” “average,” “good,” and “excellent”) : ordinal encoding for categorical variables that have a natural rank ordering but it is obviously not the case for the ocean_proximity column (for example, categories 0 and 4 are clearly more similar than categories 0 and 1). . To fix this issue, a common solution is to create one binary attribute per category: one attribute equal to 1 when the category is “&lt;1H OCEAN” (and 0 otherwise), another attribute equal to 1 when the category is “INLAND” (and 0 otherwise), and so on. This is called one-hot encoding. The new attributes are sometimes called dummy attributes. Scikit-Learn provides a OneHotEncoder class to convert categorical values into one-hot vectors . See this blogpost : https://machinelearningmastery.com/one-hot-encoding-for-categorical-data/ . from sklearn.preprocessing import OneHotEncoder cat_encoder = OneHotEncoder() housing_cat_1hot = cat_encoder.fit_transform(housing_cat) housing_cat_1hot &lt;16512x5 sparse matrix of type &#39;&lt;class &#39;numpy.float64&#39;&gt;&#39; with 16512 stored elements in Compressed Sparse Row format&gt; . Notice that the output is a SciPy sparse matrix, instead of a NumPy array. This is very useful when you have categorical attributes with thousands of categories. After onehot encoding, we get a matrix with thousands of columns, and the matrix is full of 0s except for a single 1 per row. Using up tons of memory mostly to store zeros would be very wasteful, so instead a sparse matrix only stores the location of the nonzero elements. we can use it mostly like a normal 2D array,but if we really want to convert it to a (dense) NumPy array, we call the toarray() method: . housing_cat_1hot.toarray() array([[0., 1., 0., 0., 0.], [0., 0., 0., 0., 1.], [0., 1., 0., 0., 0.], ..., [1., 0., 0., 0., 0.], [1., 0., 0., 0., 0.], [0., 1., 0., 0., 0.]]) housing_cat.head(3) ocean_proximity 12655 INLAND 15502 NEAR OCEAN 2908 INLAND # to get the list of categories cat_encoder.categories_ [array([&#39;&lt;1H OCEAN&#39;, &#39;INLAND&#39;, &#39;ISLAND&#39;, &#39;NEAR BAY&#39;, &#39;NEAR OCEAN&#39;], dtype=object)] . If a categorical attribute has a large number of possible categories (e.g., country code, profession, species), then one-hot encoding will result in a large number of input features. This may slow down training and degrade performance. . If this happens, we may want to replace the categorical input with useful numerical features related to the categories: for example, we could replace the ocean_proximity feature with the distance to the ocean (similarly, a country code could be replaced with the country’s population and GDP per capita). Alternatively, we could replace each category with a learnable, low-dimensional vector called an embedding. . Each category’s representation would be learned during training. This is an example of representation learning. . Custom Transformers . retun back for more details ? see blogpost : https://towardsdatascience.com/pipelines-custom-transformers-in-scikit-learn-the-step-by-step-guide-with-python-code-4a7d9b068156 . and this : https://github.com/ageron/handson-ml2/blob/master/02_end_to_end_machine_learning_project.ipynb . housing.values[: ,4:] array([[797.0, 2237.0, 706.0, 2.1736, &#39;INLAND&#39;, 2], [855.0, 2015.0, 768.0, 6.3373, &#39;NEAR OCEAN&#39;, 5], [310.0, 667.0, 300.0, 2.875, &#39;INLAND&#39;, 2], ..., [166.0, 458.0, 172.0, 3.1797, &#39;&lt;1H OCEAN&#39;, 3], [580.0, 1208.0, 501.0, 4.1964, &#39;&lt;1H OCEAN&#39;, 3], [222.0, 625.0, 197.0, 3.1319, &#39;INLAND&#39;, 3]], dtype=object) housing.head(3) longitude latitude housing_median_age total_rooms total_bedrooms 12655 -121.46 38.52 29.0 3873.0 797.0 15502 -117.23 33.09 7.0 5320.0 855.0 2908 -119.04 35.37 44.0 1618.0 310.0 population households median_income ocean_proximity income_cat 12655 2237.0 706.0 2.1736 INLAND 2 15502 2015.0 768.0 6.3373 NEAR OCEAN 5 2908 667.0 300.0 2.8750 INLAND 2 from sklearn.base import BaseEstimator, TransformerMixin rooms_ix, bedrooms_ix, population_ix, households_ix = 3, 4, 5, 6 class CombinedAttributesAdder(BaseEstimator, TransformerMixin): def __init__(self, add_bedrooms_per_room = True): # no *args or **kargs self.add_bedrooms_per_room = add_bedrooms_per_room def fit(self, X, y=None): return self # nothing else to do def transform(self, X): rooms_per_household = X[:, rooms_ix] / X[:, households_ix] population_per_household = X[:, population_ix] / X[:, households_ix] if self.add_bedrooms_per_room: bedrooms_per_room = X[:, bedrooms_ix] / X[:, rooms_ix] return np.c_[X, rooms_per_household, population_per_household, bedrooms_per_room] else: return np.c_[X, rooms_per_household, population_per_household] attr_adder = CombinedAttributesAdder(add_bedrooms_per_room=False ) housing_extra_attribs = attr_adder.transform(housing.values) housing_extra_attribs array([[-121.46, 38.52, 29.0, ..., 2, 5.485835694050992, 3.168555240793201], [-117.23, 33.09, 7.0, ..., 5, 6.927083333333333, 2.6236979166666665], [-119.04, 35.37, 44.0, ..., 2, 5.3933333333333335, 2.223333333333333], ..., [-122.72, 38.44, 48.0, ..., 3, 4.1104651162790695, 2.6627906976744184], [-122.7, 38.31, 14.0, ..., 3, 6.297405189620759, 2.411177644710579], [-122.14, 39.97, 27.0, ..., 3, 5.477157360406092, 3.1725888324873095]], dtype=object) . Feature Scaling . One of the most important transformations you need to apply to your data is feature scaling. With few exceptions, Machine Learning algorithms don’t perform well when the input numerical attributes have very different scales . This is the case for the housing data: total_rooms ranges from about 6 to 39320, while median_income only range from 0 to 15 : . Note that scaling the target values is generally not required. . housing_num.describe().T count mean std min 25% longitude 16512.0 -119.575635 2.001828 -124.3500 -121.80000 latitude 16512.0 35.639314 2.137963 32.5400 33.94000 housing_median_age 16512.0 28.653404 12.574819 1.0000 18.00000 total_rooms 16512.0 2622.539789 2138.417080 6.0000 1443.00000 total_bedrooms 16512.0 533.939438 410.806260 2.0000 296.00000 population 16512.0 1419.687379 1115.663036 3.0000 784.00000 households 16512.0 497.011810 375.696156 2.0000 279.00000 median_income 16512.0 3.875884 1.904931 0.4999 2.56695 50% 75% max longitude -118.51000 -118.010000 -114.3100 latitude 34.26000 37.720000 41.9500 housing_median_age 29.00000 37.000000 52.0000 total_rooms 2119.00000 3141.000000 39320.0000 total_bedrooms 433.00000 641.000000 6210.0000 population 1164.00000 1719.000000 35682.0000 households 408.00000 602.000000 5358.0000 median_income 3.54155 4.745325 15.0001 . There are two common ways to get all attributes to have the same scale: . min-max scaling : ranging from 0 to 1. Scikit-Learn provides a transformer called MinMaxScaler for this. It has a feature_range hyperparameter that lets you change the range if, for some reason, you don’t want 0–1 : https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html . | Standardization : returns has unit variance and unlike min-max scaling, standardization does not bound values to a specific range, which may be a problem for some algorithms (e.g., neural networks often expect an input value ranging from 0 to 1).However, standardization is much less affected by outliers. Scikit-Learn provides a transformer called StandardScaler for standardization : https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html . | . Transformation Pipelines . As we can see, there are many data transformation steps that need to be executed in the right order. Fortunately, Scikit-Learn provides the Pipeline class to help with such sequences of transformations. . Here is a small pipeline for the numerical attributes: . from sklearn.pipeline import Pipeline from sklearn.impute import SimpleImputer from sklearn.preprocessing import StandardScaler,MinMaxScaler # plus the class add attribure that we created . The Pipeline constructor takes a list of name/estimator pairs defining a sequence of steps. All but the last estimator must be transformers (i.e., they must have a fit_transform() method). . # This pipeline is for numeric pipeline, we call is num_pipeline num_pipeline = Pipeline([ (&#39;imputer&#39;, SimpleImputer(strategy = &#39;median&#39;)), #(&#39;attribs_adder&#39;, CombinedAttributesAdder()), (&#39;std_scaler&#39;, StandardScaler()) ]) housing_num_tr = num_pipeline.fit_transform(housing_num) . So far, we have handled the categorical columns and the numerical columns separately. It would be more convenient to have a single transformer able to handle all columns, applying the appropriate transformations to each column. In version 0.20, Scikit-Learn introduced the ColumnTransformer for this purpose, and the good news is that it works great with pandas DataFrames. Let’s use it to apply all the transformations to the housing data: . from sklearn.compose import ColumnTransformer num_attribs = list(housing_num) cat_attribs = [&#39;ocean_proximity&#39;] NameError Traceback (most recent call last) &lt;ipython-input-2-344d101417c2&gt; in &lt;module&gt; -&gt; 1 num_attribs = list(housing_num) 2 cat_attribs = [&#39;ocean_proximity&#39;] NameError: name &#39;housing_num&#39; is not defined full_pipeline = ColumnTransformer([ (&#39;num&#39;, num_pipeline, num_attribs), (&#39;cat&#39;, OneHotEncoder(), cat_attribs) ] ) housing_prepared = full_pipeline.fit_transform(housing) housing_prepared array([[-0.94135046, 1.34743822, 0.02756357, ..., 0. , 0. , 0. ], [ 1.17178212, -1.19243966, -1.72201763, ..., 0. , 0. , 1. ], [ 0.26758118, -0.1259716 , 1.22045984, ..., 0. , 0. , 0. ], ..., [-1.5707942 , 1.31001828, 1.53856552, ..., 0. , 0. , 0. ], [-1.56080303, 1.2492109 , -1.1653327 , ..., 0. , 0. , 0. ], [-1.28105026, 2.02567448, -0.13148926, ..., 0. , 0. , 0. ]]) housing_prepared[0].shape (13,) housing_prepared.shape (16512, 13) . to read carefully for later : . First we import the ColumnTransformer class, next we get the list of numerical column . names and the list of categorical column names, and then we construct a Colum nTransformer. The constructor requires a list of tuples, where each tuple contains a name,22 a transformer, and a list of names (or indices) of columns that the transformer should be applied to. In this example, we specify that the numerical columns should be transformed using the num_pipeline that we defined earlier, and the categorical columns should be transformed using a OneHotEncoder. Finally, we apply this ColumnTransformer to the housing data: it applies each transformer to the appropriate columns and concatenates the outputs along the second axis (the transformers must return the same number of rows). Note that the OneHotEncoder returns a sparse matrix, while the num_pipeline returns a dense matrix. When there is such a mix of sparse and dense matrices, the Colum nTransformer estimates the density of the final matrix (i.e., the ratio of nonzero cells), and it returns a sparse matrix if the density is lower than a given threshold (by default, sparse_threshold=0.3). In this example, it returns a dense matrix. And that’s it! We have a preprocessing pipeline that takes the full housing data and applies the appropriate transformations to each column. Instead of using a transformer, you can specify the string “drop” if you want the columns to be dropped, or you can specify “pass through” if you want the columns to be left untouched. By default, the remaining columns (i.e., the ones that were not listed) will be dropped, but you can set the remainder hyperparameter to any transformer (or to “passthrough”) if you want these columns to be handled differently. If you are using Scikit-Learn 0.19 or earlier, you can use a third-party library such as sklearn-pandas, or you can roll out your own custom transformer to get the same functionality as the ColumnTransformer. Alternatively, you can use the FeatureUnion class, which can apply different transformers and concatenate their outputs. But you cannot specify different columns for each transformer; they all apply to the whole data. It is possible to work around this limitation using a custom transformer for column selection (see the Jupyter notebook for an example). . see link : https://github.com/ageron/handson-ml2/blob/master/02_end_to_end_machine_learning_project.ipynb . Select and Train a Model . we framed the problem | we got the data and explored it | we sampled a training set and a test set | we wrote transformation pipelines to clean up and prepare your data for Machine Learning algorithms automatically. | . Training and Evaluating on the Training Set . Let’s first train a Linear Regression model . housing_prepared.shape (16512, 13) from sklearn.linear_model import LinearRegression lin_reg = LinearRegression() lin_reg.fit(housing_prepared, housing_labels) LinearRegression() . Let’s try it out on a few instances from the training set: . # let&#39;s try the full preprocessing pipeline on a few training instances some_data = housing.iloc[:5] some_labels = housing_labels.iloc[:5] some_data_prepared = full_pipeline.transform(some_data) print(&quot;Predictions:&quot;, lin_reg.predict(some_data_prepared)) Predictions: [ 88983.14806384 305351.35385026 153334.71183453 184302.55162102 246840.18988841] print(&quot;Labels:&quot;, list(some_labels)) Labels: [72100.0, 279600.0, 82700.0, 112500.0, 238300.0] some_data_prepared.shape (5, 13) . Some remarks : If we encotered missing values are in the test set ? ( OneHotEncode() has a paramete : handle_unknown = ‘ignore’) | Indexing and selection data : if we want to modifiy a certain column in the dataframe, we should not proceed in this way : df[‘ocean_proxemity][0]= np.nan but rather copy the dataset first and then df.loc[0,’ocean_proxemity’]=np.nan. Read : https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy | . | . More on sklearn pipelines : https://scikit-learn.org/stable/auto_examples/compose/plot_column_transformer_mixed_types.html . The iloc indexer for Pandas Dataframe is used for integer-location based indexing / selection by position. | The Pandas loc indexer can be used with DataFrames for two different use cases: Selecting rows by label/index | Selecting rows with a boolean / conditional lookup | . | . Read : https://www.shanelynn.ie/pandas-iloc-loc-select-rows-and-columns-dataframe/ . Let’s measure this regression model’s RMSE on the whole training set using Scikit-Learn’s mean_squared_error() function : . from sklearn.metrics import mean_squared_error # housing_predictions = lin_reg.predict(housing_prepared) housing_predictions[:4] array([ 88983.14806384, 305351.35385026, 153334.71183453, 184302.55162102]) housing_labels 12655 72100.0 15502 279600.0 2908 82700.0 14053 112500.0 20496 238300.0 ... 15174 268500.0 12661 90400.0 19263 140400.0 19140 258100.0 19773 62700.0 Name: median_house_value, Length: 16512, dtype: float64 #RMSE lin_rmse = mean_squared_error(housing_predictions,housing_labels, squared= False) lin_rmse 69050.56219504567 . Most districts’ median_housing_values range between $120,000 and $265,000, so a typical prediction error of $69,050 is not very satisfying: . housing_labels.describe() count 16512.000000 mean 207005.322372 std 115701.297250 min 14999.000000 25% 119800.000000 50% 179500.000000 75% 263900.000000 max 500001.000000 Name: median_house_value, dtype: float64 . This is an example of a model underfitting the training data : When this happens it can mean that the features do not provide enough information to make good predictions, or that the model is not powerful enough. As we saw in the previous chapter, the main ways to fix underfitting are to select a more powerful model, to feed the training algorithm with better features, or to reduce the constraints on the model. . Let’s train a DecisionTreeRegressor. This is a powerful model, capable of finding complex nonlinear relationships in the data | . from sklearn.tree import DecisionTreeRegressor tree_reg = DecisionTreeRegressor() tree_reg.fit(housing_prepared, housing_labels) DecisionTreeRegressor() #Now that the model is trained, let’s evaluate it on the training set: housing_predictions = tree_reg.predict(housing_prepared) tree_rmse = mean_squared_error(housing_labels, housing_predictions, squared=False) tree_rmse 0.0 . As we saw earlier, we don’t want to touch the test set until we are ready to launch a model we are confident about, so we need to use part of the training set for training and part of it for model validation. . Better Evaluation Using Cross-Validation . One way to evaluate the Decision Tree model would be to use the train_test_split() function to split the training set into a smaller training set and a validation set, then train our models against the smaller training set and evaluate them against the validation set. . | A great alternative is to use Scikit-Learn’s K-fold cross-validation feature : The following code randomly splits the training set into 10 distinct subsets called folds, then it trains and evaluates the Decision Tree model 10 times, picking a different fold for evaluation every time and training on the other 9 folds. The result is an array containing the 10 evaluation scores: . | . from sklearn.model_selection import cross_val_score # to get the &#39;scoring&#39; options, use ssorted(sklearn.metrics.SCORERS.keys()) scores = - cross_val_score(tree_reg, housing_prepared, housing_labels, scoring=&#39;neg_root_mean_squared_error&#39;, cv=10) scores array([71270.15951523, 68888.32011559, 64997.85188763, 69263.03318422, 68197.14503697, 68963.98885461, 73536.17215975, 69183.4936482 , 66243.08004208, 71783.50940468]) . to get the ‘scoring’ options, use ssorted(sklearn.metrics.SCORERS.keys()) | Scikit-Learn’s cross-validation features expect a utility function (greater is better) rather than a cost function (lower is better), so the scoring function is actually the opposite of the MSE (i.e., a negative value) | . # display the results def display_scores(scores): print(&#39;Scores :&#39;,scores) print(&#39;Mean :&#39;,scores.mean()) print(&#39;Standard deviation :&#39;,scores.std()) display_scores(scores) Scores : [71270.15951523 68888.32011559 64997.85188763 69263.03318422 68197.14503697 68963.98885461 73536.17215975 69183.4936482 66243.08004208 71783.50940468] Mean : 69232.67538489516 Standard deviation : 2394.0765898258674 . Cross-validation allows you to get not only an estimate of the performance of your model, but also a measure of how precise this estimate is (i.e., its standard deviation). The Decision Tree has a score of approximately 69,232, generally ±2,394. We would not have this information if we just used one validation set. | Let’s compute the same scores for the Linear Regression model just to be sure: | . lin_scores = - cross_val_score(lin_reg,housing_prepared, housing_labels, scoring=&#39;neg_root_mean_squared_error&#39;, cv = 10) lin_scores array([72229.03469752, 65318.2240289 , 67706.39604745, 69368.53738998, 66767.61061621, 73003.75273869, 70522.24414582, 69440.77896541, 66930.32945876, 70756.31946074]) display_scores(lin_scores) Scores : [72229.03469752 65318.2240289 67706.39604745 69368.53738998 66767.61061621 73003.75273869 70522.24414582 69440.77896541 66930.32945876 70756.31946074] Mean : 69204.32275494766 Standard deviation : 2372.07079105592 . The Decision Tree model is overfitting so badly that it performs worse than the Linear Regression model. . Let’s try one last model now: the RandomForestRegressor : Random Forests work by training many Decision Trees on random subsets of the features, then averaging out their predictions. Building a model on top of many other models is called Ensemble Learning, and it is often a great way to push ML algorithms even further. . from sklearn.ensemble import RandomForestRegressor forest_reg = RandomForestRegressor() forest_reg.fit(housing_prepared, housing_labels) RandomForestRegressor() forest_reg_scores = - cross_val_score(forest_reg,housing_prepared, housing_labels, scoring=&#39;neg_root_mean_squared_error&#39;, cv = 10) forest_reg_scores array([50311.08798022, 49043.69572163, 46081.95238283, 50467.56214907, 47657.84153211, 49419.96274189, 51772.42197545, 49030.57976501, 47498.17482111, 53167.33074077]) display_scores(forest_reg_scores) Scores : [50311.08798022 49043.69572163 46081.95238283 50467.56214907 47657.84153211 49419.96274189 51772.42197545 49030.57976501 47498.17482111 53167.33074077] Mean : 49445.06098101039 Standard deviation : 1992.3842490271882 forest_predictions = forest_reg.predict(housing_prepared) forst_rmse = mean_squared_error(housing_labels, forest_predictions, squared=False) forst_rmse 18266.74368085342 . Note that the score on the training set(18,266) is still much lower than on the validation sets(49,445 +/-1992.38), meaning that the model is still overfitting the training set. . We should save every model we experiment with so that er can come back easily to any model you want. Make sure you save both the hyperparameters and the trained parameters, as well as the cross-validation scores and perhaps the actual predictions as well. This will allow you to easily compare scores across model types, and compare the types of errors they make. . | We can easily save Scikit-Learn models by using Python’s pickle module or by using the joblib library, which is more efficient at serializing large NumPy arrays (you can install this library using pip): . | . pip install joblib Requirement already satisfied: joblib in /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages (1.1.0) Note: you may need to restart the kernel to use updated packages. import joblib # random forest : joblib.dump(forest_reg,&#39;rmd_forest.pkl&#39;) # saving the model as pkl file and named &#39;rmd_forest.pkl model_reload = joblib.load(&#39;rmd_forest.pkl&#39;) # loading the model rmd_forest_prediction = model_reload.predict(housing_prepared) # saving the predictions rmd_forest_rmse = mean_squared_error(rmd_forest_prediction, housing_labels) # saving the rmse on the train test to check overfit rmd_forest_cross_validation = -cross_val_score(model_reload, housing_prepared, housing_labels, scoring=&#39;neg_root_mean_squared_error&#39;, cv = 10) # rmse on validation to check overfit display_scores(rmd_forest_cross_validation) # cross validation score Scores : [50992.51555592 49288.84220573 46237.11091931 50248.85062075 47806.3116179 49272.797347 51801.0468531 48800.83283468 47540.31917616 53091.63650718] Mean : 49508.0263637728 Standard deviation : 1972.8867918884973 . Fine-Tune Our Model . Grid Search . Using Scikit-Learn’s GridSearchCV, All we need to do is tell it which hyperparameters we want it to experiment with and what valuesto try out, and it will use cross-validation to evaluate all the possible combinations of hyperparameter values : . from sklearn.model_selection import GridSearchCV param_grid = [ {&#39;n_estimators&#39;: [3, 10, 30], &#39;max_features&#39;: [2, 4, 6, 8]}, {&#39;bootstrap&#39;: [False], &#39;n_estimators&#39;: [3, 10], &#39;max_features&#39;: [2, 3, 4]}, ] forest_reg = RandomForestRegressor() grid_search = GridSearchCV(forest_reg, param_grid, cv=5, scoring=&#39;neg_root_mean_squared_error&#39;, return_train_score = True) grid_search.fit(housing_prepared, housing_labels) GridSearchCV(cv=5, estimator=RandomForestRegressor(), param_grid=[{&#39;max_features&#39;: [2, 4, 6, 8], &#39;n_estimators&#39;: [3, 10, 30]}, {&#39;bootstrap&#39;: [False], &#39;max_features&#39;: [2, 3, 4], &#39;n_estimators&#39;: [3, 10]}], return_train_score=True, scoring=&#39;neg_root_mean_squared_error&#39;) ?GridSearchCV . This param_grid tells Scikit-Learn to first evaluate all 3 × 4 = 12 combinations of n_estimators and max_features hyperparameter values specified in the first dict. -Then try all 2 × 3 = 6 combinations of hyperparameter values in the second dict, but this time with the bootstrap hyperparameter set to False instead of True | The grid search will explore 12 + 6 = 18 combinations of `RandomForestRegressor hyperparameter values`` | And it will train each model 5 times (cv=5).In other words, all in all, there will be 18 × 5 = 90 rounds of training. | . We can get the best combination of parameters like this : . grid_search.best_params_ #we should probably try searching again with higher values; the score may continue to improve. {&#39;max_features&#39;: 8, &#39;n_estimators&#39;: 30} grid_search.best_estimator_ RandomForestRegressor(max_features=8, n_estimators=30) cvres = grid_search.cv_results_ for mean_score, params in zip(cvres[&quot;mean_test_score&quot;], cvres[&quot;params&quot;]): print(-mean_score, params) 64530.85647414619 {&#39;max_features&#39;: 2, &#39;n_estimators&#39;: 3} 55067.77832337284 {&#39;max_features&#39;: 2, &#39;n_estimators&#39;: 10} 52781.7167175866 {&#39;max_features&#39;: 2, &#39;n_estimators&#39;: 30} 60327.066875895776 {&#39;max_features&#39;: 4, &#39;n_estimators&#39;: 3} 52586.95798629394 {&#39;max_features&#39;: 4, &#39;n_estimators&#39;: 10} 50346.63948997191 {&#39;max_features&#39;: 4, &#39;n_estimators&#39;: 30} 58398.87657548992 {&#39;max_features&#39;: 6, &#39;n_estimators&#39;: 3} 52075.368300249116 {&#39;max_features&#39;: 6, &#39;n_estimators&#39;: 10} 50093.621910952315 {&#39;max_features&#39;: 6, &#39;n_estimators&#39;: 30} 58628.430409285626 {&#39;max_features&#39;: 8, &#39;n_estimators&#39;: 3} 51936.797178963665 {&#39;max_features&#39;: 8, &#39;n_estimators&#39;: 10} 50089.501357132904 {&#39;max_features&#39;: 8, &#39;n_estimators&#39;: 30} 62303.627420601595 {&#39;bootstrap&#39;: False, &#39;max_features&#39;: 2, &#39;n_estimators&#39;: 3} 54183.89357722118 {&#39;bootstrap&#39;: False, &#39;max_features&#39;: 2, &#39;n_estimators&#39;: 10} 60004.47703668058 {&#39;bootstrap&#39;: False, &#39;max_features&#39;: 3, &#39;n_estimators&#39;: 3} 52603.81684475204 {&#39;bootstrap&#39;: False, &#39;max_features&#39;: 3, &#39;n_estimators&#39;: 10} 58002.985249363366 {&#39;bootstrap&#39;: False, &#39;max_features&#39;: 4, &#39;n_estimators&#39;: 3} 52121.979634950054 {&#39;bootstrap&#39;: False, &#39;max_features&#39;: 4, &#39;n_estimators&#39;: 10} . Rq: Don’t forget that we can treat some of the data preparation steps as hyperparameters. The grid search will automatically find out whether or not to add a feature you were not sure about. Ex : using the add_bedrooms_per_room hyperparameter of your CombinedAttributesAdder transformer). . .",
            "url": "https://younesszaim.github.io/myportfolio/2021/11/15/Sickit-Learn-for-Machine-Learning.html",
            "relUrl": "/2021/11/15/Sickit-Learn-for-Machine-Learning.html",
            "date": " • Nov 15, 2021"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Hi ! I’m Youness ZAIM. . Please check my resume on Linkedin. . You can also email me on zaimyouness9797@gmail.com. .",
          "url": "https://younesszaim.github.io/myportfolio/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://younesszaim.github.io/myportfolio/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}