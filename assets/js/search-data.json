{
  
    
        "post0": {
            "title": "Voting Classifier",
            "content": "from sklearn.metrics import r2_score # r2-score : variance explained by the model r2_score(y, y_pred) . import numpy as np . from sklearn.metrics import mean_squared_error # MSE mean_squared_error([1], [2], squared = False) . 1.0 . from sklearn.metrics import mean_squared_log_error #MSLE mean_squared_log_error([1],[0.1], squared = False) . 0.5978370007556204 . from sklearn.metrics import mean_absolute_error #MAE . from sklearn.metrics import r2_score # r2 from sklearn.metrics import mean_squared_error # MSE (RMSE : mean_squared_error(y,y_pred, squared = False)) from sklearn.metrics import mean_squared_log_error # MSLE (RMSLE: mean_squared_log_error(y, y_pred, squared = False)) from sklearn.metrics import mean_absolute_error #MAE . PATH = &#39;/Users/rmbp/handson-ml2/datasets/&#39; !ls {PATH} . housing inception jsb_chorales lifesat titanic . import pandas as pd housing = pd.read_csv(f&#39;{PATH}/housing/housing.csv&#39;) housing.head() . longitude latitude housing_median_age total_rooms total_bedrooms population households median_income median_house_value ocean_proximity . 0 -122.23 | 37.88 | 41.0 | 880.0 | 129.0 | 322.0 | 126.0 | 8.3252 | 452600.0 | NEAR BAY | . 1 -122.22 | 37.86 | 21.0 | 7099.0 | 1106.0 | 2401.0 | 1138.0 | 8.3014 | 358500.0 | NEAR BAY | . 2 -122.24 | 37.85 | 52.0 | 1467.0 | 190.0 | 496.0 | 177.0 | 7.2574 | 352100.0 | NEAR BAY | . 3 -122.25 | 37.85 | 52.0 | 1274.0 | 235.0 | 558.0 | 219.0 | 5.6431 | 341300.0 | NEAR BAY | . 4 -122.25 | 37.85 | 52.0 | 1627.0 | 280.0 | 565.0 | 259.0 | 3.8462 | 342200.0 | NEAR BAY | . housing.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 20640 entries, 0 to 20639 Data columns (total 10 columns): # Column Non-Null Count Dtype -- -- 0 longitude 20640 non-null float64 1 latitude 20640 non-null float64 2 housing_median_age 20640 non-null float64 3 total_rooms 20640 non-null float64 4 total_bedrooms 20433 non-null float64 5 population 20640 non-null float64 6 households 20640 non-null float64 7 median_income 20640 non-null float64 8 median_house_value 20640 non-null float64 9 ocean_proximity 20640 non-null object dtypes: float64(9), object(1) memory usage: 1.6+ MB . housing[&#39;ocean_proximity&#39;].value_counts() . &lt;1H OCEAN 9136 INLAND 6551 NEAR OCEAN 2658 NEAR BAY 2290 ISLAND 5 Name: ocean_proximity, dtype: int64 . housing.describe().T . count mean std min 25% 50% 75% max . longitude 20640.0 | -119.569704 | 2.003532 | -124.3500 | -121.8000 | -118.4900 | -118.01000 | -114.3100 | . latitude 20640.0 | 35.631861 | 2.135952 | 32.5400 | 33.9300 | 34.2600 | 37.71000 | 41.9500 | . housing_median_age 20640.0 | 28.639486 | 12.585558 | 1.0000 | 18.0000 | 29.0000 | 37.00000 | 52.0000 | . total_rooms 20640.0 | 2635.763081 | 2181.615252 | 2.0000 | 1447.7500 | 2127.0000 | 3148.00000 | 39320.0000 | . total_bedrooms 20433.0 | 537.870553 | 421.385070 | 1.0000 | 296.0000 | 435.0000 | 647.00000 | 6445.0000 | . population 20640.0 | 1425.476744 | 1132.462122 | 3.0000 | 787.0000 | 1166.0000 | 1725.00000 | 35682.0000 | . households 20640.0 | 499.539680 | 382.329753 | 1.0000 | 280.0000 | 409.0000 | 605.00000 | 6082.0000 | . median_income 20640.0 | 3.870671 | 1.899822 | 0.4999 | 2.5634 | 3.5348 | 4.74325 | 15.0001 | . median_house_value 20640.0 | 206855.816909 | 115395.615874 | 14999.0000 | 119600.0000 | 179700.0000 | 264725.00000 | 500001.0000 | . %matplotlib inline import matplotlib.pyplot as plt housing.hist(bins = 60, figsize=(15,10)) plt.show() . housing[&#39;ocean_proximity&#39;].value_counts().plot(kind=&#39;bar&#39;, color = &#39;blue&#39; ,edgecolor = &#39;black&#39;) . &lt;AxesSubplot:&gt; . housing[&#39;median_house_value&#39;].describe() . count 20640.000000 mean 206855.816909 std 115395.615874 min 14999.000000 25% 119600.000000 50% 179700.000000 75% 264725.000000 max 500001.000000 Name: median_house_value, dtype: float64 . housing[&#39;median_house_value&#39;].plot(kind=&#39;hist&#39;, bins = int((500001 - 14999)/10000), color = &#39;blue&#39;, edgecolor = &#39;black&#39;) . &lt;AxesSubplot:ylabel=&#39;Frequency&#39;&gt; . housing[&#39;median_income&#39;].plot(kind=&#39;hist&#39;, bins = 60, color = &#39;blue&#39;, edgecolor = &#39;black&#39;) . &lt;AxesSubplot:ylabel=&#39;Frequency&#39;&gt; . housing[&#39;median_income&#39;].describe() . count 20640.000000 mean 3.870671 std 1.899822 min 0.499900 25% 2.563400 50% 3.534800 75% 4.743250 max 15.000100 Name: median_income, dtype: float64 . housing[&#39;income_cat&#39;]= pd.cut(housing[&#39;median_income&#39;], bins = [0.,1.5,3.0,4.5,6.,np.inf], labels = [1,2,3,4,5]) . housing[&#39;income_cat&#39;].value_counts().plot(kind=&#39;bar&#39;) . &lt;AxesSubplot:&gt; . housing.dtypes . longitude float64 latitude float64 housing_median_age float64 total_rooms float64 total_bedrooms float64 population float64 households float64 median_income float64 median_house_value float64 ocean_proximity object income_cat category dtype: object . from sklearn.model_selection import train_test_split strat_train_set, strat_test_set = train_test_split(housing, test_size= 0.2, stratify= housing[&#39;income_cat&#39;], random_state = 1997) . housing[&quot;income_cat&quot;].value_counts() / len(housing) . 3 0.350581 2 0.318847 4 0.176308 5 0.114438 1 0.039826 Name: income_cat, dtype: float64 . strat_train_set[&quot;income_cat&quot;].value_counts() / len(strat_train_set) . 3 0.350594 2 0.318859 4 0.176296 5 0.114462 1 0.039789 Name: income_cat, dtype: float64 . strat_test_set[&quot;income_cat&quot;].value_counts() / len(strat_test_set) . 3 0.350533 2 0.318798 4 0.176357 5 0.114341 1 0.039971 Name: income_cat, dtype: float64 . for l in (strat_train_set,strat_test_set): l.drop(&#39;income_cat&#39;, axis= 1, inplace= True) . strat_train_set.columns . Index([&#39;longitude&#39;, &#39;latitude&#39;, &#39;housing_median_age&#39;, &#39;total_rooms&#39;, &#39;total_bedrooms&#39;, &#39;population&#39;, &#39;households&#39;, &#39;median_income&#39;, &#39;median_house_value&#39;, &#39;ocean_proximity&#39;], dtype=&#39;object&#39;) . housing = strat_train_set.copy() . housing.plot(kind=&#39;scatter&#39;, x=&#39;longitude&#39;, y= &#39;latitude&#39;, alpha=0.3, s=housing[&#39;population&#39;]/100, label=&#39;population&#39;,figsize=(10,7), c=&#39;median_house_value&#39;, cmap=plt.get_cmap(&quot;jet&quot;), colorbar=True) . &lt;AxesSubplot:xlabel=&#39;longitude&#39;, ylabel=&#39;latitude&#39;&gt; . corr= housing.corr() corr[&#39;median_house_value&#39;].sort_values(ascending = False) . median_house_value 1.000000 median_income 0.687649 total_rooms 0.131648 housing_median_age 0.107866 households 0.065199 total_bedrooms 0.049777 population -0.023809 longitude -0.046290 latitude -0.143355 Name: median_house_value, dtype: float64 . from pandas.plotting import scatter_matrix attributes = [&#39;median_house_value&#39;, &#39;median_income&#39;, &#39;total_rooms&#39;, &#39;housing_median_age&#39; ] scatter_matrix(housing[attributes],figsize=(10,6)) . array([[&lt;AxesSubplot:xlabel=&#39;median_house_value&#39;, ylabel=&#39;median_house_value&#39;&gt;, &lt;AxesSubplot:xlabel=&#39;median_income&#39;, ylabel=&#39;median_house_value&#39;&gt;, &lt;AxesSubplot:xlabel=&#39;total_rooms&#39;, ylabel=&#39;median_house_value&#39;&gt;, &lt;AxesSubplot:xlabel=&#39;housing_median_age&#39;, ylabel=&#39;median_house_value&#39;&gt;], [&lt;AxesSubplot:xlabel=&#39;median_house_value&#39;, ylabel=&#39;median_income&#39;&gt;, &lt;AxesSubplot:xlabel=&#39;median_income&#39;, ylabel=&#39;median_income&#39;&gt;, &lt;AxesSubplot:xlabel=&#39;total_rooms&#39;, ylabel=&#39;median_income&#39;&gt;, &lt;AxesSubplot:xlabel=&#39;housing_median_age&#39;, ylabel=&#39;median_income&#39;&gt;], [&lt;AxesSubplot:xlabel=&#39;median_house_value&#39;, ylabel=&#39;total_rooms&#39;&gt;, &lt;AxesSubplot:xlabel=&#39;median_income&#39;, ylabel=&#39;total_rooms&#39;&gt;, &lt;AxesSubplot:xlabel=&#39;total_rooms&#39;, ylabel=&#39;total_rooms&#39;&gt;, &lt;AxesSubplot:xlabel=&#39;housing_median_age&#39;, ylabel=&#39;total_rooms&#39;&gt;], [&lt;AxesSubplot:xlabel=&#39;median_house_value&#39;, ylabel=&#39;housing_median_age&#39;&gt;, &lt;AxesSubplot:xlabel=&#39;median_income&#39;, ylabel=&#39;housing_median_age&#39;&gt;, &lt;AxesSubplot:xlabel=&#39;total_rooms&#39;, ylabel=&#39;housing_median_age&#39;&gt;, &lt;AxesSubplot:xlabel=&#39;housing_median_age&#39;, ylabel=&#39;housing_median_age&#39;&gt;]], dtype=object) . housing.plot(kind= &#39;scatter&#39;, x= &#39;median_income&#39;, y= &#39;median_house_value&#39;, alpha=0.1, ) . &lt;AxesSubplot:xlabel=&#39;median_income&#39;, ylabel=&#39;median_house_value&#39;&gt; . housing.columns . Index([&#39;longitude&#39;, &#39;latitude&#39;, &#39;housing_median_age&#39;, &#39;total_rooms&#39;, &#39;total_bedrooms&#39;, &#39;population&#39;, &#39;households&#39;, &#39;median_income&#39;, &#39;median_house_value&#39;, &#39;ocean_proximity&#39;], dtype=&#39;object&#39;) . coor= housing[[&#39;total_rooms&#39;,&#39;total_bedrooms&#39;,&#39;households&#39;,&#39;population&#39;,&#39;housing_median_age&#39;,&#39;median_income&#39;]].corr() . import seaborn as sns sns.heatmap(coor, cmap=&#39;Blues&#39;, annot=True) . &lt;AxesSubplot:&gt; . housing[&quot;rooms_per_household&quot;] = housing[&quot;total_rooms&quot;]/housing[&quot;households&quot;] housing[&quot;bedrooms_per_room&quot;] = housing[&quot;total_bedrooms&quot;]/housing[&quot;total_rooms&quot;] housing[&quot;population_per_household&quot;]=housing[&quot;population&quot;]/housing[&quot;households&quot;] . housing.corr()[&#39;median_house_value&#39;].sort_values(ascending= False) . median_house_value 1.000000 median_income 0.687649 rooms_per_household 0.150955 total_rooms 0.131648 housing_median_age 0.107866 households 0.065199 total_bedrooms 0.049777 population -0.023809 population_per_household -0.033130 longitude -0.046290 latitude -0.143355 bedrooms_per_room -0.252928 Name: median_house_value, dtype: float64 . housing = strat_train_set.drop(&#39;median_house_value&#39;, axis=1) housing_labels = strat_train_set[&#39;median_house_value&#39;].copy() . housing[&#39;total_bedrooms&#39;].describe() . count 16345.000000 mean 538.401958 std 427.323561 min 2.000000 25% 294.000000 50% 434.000000 75% 647.000000 max 6445.000000 Name: total_bedrooms, dtype: float64 . housing[&#39;total_bedrooms&#39;].plot(kind = &#39;hist&#39; , bins = int ((6210.000000 - 2.000000 )/500) , color = &#39;blue&#39; , edgecolor = &#39;black&#39; ) . &lt;AxesSubplot:ylabel=&#39;Frequency&#39;&gt; . housing.isna().sum()/len(housing) . longitude 0.000000 latitude 0.000000 housing_median_age 0.000000 total_rooms 0.000000 total_bedrooms 0.010114 population 0.000000 households 0.000000 median_income 0.000000 ocean_proximity 0.000000 dtype: float64 . housing.shape . (16512, 9) . housing.dropna(subset=[&#39;total_bedrooms&#39;]).shape . (16345, 9) . housing.dropna().shape . (16345, 9) . housing.drop(&#39;total_bedrooms&#39;,axis=1).shape . (16512, 8) . median= housing[&#39;total_bedrooms&#39;].median() housing.fillna(median, inplace= True) . housing.isna().sum() . longitude 0 latitude 0 housing_median_age 0 total_rooms 0 total_bedrooms 0 population 0 households 0 median_income 0 ocean_proximity 0 dtype: int64 . from sklearn.impute import SimpleImputer imputer = SimpleImputer( strategy = &#39;median&#39;) . housing.dtypes . longitude float64 latitude float64 housing_median_age float64 total_rooms float64 total_bedrooms float64 population float64 households float64 median_income float64 ocean_proximity object dtype: object . housing_num = housing.drop(&#39;ocean_proximity&#39;, axis=1) . housing_num.dtypes . longitude float64 latitude float64 housing_median_age float64 total_rooms float64 total_bedrooms float64 population float64 households float64 median_income float64 dtype: object . housing_num.shape . (16512, 8) . X = imputer.fit_transform(housinf_num) housing_num_tr = pd.DataFrame(X, columns = housing_num.columns, index= housing_num.index) . X.shape . (16512, 8) . housing_num_tr.shape . (8, 16512) . imputer.statistics_.shape . (8,) . pd.DataFrame(imputer.statistics_, index = housing_num.columns, ) . 0 . longitude -118.49000 | . latitude 34.26000 | . housing_median_age 29.00000 | . total_rooms 2125.00000 | . total_bedrooms 434.00000 | . population 1166.00000 | . households 409.00000 | . median_income 3.54025 | . from sklearn.impute import SimpleImputer imputer = SimpleImputer(strategy = &#39;median&#39;) imputer.fit_transform() . housing[&#39;ocean_proximity&#39;].value_counts() . &lt;1H OCEAN 7310 INLAND 5237 NEAR OCEAN 2141 NEAR BAY 1820 ISLAND 4 Name: ocean_proximity, dtype: int64 . housing_cat = housing[[&#39;ocean_proximity&#39;]] . ocean_proximity . 12307 INLAND | . 7410 &lt;1H OCEAN | . 11287 &lt;1H OCEAN | . 5230 &lt;1H OCEAN | . 1362 INLAND | . ... ... | . 6434 INLAND | . 14235 NEAR OCEAN | . 17333 &lt;1H OCEAN | . 8161 &lt;1H OCEAN | . 7746 &lt;1H OCEAN | . 16512 rows × 1 columns . housing[&#39;ocean_proximity&#39;] . 12307 INLAND 7410 &lt;1H OCEAN 11287 &lt;1H OCEAN 5230 &lt;1H OCEAN 1362 INLAND ... 6434 INLAND 14235 NEAR OCEAN 17333 &lt;1H OCEAN 8161 &lt;1H OCEAN 7746 &lt;1H OCEAN Name: ocean_proximity, Length: 16512, dtype: object . from sklearn.preprocessing import OrdinalEncoder housing_cat = housing[[&#39;ocean_proximity&#39;]] ordinal_encoder = OrdinalEncoder() housing_cat_encoded = ordinal_encoder.fit_transform(housing_cat) . ordinal_encoder.categories_ . [array([&#39;&lt;1H OCEAN&#39;, &#39;INLAND&#39;, &#39;ISLAND&#39;, &#39;NEAR BAY&#39;, &#39;NEAR OCEAN&#39;], dtype=object)] . housing_cat_encoded.shape . (16512, 1) . np.unique(housing_cat_encoded) . array([0., 1., 2., 3., 4.]) . from sklearn.preprocessing import OrdinalEncoder housing_cat = housing[[&#39;ocean_proximity&#39;]] ordinal_encoder = OrdinalEncoder() housing_cat_encoded = ordinal_encoder.fit_transform(housing_cat) . print(ordinal_encoder.categories_) print(np.unique(housing_cat_encoded)) . [array([&#39;&lt;1H OCEAN&#39;, &#39;INLAND&#39;, &#39;ISLAND&#39;, &#39;NEAR BAY&#39;, &#39;NEAR OCEAN&#39;], dtype=object)] [0. 1. 2. 3. 4.] . from sklearn.preprocessing import OneHotEncoder one_hot_encoder = OneHotEncoder() housing_cat=housing[[&#39;ocean_proximity&#39;]] housing_cat_encoded = one_hot_encoder.fit_transform(housing_cat) . print(housing_cat.shape) print(housing_cat_encoded.shape) . (16512, 1) (16512, 5) . one_hot_encoder.categories_ . [array([&#39;&lt;1H OCEAN&#39;, &#39;INLAND&#39;, &#39;ISLAND&#39;, &#39;NEAR BAY&#39;, &#39;NEAR OCEAN&#39;], dtype=object)] . housing_cat.head(3) . ocean_proximity . 12307 INLAND | . 7410 &lt;1H OCEAN | . 11287 &lt;1H OCEAN | . housing_cat_encoded.toarray() . array([[0., 1., 0., 0., 0.], [1., 0., 0., 0., 0.], [1., 0., 0., 0., 0.], ..., [1., 0., 0., 0., 0.], [1., 0., 0., 0., 0.], [1., 0., 0., 0., 0.]]) . housing.describe().T . count mean std min 25% 50% 75% max . longitude 16512.0 | -119.572200 | 2.007571 | -124.3500 | -121.8000 | -118.49000 | -118.0100 | -114.3100 | . latitude 16512.0 | 35.633831 | 2.139549 | 32.5500 | 33.9400 | 34.26000 | 37.7200 | 41.9500 | . housing_median_age 16512.0 | 28.682231 | 12.591242 | 1.0000 | 18.0000 | 29.00000 | 37.0000 | 52.0000 | . total_rooms 16512.0 | 2643.583091 | 2228.841612 | 2.0000 | 1437.0000 | 2125.00000 | 3155.0000 | 39320.0000 | . total_bedrooms 16512.0 | 537.346051 | 425.285315 | 2.0000 | 295.0000 | 434.00000 | 643.0000 | 6445.0000 | . population 16512.0 | 1428.494065 | 1154.121718 | 3.0000 | 783.0000 | 1166.00000 | 1729.0000 | 35682.0000 | . households 16512.0 | 500.206940 | 386.165379 | 2.0000 | 278.0000 | 409.00000 | 606.0000 | 6082.0000 | . median_income 16512.0 | 3.873652 | 1.901844 | 0.4999 | 2.5634 | 3.54025 | 4.7426 | 15.0001 | . # option 2 : standard scaling . from sklearn.compose import ColumnTransformer from sklearn.pipeline import Pipeline from sklearn.preprocessing import OneHotEncoder, StandardScaler from sklearn.impute import SimpleImputer . housing.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; Int64Index: 16512 entries, 12307 to 7746 Data columns (total 9 columns): # Column Non-Null Count Dtype -- -- 0 longitude 16512 non-null float64 1 latitude 16512 non-null float64 2 housing_median_age 16512 non-null float64 3 total_rooms 16512 non-null float64 4 total_bedrooms 16512 non-null float64 5 population 16512 non-null float64 6 households 16512 non-null float64 7 median_income 16512 non-null float64 8 ocean_proximity 16512 non-null category dtypes: category(1), float64(8) memory usage: 1.7 MB . num_attribs = list(housing.drop(&#39;ocean_proximity&#39;,axis=1)) cat_attribs = [&#39;ocean_proximity&#39;] . num_pipeline = Pipeline( [(&#39;imputer&#39;, SimpleImputer(strategy=&#39;median&#39;)), (&#39;std_scaler&#39;, StandardScaler()) ] ) . full_pipeline = ColumnTransformer([(&#39;num&#39;,num_pipeline,num_attribs), (&#39;cat&#39;,OneHotEncoder(), cat_attribs)]) . full_pipeline . &lt;div id=&quot;sk-190c0162-b934-4f07-9edd-4aba295d5a1e&quot; class&quot;sk-top-container&quot;&gt;ColumnTransformerColumnTransformer(transformers=[(&#39;num&#39;, Pipeline(steps=[(&#39;imputer&#39;, SimpleImputer(strategy=&#39;median&#39;)), (&#39;std_scaler&#39;, StandardScaler())]), [&#39;longitude&#39;, &#39;latitude&#39;, &#39;housing_median_age&#39;, &#39;total_rooms&#39;, &#39;total_bedrooms&#39;, &#39;population&#39;, &#39;households&#39;, &#39;median_income&#39;]), (&#39;cat&#39;, OneHotEncoder(), [&#39;ocean_proximity&#39;])]) . num[&#39;longitude&#39;, &#39;latitude&#39;, &#39;housing_median_age&#39;, &#39;total_rooms&#39;, &#39;total_bedrooms&#39;, &#39;population&#39;, &#39;households&#39;, &#39;median_income&#39;] . SimpleImputerSimpleImputer(strategy=&#39;median&#39;) . StandardScalerStandardScaler() . cat[&#39;ocean_proximity&#39;] . OneHotEncoderOneHotEncoder() . housing_prepared = full_pipeline.fit_transform(housing) . housing_prepared[0] . array([ 1.41080207, -0.94126926, -1.40437031, -0.11602032, 0.29076408, -0.64682664, -0.66866367, -0.50455296, 0. , 1. , 0. , 0. , 0. ]) . from sklearn import set_config set_config(display=&quot;diagram&quot;) full_pipeline . &lt;div id=&quot;sk-8661009f-3823-43d3-aa39-875ed02df899&quot; class&quot;sk-top-container&quot;&gt;ColumnTransformerColumnTransformer(transformers=[(&#39;num&#39;, Pipeline(steps=[(&#39;imputer&#39;, SimpleImputer(strategy=&#39;median&#39;)), (&#39;std_scaler&#39;, StandardScaler())]), [&#39;longitude&#39;, &#39;latitude&#39;, &#39;housing_median_age&#39;, &#39;total_rooms&#39;, &#39;total_bedrooms&#39;, &#39;population&#39;, &#39;households&#39;, &#39;median_income&#39;]), (&#39;cat&#39;, OneHotEncoder(), [&#39;ocean_proximity&#39;])]) . num[&#39;longitude&#39;, &#39;latitude&#39;, &#39;housing_median_age&#39;, &#39;total_rooms&#39;, &#39;total_bedrooms&#39;, &#39;population&#39;, &#39;households&#39;, &#39;median_income&#39;] . SimpleImputerSimpleImputer(strategy=&#39;median&#39;) . StandardScalerStandardScaler() . cat[&#39;ocean_proximity&#39;] . OneHotEncoderOneHotEncoder() . &lt;/div&gt; . housing_extra_attribs = pd.DataFrame( housing_prepared, columns=num_attribs+[&quot;var_1&quot;,&quot;var_2&quot;,&quot;var_3&quot;,&quot;var_4&quot;,&quot;var_5&quot;], index=housing.index) housing_extra_attribs.head() . longitude latitude housing_median_age total_rooms total_bedrooms population households median_income var_1 var_2 var_3 var_4 var_5 . 12307 1.410802 | -0.941269 | -1.404370 | -0.116020 | 0.290764 | -0.646827 | -0.668664 | -0.504553 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | . 7410 0.678552 | -0.787026 | 0.740042 | -0.338564 | 0.065026 | 0.570586 | 0.152253 | -1.102044 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 11287 0.808065 | -0.857137 | 0.422351 | -0.444003 | -0.593375 | -0.453600 | -0.539182 | 0.402384 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 5230 0.663608 | -0.796375 | 0.660620 | -0.725324 | -0.656864 | -0.523786 | -0.653126 | -1.241176 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 1362 -1.209359 | 1.119975 | -1.960329 | -0.135762 | -0.304149 | -0.167660 | -0.179221 | 1.140374 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | . from sklearn.linear_model import LinearRegression lin_reg = LinearRegression() lin_reg.fit(housing_prepared, housing_labels) . &lt;div id=&quot;sk-f2b96319-185c-494f-bc94-d86537d1c4e4&quot; class&quot;sk-top-container&quot;&gt;LinearRegressionLinearRegression() . &lt;/div&gt; . some_data = housing.iloc[:5] some_data_label = housing_labels.iloc[:5] some_data_prepared = full_pipeline.transform(some_data) some_data_prepared[0] . array([ 1.41080207, -0.94126926, -1.40437031, -0.11602032, 0.29076408, -0.64682664, -0.66866367, -0.50455296, 0. , 1. , 0. , 0. , 0. ]) . some_data = housing.iloc[:5] some_data_label = housing_labels.iloc[:5] some_data_prepared = full_pipeline.transform(some_data) some_data_prepared[0] . array([ 1.41080207, -0.94126926, -1.40437031, -0.11602032, 0.29076408, -0.64682664, -0.66866367, -0.50455296, 0. , 1. , 0. , 0. , 0. ]) . some_data = housing.iloc[:5] some_labels = housing_labels.iloc[:5] some_data_prepared = full_pipeline.transform(some_data) print(&quot;Predictions:&quot;, lin_reg.predict(some_data_prepared)) . Predictions: [120059.12889448 139886.68146423 248726.27344698 136271.01467458 236318.99250576] . import numpy as np . array([[1, 2, 3], [4, 5, 6]]) . n, m = input().split() . 2 2 . [input().strip().split() for _ in range(2)] . 1 2 3 4 1 2 3 4 . [[&#39;1&#39;, &#39;2&#39;, &#39;3&#39;, &#39;4&#39;], [&#39;1&#39;, &#39;2&#39;, &#39;3&#39;, &#39;4&#39;]] . array_1 = np.array([[1,2,3],[0,0,0]]) array_2 = np.array([[0,0,0],[7,8,9]]) print( np.concatenate((array_1, array_2), axis = 1) ) . [[1 2 3 0 0 0] [0 0 0 7 8 9]] . np.zeros((1,2),int) . array([[0, 0]]) . np.zeros((1,2,1),int) . array([[[0], [0]]]) . np.zeros((1,2)) . array([[0., 0.]]) . n = int(input()) . 3 . from collections import Counter lst = [1,1,2,3,4,5,3,2,3,4,2,1,2,3] print(Counter(lst)) . Counter({2: 4, 3: 4, 1: 3, 4: 2, 5: 1}) . print (Counter(lst).items()) . dict_items([(1, 3), (2, 4), (3, 4), (4, 2), (5, 1)]) . Counter(lst).values() . dict_values([3, 4, 4, 2, 1]) . a = Counter(lst) dict(a) . {1: 3, 2: 4, 3: 4, 4: 2, 5: 1} . a.keys() . dict_keys([1, 2, 3, 4, 5]) . len(housing) . 16512 . housing[&#39;median_income&#39;].plot( kind=&#39;hist&#39;, bins = int(1+3.3*np.log(16512))) . &lt;AxesSubplot:ylabel=&#39;Frequency&#39;&gt; . attributes . [&#39;median_house_value&#39;, &#39;median_income&#39;, &#39;total_rooms&#39;, &#39;housing_median_age&#39;] . from sklearn.datasets import make_moons . X, y = make_moons(n_samples=500, noise=0.30, random_state=42) . from sklearn.model_selection import train_test_split . X_train,X_test, y_train, y_test = train_test_split (X, y, random_state=42) . from sklearn.ensemble import RandomForestClassifier from sklearn.linear_model import LogisticRegression from sklearn.svm import SVC . from sklearn.ensemble import VotingClassifier . Decision tree parameters : https://medium.com/@mohtedibf/indepth-parameter-tuning-for-decision-tree-6753118a03c3 | random forest : https://medium.com/all-things-ai/in-depth-parameter-tuning-for-random-forest-d67bb7e920d | SVC : https://medium.com/all-things-ai/in-depth-parameter-tuning-for-svc-758215394769 | Gradiant Boosting : https://medium.com/all-things-ai/in-depth-parameter-tuning-for-gradient-boosting-3363992e9bae | KNN : https://medium.com/@mohtedibf/in-depth-parameter-tuning-for-knn-4c0de485baf6 | . log_clf = LogisticRegression(solver=&quot;lbfgs&quot;, random_state=42) rnd_clf = RandomForestClassifier(n_estimators=100, random_state=42) svm_clf = SVC(gamma=&quot;scale&quot;, random_state=42) . voting_clf = VotingClassifier( estimators = [(&#39;lr&#39;,log_clf ),(&#39;rf&#39;,rnd_clf),(&#39;svc&#39;,svm_clf)], verbose = True ) . voting_clf.fit(X_train, y_train) . [Voting] ....................... (1 of 3) Processing lr, total= 0.0s [Voting] ....................... (2 of 3) Processing rf, total= 0.2s [Voting] ...................... (3 of 3) Processing svc, total= 0.0s . VotingClassifier(estimators=[(&#39;lr&#39;, LogisticRegression(random_state=42)), (&#39;rf&#39;, RandomForestClassifier(random_state=42)), (&#39;svc&#39;, SVC(random_state=42))], verbose=True) . from sklearn.metrics import accuracy_score, log_loss . Metrics for classification Evaluation : https://towardsdatascience.com/the-5-classification-evaluation-metrics-you-must-know-aa97784ff226 | . for clf in (log_clf, rnd_clf, svm_clf,voting_clf ): clf.fit(X_train, y_train) y_pred = clf.predict(X_test) print(clf.__class__.__name__, accuracy_score(y_test, y_pred),log_loss(y_test, y_pred)) . LogisticRegression 0.864 4.697305573605061 RandomForestClassifier 0.896 3.592051935409036 SVC 0.896 3.5920391418501527 [Voting] ....................... (1 of 3) Processing lr, total= 0.0s [Voting] ....................... (2 of 3) Processing rf, total= 0.2s [Voting] ...................... (3 of 3) Processing svc, total= 0.0s VotingClassifier 0.912 3.039425116311024 . rnd_clf.fit(X_train, y_train) . RandomForestClassifier(random_state=42) . svm_clf_soft = SVC(gamma=&quot;scale&quot;, probability = True ,random_state=42) . voting_clf_soft = VotingClassifier( estimators = [(&#39;lr&#39;,log_clf ),(&#39;rf&#39;,rnd_clf),(&#39;svc&#39;,svm_clf_soft)], voting = &#39;soft&#39; ) . for clf in (log_clf, rnd_clf, svm_clf_soft,voting_clf_soft ): clf.fit(X_train, y_train) y_pred = clf.predict(X_test) print(clf.__class__.__name__, accuracy_score(y_test, y_pred),f1_score(y_test, y_pred)) . LogisticRegression 0.864 0.859504132231405 RandomForestClassifier 0.896 0.8925619834710744 SVC 0.896 0.8888888888888888 VotingClassifier 0.92 0.9180327868852458 . Bagging and pasting . Bias vs variance : https://towardsdatascience.com/understanding-the-bias-variance-tradeoff-165e6942b229 . from sklearn.ensemble import BaggingClassifier from sklearn.tree import DecisionTreeClassifier . bag_clf = BaggingClassifier(DecisionTreeClassifier(),n_estimators=500, max_samples=100, bootstrap=True, n_jobs=-1) . %%time bag_clf.fit(X_train, y_train) . CPU times: user 58.9 ms, sys: 6.87 ms, total: 65.8 ms Wall time: 646 ms . BaggingClassifier(base_estimator=DecisionTreeClassifier(), max_samples=100, n_estimators=500, n_jobs=-1) . y_pred = bag_clf.predict(X_test) . from sklearn.metrics import accuracy_score, f1_score . print(accuracy_score(y_pred,y_test),f1_score(y_pred,y_test)) . 0.912 0.9105691056910569 . %%time tree_clf = DecisionTreeClassifier(random_state=42) tree_clf.fit(X_train,y_train) y_pred = tree_clf.predict(X_test) print(accuracy_score(y_pred,y_test),f1_score(y_pred,y_test)) . 0.856 0.847457627118644 CPU times: user 4.91 ms, sys: 2.56 ms, total: 7.47 ms Wall time: 4.85 ms . Out-of-Bag Evaluation . bag_clf = BaggingClassifier(DecisionTreeClassifier(),n_estimators=500,max_samples=200, bootstrap=True,n_jobs=-1, oob_score=True) . bag_clf.fit(X_train,y_train) . BaggingClassifier(base_estimator=DecisionTreeClassifier(), max_samples=200, n_estimators=500, n_jobs=-1, oob_score=True) . bag_clf.oob_score_ . 0.928 . According to this oob evaluation, this BaggingClassifier is likely to achieve about 90.1% accuracy on the test set. . The OOB_score is computed as the number of correctly predicted rows from the out-of-bag sample . print(accuracy_score(y_test,bag_clf.predict(X_test))) . 0.904 . Random forest . from sklearn.ensemble import RandomForestClassifier . rnd_clf = RandomForestClassifier(n_estimators = 500, max_leaf_nodes=16, n_jobs=-1) . rnd_clf.fit(X_train, y_train) . RandomForestClassifier(max_leaf_nodes=16, n_estimators=500, n_jobs=-1) . print(f1_score(y_test,rnd_clf.predict(X_test))) . 0.9090909090909092 . rnd_clf2 = RandomForestClassifier(n_estimators = 500, max_leaf_nodes=16, n_jobs=-1, bootstrap=True, max_samples=100,oob_score=True) . rnd_clf2.fit(X_train, y_train) . RandomForestClassifier(max_leaf_nodes=16, max_samples=100, n_estimators=500, n_jobs=-1, oob_score=True) . rnd_clf2.oob_score_ . 0.9226666666666666 . print(f1_score(y_test,rnd_clf2.predict(X_test))) . 0.9180327868852458 . bag_clf = BaggingClassifier(DecisionTreeClassifier(max_leaf_nodes=16, splitter=&#39;random&#39;), n_estimators=500, max_samples = 1.0, bootstrap = False) . bag_clf.fit(X_train,y_train) . BaggingClassifier(base_estimator=DecisionTreeClassifier(max_leaf_nodes=16, splitter=&#39;random&#39;), bootstrap=False, n_estimators=500) . print(f1_score(y_test,bag_clf.predict(X_test))) . 0.9090909090909092 . Extra Tree Classifier . from sklearn.ensemble import ExtraTreesClassifier . extra_clf = ExtraTreesClassifier(n_estimators = 500, max_leaf_nodes=16, n_jobs=-1) . extra_clf.fit(X_train,y_train) . ExtraTreesClassifier(max_leaf_nodes=16, n_estimators=500, n_jobs=-1) . print(f1_score(y_test,extra_clf.predict(X_test))) . 0.9090909090909092 . Feature importance in Random Forest . from sklearn.datasets import load_iris . iris= load_iris() . rnd_clf = RandomForestClassifier(n_estimators=500, n_jobs=-1) . rnd_clf.fit(iris[&#39;data&#39;],iris[&#39;target&#39;]) . RandomForestClassifier(n_estimators=500, n_jobs=-1) . for name, score in zip(iris[&#39;feature_names&#39;],rnd_clf.feature_importances_): print(name,score) . sepal length (cm) 0.11032689263447301 sepal width (cm) 0.02807946651107976 petal length (cm) 0.4255296147650561 petal width (cm) 0.4360640260893911 . Adaboost Classifier . from sklearn.ensemble import AdaBoostClassifier . ada_clf = AdaBoostClassifier(DecisionTreeClassifier(max_depth=1), n_estimators=300, algorithm=&#39;SAMME.R&#39;, learning_rate=0.5) . ada_clf.fit(X_train,y_train) . AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=1), learning_rate=0.5, n_estimators=300) . print(f1_score(y_test,ada_clf.predict(X_test))) . 0.8943089430894309 . &lt;/div&gt; .",
            "url": "https://younesszaim.github.io/myportfolio/2021/12/20/Untitled.html",
            "relUrl": "/2021/12/20/Untitled.html",
            "date": " • Dec 20, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://younesszaim.github.io/myportfolio/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Hi ! I’m Youness ZAIM. Please check my resume on **Linkedin .",
          "url": "https://younesszaim.github.io/myportfolio/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://younesszaim.github.io/myportfolio/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}