{
  
    
        "post0": {
            "title": "Us Cencus Income   Ensembles, Bagging And Shap Values",
            "content": "US cencus income : Ensembles, Bagging and Shap Values . Problem Framework . Our task is to determine the income level for the person represented by the record. Incomes have been binned at the $50K level to present a binary classification problem. . The dataset used in this analysis was extracted from the census bureau database found at. The data was split into train/test in approximately 2/3, 1/3 proportions. . The following mappings of the data is as follow : . Datasets can be found in this link : https://github.com/younesszaim/myportfolio/tree/master/_notebooks/Datasets . import pandas as pd from pandas.api.types import is_string_dtype, is_numeric_dtype, is_categorical_dtype import numpy as np PATH = &#39;/Users/rmbp/Desktop/Dataiku Data Scientist Technical Assessment&#39; df_labels = pd.read_csv(f&#39;{PATH}/census_income_metadata_column.csv&#39;, sep=&#39;;&#39;) df_labels.head(5) column_name dtype 0 age continuous 1 class_of_worker nominal 2 detailed_industry_recode nominal 3 detailed_occupation_recode nominal 4 education nominal . In any sort of data science work, it’s important to look at our data directly to make sure we understand the format, how it’s stored, what types of values it holds, etc. Even if we’ve read a description of the data, the actual data may not be what we expect. We’ll start by reading the training set into a Pandas DataFrame : . # Loading the train data df = pd.read_csv(f&#39;{PATH}/census_income_learn.csv&#39;, names = df_labels[&#39;column_name&#39;]) df.shape (199523, 42) . Let’s have a look at the columns, their types defined by Pandas and compared it to their actual mapping types : . # Chekcing the mapping of the data d1 = df.dtypes.apply(lambda x: x.name).to_dict() d2 = {c: d for c,d in zip(df_labels[&#39;column_name&#39;],df_labels[&#39;dtype&#39;])} mapping = [d1, d2] d = {} for k in d1.keys(): d[k] = tuple(d[k] for d in mapping) d {&#39;age&#39;: (&#39;int64&#39;, &#39;continuous&#39;), &#39;class_of_worker&#39;: (&#39;object&#39;, &#39;nominal&#39;), &#39;detailed_industry_recode&#39;: (&#39;int64&#39;, &#39;nominal&#39;), &#39;detailed_occupation_recode&#39;: (&#39;int64&#39;, &#39;nominal&#39;), &#39;education&#39;: (&#39;object&#39;, &#39;nominal&#39;), &#39;wage_per_hour&#39;: (&#39;int64&#39;, &#39;continuous&#39;), &#39;enroll_in_edu_inst_last_wk&#39;: (&#39;object&#39;, &#39;nominal&#39;), &#39;marital_stat&#39;: (&#39;object&#39;, &#39;nominal&#39;), &#39;major_industry_code&#39;: (&#39;object&#39;, &#39;nominal&#39;), &#39;major_occupation_code&#39;: (&#39;object&#39;, &#39;nominal&#39;), &#39;race&#39;: (&#39;object&#39;, &#39;nominal&#39;), &#39;hispanic_origin&#39;: (&#39;object&#39;, &#39;nominal&#39;), &#39;sex&#39;: (&#39;object&#39;, &#39;nominal&#39;), &#39;member_of_a_labor_union&#39;: (&#39;object&#39;, &#39;nominal&#39;), &#39;reason_for_unemployment&#39;: (&#39;object&#39;, &#39;nominal&#39;), &#39;full_or_part_time_employment_stat&#39;: (&#39;object&#39;, &#39;nominal&#39;), &#39;capital_gains&#39;: (&#39;int64&#39;, &#39;continuous&#39;), &#39;capital_losses&#39;: (&#39;int64&#39;, &#39;continuous&#39;), &#39;dividends_from_stocks&#39;: (&#39;int64&#39;, &#39;continuous&#39;), &#39;tax_filer_stat&#39;: (&#39;object&#39;, &#39;nominal&#39;), &#39;region_of_previous_residence&#39;: (&#39;object&#39;, &#39;nominal&#39;), &#39;state_of_previous_residence&#39;: (&#39;object&#39;, &#39;nominal&#39;), &#39;detailed_household_and_family_stat&#39;: (&#39;object&#39;, &#39;nominal&#39;), &#39;detailed_household_summary_in_household&#39;: (&#39;object&#39;, &#39;nominal&#39;), &#39;ignore&#39;: (&#39;float64&#39;, &#39;continuous&#39;), &#39;migration_code-change_in_msa&#39;: (&#39;object&#39;, &#39;nominal&#39;), &#39;migration_code-change_in_reg&#39;: (&#39;object&#39;, &#39;nominal&#39;), &#39;migration_code-move_within_reg&#39;: (&#39;object&#39;, &#39;nominal&#39;), &#39;live_in_this_house_1_year_ago&#39;: (&#39;object&#39;, &#39;nominal&#39;), &#39;migration_prev_res_in_sunbelt&#39;: (&#39;object&#39;, &#39;nominal&#39;), &#39;num_persons_worked_for_employer&#39;: (&#39;int64&#39;, &#39;continuous&#39;), &#39;family_members_under_18&#39;: (&#39;object&#39;, &#39;nominal&#39;), &#39;country_of_birth_father&#39;: (&#39;object&#39;, &#39;nominal&#39;), &#39;country_of_birth_mother&#39;: (&#39;object&#39;, &#39;nominal&#39;), &#39;country_of_birth_self&#39;: (&#39;object&#39;, &#39;nominal&#39;), &#39;citizenship&#39;: (&#39;object&#39;, &#39;nominal&#39;), &#39;own_business_or_self_employed&#39;: (&#39;int64&#39;, &#39;nominal&#39;), &quot;fill_inc_questionnaire_for_veteran&#39;s_admin&quot;: (&#39;object&#39;, &#39;nominal&#39;), &#39;veterans_benefits&#39;: (&#39;int64&#39;, &#39;nominal&#39;), &#39;weeks_worked_in_year&#39;: (&#39;int64&#39;, &#39;continuous&#39;), &#39;year&#39;: (&#39;int64&#39;, &#39;nominal&#39;), &#39;income_level&#39;: (&#39;object&#39;, &#39;nominal&#39;)} . We can see that detailed_industry_recode, detailed_occupation_recode, own_business_or_self_employed, veterans_benefits and year is set by default as a continuos category. . Let’s redifined their types : . # Correcting data types d1[&#39;detailed_industry_recode&#39;]=&#39;object&#39; d1[&#39;detailed_occupation_recode&#39;]=&#39;object&#39; d1[&#39;own_business_or_self_employed&#39;]=&#39;object&#39; d1[&#39;veterans_benefits&#39;]=&#39;object&#39; d1[&#39;year&#39;]=&#39;object&#39; . Let’s reload the data with its correspind feature’s mapping : . # reload data with coorexted types df = pd.read_csv(f&#39;{PATH}/census_income_learn.csv&#39;, names =df_labels[&#39;column_name&#39;], dtype= d1) . The info() method is useful to get a quick description of the data, in particular the total number of rows, each attribute’s type, and the number of nonnull values : . df.info() &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 199523 entries, 0 to 199522 Data columns (total 42 columns): # Column Non-Null Count Dtype -- -- 0 age 199523 non-null int64 1 class_of_worker 199523 non-null object 2 detailed_industry_recode 199523 non-null object 3 detailed_occupation_recode 199523 non-null object 4 education 199523 non-null object 5 wage_per_hour 199523 non-null int64 6 enroll_in_edu_inst_last_wk 199523 non-null object 7 marital_stat 199523 non-null object 8 major_industry_code 199523 non-null object 9 major_occupation_code 199523 non-null object 10 race 199523 non-null object 11 hispanic_origin 199523 non-null object 12 sex 199523 non-null object 13 member_of_a_labor_union 199523 non-null object 14 reason_for_unemployment 199523 non-null object 15 full_or_part_time_employment_stat 199523 non-null object 16 capital_gains 199523 non-null int64 17 capital_losses 199523 non-null int64 18 dividends_from_stocks 199523 non-null int64 19 tax_filer_stat 199523 non-null object 20 region_of_previous_residence 199523 non-null object 21 state_of_previous_residence 199523 non-null object 22 detailed_household_and_family_stat 199523 non-null object 23 detailed_household_summary_in_household 199523 non-null object 24 ignore 199523 non-null float64 25 migration_code-change_in_msa 199523 non-null object 26 migration_code-change_in_reg 199523 non-null object 27 migration_code-move_within_reg 199523 non-null object 28 live_in_this_house_1_year_ago 199523 non-null object 29 migration_prev_res_in_sunbelt 199523 non-null object 30 num_persons_worked_for_employer 199523 non-null int64 31 family_members_under_18 199523 non-null object 32 country_of_birth_father 199523 non-null object 33 country_of_birth_mother 199523 non-null object 34 country_of_birth_self 199523 non-null object 35 citizenship 199523 non-null object 36 own_business_or_self_employed 199523 non-null object 37 fill_inc_questionnaire_for_veteran&#39;s_admin 199523 non-null object 38 veterans_benefits 199523 non-null object 39 weeks_worked_in_year 199523 non-null int64 40 year 199523 non-null object 41 income_level 199523 non-null object dtypes: float64(1), int64(7), object(34) memory usage: 63.9+ MB # dispplay first rows with pd.option_context(&#39;display.max_rows&#39;, None, &#39;display.max_columns&#39;, None): display(df.head(3)) age class_of_worker detailed_industry_recode 0 73 Not in universe 0 1 58 Self-employed-not incorporated 4 2 18 Not in universe 0 detailed_occupation_recode education wage_per_hour 0 0 High school graduate 0 1 34 Some college but no degree 0 2 0 10th grade 0 enroll_in_edu_inst_last_wk marital_stat major_industry_code 0 Not in universe Widowed Not in universe or children 1 Not in universe Divorced Construction 2 High school Never married Not in universe or children major_occupation_code race 0 Not in universe White 1 Precision production craft &amp; repair White 2 Not in universe Asian or Pacific Islander hispanic_origin sex member_of_a_labor_union reason_for_unemployment 0 All other Female Not in universe Not in universe 1 All other Male Not in universe Not in universe 2 All other Female Not in universe Not in universe full_or_part_time_employment_stat capital_gains capital_losses 0 Not in labor force 0 0 1 Children or Armed Forces 0 0 2 Not in labor force 0 0 dividends_from_stocks tax_filer_stat region_of_previous_residence 0 0 Nonfiler Not in universe 1 0 Head of household South 2 0 Nonfiler Not in universe state_of_previous_residence detailed_household_and_family_stat 0 Not in universe Other Rel 18+ ever marr not in subfamily 1 Arkansas Householder 2 Not in universe Child 18+ never marr Not in a subfamily detailed_household_summary_in_household ignore 0 Other relative of householder 1700.09 1 Householder 1053.55 2 Child 18 or older 991.95 migration_code-change_in_msa migration_code-change_in_reg 0 ? ? 1 MSA to MSA Same county 2 ? ? migration_code-move_within_reg live_in_this_house_1_year_ago 0 ? Not in universe under 1 year old 1 Same county No 2 ? Not in universe under 1 year old migration_prev_res_in_sunbelt num_persons_worked_for_employer 0 ? 0 1 Yes 1 2 ? 0 family_members_under_18 country_of_birth_father country_of_birth_mother 0 Not in universe United-States United-States 1 Not in universe United-States United-States 2 Not in universe Vietnam Vietnam country_of_birth_self citizenship 0 United-States Native- Born in the United States 1 United-States Native- Born in the United States 2 Vietnam Foreign born- Not a citizen of U S own_business_or_self_employed fill_inc_questionnaire_for_veteran&#39;s_admin 0 0 Not in universe 1 0 Not in universe 2 0 Not in universe veterans_benefits weeks_worked_in_year year income_level 0 2 0 95 - 50000. 1 2 52 94 - 50000. 2 2 0 95 - 50000. # drop &#39;ignore&#39; column df.drop(&#39;ignore&#39;, axis=1,inplace=True) # list columns df.columns Index([&#39;age&#39;, &#39;class_of_worker&#39;, &#39;detailed_industry_recode&#39;, &#39;detailed_occupation_recode&#39;, &#39;education&#39;, &#39;wage_per_hour&#39;, &#39;enroll_in_edu_inst_last_wk&#39;, &#39;marital_stat&#39;, &#39;major_industry_code&#39;, &#39;major_occupation_code&#39;, &#39;race&#39;, &#39;hispanic_origin&#39;, &#39;sex&#39;, &#39;member_of_a_labor_union&#39;, &#39;reason_for_unemployment&#39;, &#39;full_or_part_time_employment_stat&#39;, &#39;capital_gains&#39;, &#39;capital_losses&#39;, &#39;dividends_from_stocks&#39;, &#39;tax_filer_stat&#39;, &#39;region_of_previous_residence&#39;, &#39;state_of_previous_residence&#39;, &#39;detailed_household_and_family_stat&#39;, &#39;detailed_household_summary_in_household&#39;, &#39;migration_code-change_in_msa&#39;, &#39;migration_code-change_in_reg&#39;, &#39;migration_code-move_within_reg&#39;, &#39;live_in_this_house_1_year_ago&#39;, &#39;migration_prev_res_in_sunbelt&#39;, &#39;num_persons_worked_for_employer&#39;, &#39;family_members_under_18&#39;, &#39;country_of_birth_father&#39;, &#39;country_of_birth_mother&#39;, &#39;country_of_birth_self&#39;, &#39;citizenship&#39;, &#39;own_business_or_self_employed&#39;, &#39;fill_inc_questionnaire_for_veteran&#39;s_admin&#39;, &#39;veterans_benefits&#39;, &#39;weeks_worked_in_year&#39;, &#39;year&#39;, &#39;income_level&#39;], dtype=&#39;object&#39;) . We load the test set with the same training data types : . # loading the test set test = pd.read_csv(f&#39;{PATH}/census_income_test.csv&#39;, names =df_labels[&#39;column_name&#39;], dtype= d1 ) test.info() &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 99762 entries, 0 to 99761 Data columns (total 42 columns): # Column Non-Null Count Dtype -- -- 0 age 99762 non-null int64 1 class_of_worker 99762 non-null object 2 detailed_industry_recode 99762 non-null object 3 detailed_occupation_recode 99762 non-null object 4 education 99762 non-null object 5 wage_per_hour 99762 non-null int64 6 enroll_in_edu_inst_last_wk 99762 non-null object 7 marital_stat 99762 non-null object 8 major_industry_code 99762 non-null object 9 major_occupation_code 99762 non-null object 10 race 99762 non-null object 11 hispanic_origin 99762 non-null object 12 sex 99762 non-null object 13 member_of_a_labor_union 99762 non-null object 14 reason_for_unemployment 99762 non-null object 15 full_or_part_time_employment_stat 99762 non-null object 16 capital_gains 99762 non-null int64 17 capital_losses 99762 non-null int64 18 dividends_from_stocks 99762 non-null int64 19 tax_filer_stat 99762 non-null object 20 region_of_previous_residence 99762 non-null object 21 state_of_previous_residence 99762 non-null object 22 detailed_household_and_family_stat 99762 non-null object 23 detailed_household_summary_in_household 99762 non-null object 24 ignore 99762 non-null float64 25 migration_code-change_in_msa 99762 non-null object 26 migration_code-change_in_reg 99762 non-null object 27 migration_code-move_within_reg 99762 non-null object 28 live_in_this_house_1_year_ago 99762 non-null object 29 migration_prev_res_in_sunbelt 99762 non-null object 30 num_persons_worked_for_employer 99762 non-null int64 31 family_members_under_18 99762 non-null object 32 country_of_birth_father 99762 non-null object 33 country_of_birth_mother 99762 non-null object 34 country_of_birth_self 99762 non-null object 35 citizenship 99762 non-null object 36 own_business_or_self_employed 99762 non-null object 37 fill_inc_questionnaire_for_veteran&#39;s_admin 99762 non-null object 38 veterans_benefits 99762 non-null object 39 weeks_worked_in_year 99762 non-null int64 40 year 99762 non-null object 41 income_level 99762 non-null object dtypes: float64(1), int64(7), object(34) memory usage: 32.0+ MB . We verify if we got the same columns both on the train and the test set : . # checking columns on test set which not in train set(test.columns).difference(set(df.columns)) {&#39;ignore&#39;} # dropping &#39;ignore&#39; columns test.drop(&#39;ignore&#39;, inplace=True, axis=1) test.columns Index([&#39;age&#39;, &#39;class_of_worker&#39;, &#39;detailed_industry_recode&#39;, &#39;detailed_occupation_recode&#39;, &#39;education&#39;, &#39;wage_per_hour&#39;, &#39;enroll_in_edu_inst_last_wk&#39;, &#39;marital_stat&#39;, &#39;major_industry_code&#39;, &#39;major_occupation_code&#39;, &#39;race&#39;, &#39;hispanic_origin&#39;, &#39;sex&#39;, &#39;member_of_a_labor_union&#39;, &#39;reason_for_unemployment&#39;, &#39;full_or_part_time_employment_stat&#39;, &#39;capital_gains&#39;, &#39;capital_losses&#39;, &#39;dividends_from_stocks&#39;, &#39;tax_filer_stat&#39;, &#39;region_of_previous_residence&#39;, &#39;state_of_previous_residence&#39;, &#39;detailed_household_and_family_stat&#39;, &#39;detailed_household_summary_in_household&#39;, &#39;migration_code-change_in_msa&#39;, &#39;migration_code-change_in_reg&#39;, &#39;migration_code-move_within_reg&#39;, &#39;live_in_this_house_1_year_ago&#39;, &#39;migration_prev_res_in_sunbelt&#39;, &#39;num_persons_worked_for_employer&#39;, &#39;family_members_under_18&#39;, &#39;country_of_birth_father&#39;, &#39;country_of_birth_mother&#39;, &#39;country_of_birth_self&#39;, &#39;citizenship&#39;, &#39;own_business_or_self_employed&#39;, &#39;fill_inc_questionnaire_for_veteran&#39;s_admin&#39;, &#39;veterans_benefits&#39;, &#39;weeks_worked_in_year&#39;, &#39;year&#39;, &#39;income_level&#39;], dtype=&#39;object&#39;) # display first rows of the test set with pd.option_context(&#39;display.max_rows&#39;, None, &#39;display.max_columns&#39;, None): display(test.head(3)) age class_of_worker detailed_industry_recode 0 38 Private 6 1 44 Self-employed-not incorporated 37 2 2 Not in universe 0 detailed_occupation_recode education 0 36 1st 2nd 3rd or 4th grade 1 12 Associates degree-occup /vocational 2 0 Children wage_per_hour enroll_in_edu_inst_last_wk marital_stat 0 0 Not in universe Married-civilian spouse present 1 0 Not in universe Married-civilian spouse present 2 0 Not in universe Never married major_industry_code major_occupation_code 0 Manufacturing-durable goods Machine operators assmblrs &amp; inspctrs 1 Business and repair services Professional specialty 2 Not in universe or children Not in universe race hispanic_origin sex member_of_a_labor_union 0 White Mexican (Mexicano) Female Not in universe 1 White All other Female Not in universe 2 White Mexican-American Male Not in universe reason_for_unemployment full_or_part_time_employment_stat capital_gains 0 Not in universe Full-time schedules 0 1 Not in universe PT for econ reasons usually PT 0 2 Not in universe Children or Armed Forces 0 capital_losses dividends_from_stocks tax_filer_stat 0 0 0 Joint one under 65 &amp; one 65+ 1 0 2500 Joint both under 65 2 0 0 Nonfiler region_of_previous_residence state_of_previous_residence 0 Not in universe Not in universe 1 Not in universe Not in universe 2 Not in universe Not in universe detailed_household_and_family_stat 0 Spouse of householder 1 Spouse of householder 2 Child &lt;18 never marr not in subfamily detailed_household_summary_in_household migration_code-change_in_msa 0 Spouse of householder ? 1 Spouse of householder ? 2 Child under 18 never married ? migration_code-change_in_reg migration_code-move_within_reg 0 ? ? 1 ? ? 2 ? ? live_in_this_house_1_year_ago migration_prev_res_in_sunbelt 0 Not in universe under 1 year old ? 1 Not in universe under 1 year old ? 2 Not in universe under 1 year old ? num_persons_worked_for_employer family_members_under_18 0 4 Not in universe 1 1 Not in universe 2 0 Both parents present country_of_birth_father country_of_birth_mother country_of_birth_self 0 Mexico Mexico Mexico 1 United-States United-States United-States 2 United-States United-States United-States citizenship own_business_or_self_employed 0 Foreign born- Not a citizen of U S 0 1 Native- Born in the United States 0 2 Native- Born in the United States 0 fill_inc_questionnaire_for_veteran&#39;s_admin veterans_benefits 0 Not in universe 2 1 Not in universe 2 2 Not in universe 0 weeks_worked_in_year year income_level 0 12 95 - 50000. 1 26 95 - 50000. 2 0 95 - 50000. df.shape, test.shape ((199523, 41), (99762, 41)) . Looking at the data . The most important data column is the dependent variable—that is, the one we want to predict which is income_level : . dep_var = &#39;income_level&#39; . Let’s see its distribution : . print(df[dep_var].value_counts(normalize = True)) df[dep_var].value_counts(normalize = True).plot(kind=&#39;bar&#39;, edgecolor=&#39;black&#39;, color=&#39;blue&#39;, title=&#39;income_level&#39;) - 50000. 0.937942 50000+. 0.062058 Name: income_level, dtype: float64 &lt;matplotlib.axes._subplots.AxesSubplot at 0x124bace90&gt; . . We have an imbalanced dataset where the income level of -50k is representing more than 93% of the total records. . Next, we automatically handle which columns are continuous and which are categorical : . # get categorical and numerical variables def cont_cat_split(df, dep_var=None): &quot;Helper function that returns column names of cont and cat variables from given `df`.&quot; cont_names, cat_names = [], [] for label in df: if label in [dep_var]: continue if (pd.api.types.is_integer_dtype(df[label].dtype) or pd.api.types.is_float_dtype(df[label].dtype)): cont_names.append(label) else: cat_names.append(label) return cont_names, cat_names cont, cat = cont_cat_split(df, dep_var= dep_var) cont , cat ([&#39;age&#39;, &#39;wage_per_hour&#39;, &#39;capital_gains&#39;, &#39;capital_losses&#39;, &#39;dividends_from_stocks&#39;, &#39;num_persons_worked_for_employer&#39;, &#39;weeks_worked_in_year&#39;], [&#39;class_of_worker&#39;, &#39;detailed_industry_recode&#39;, &#39;detailed_occupation_recode&#39;, &#39;education&#39;, &#39;enroll_in_edu_inst_last_wk&#39;, &#39;marital_stat&#39;, &#39;major_industry_code&#39;, &#39;major_occupation_code&#39;, &#39;race&#39;, &#39;hispanic_origin&#39;, &#39;sex&#39;, &#39;member_of_a_labor_union&#39;, &#39;reason_for_unemployment&#39;, &#39;full_or_part_time_employment_stat&#39;, &#39;tax_filer_stat&#39;, &#39;region_of_previous_residence&#39;, &#39;state_of_previous_residence&#39;, &#39;detailed_household_and_family_stat&#39;, &#39;detailed_household_summary_in_household&#39;, &#39;migration_code-change_in_msa&#39;, &#39;migration_code-change_in_reg&#39;, &#39;migration_code-move_within_reg&#39;, &#39;live_in_this_house_1_year_ago&#39;, &#39;migration_prev_res_in_sunbelt&#39;, &#39;family_members_under_18&#39;, &#39;country_of_birth_father&#39;, &#39;country_of_birth_mother&#39;, &#39;country_of_birth_self&#39;, &#39;citizenship&#39;, &#39;own_business_or_self_employed&#39;, &quot;fill_inc_questionnaire_for_veteran&#39;s_admin&quot;, &#39;veterans_benefits&#39;, &#39;year&#39;]) . Let’s start by checking the modalties of our categorical variables : . # Check modalities of categorical varaiblies for c in cat : print(pd.DataFrame({c : df[c].value_counts()/len(df)})) class_of_worker Not in universe 0.502423 Private 0.361001 Self-employed-not incorporated 0.042326 Local government 0.039013 State government 0.021186 Self-employed-incorporated 0.016364 Federal government 0.014660 Never worked 0.002200 Without pay 0.000827 detailed_industry_recode 0 0.504624 33 0.085554 43 0.041514 4 0.029992 42 0.023471 45 0.022464 29 0.021095 37 0.020158 41 0.019867 32 0.018023 35 0.016940 39 0.014720 34 0.013858 44 0.012775 2 0.011006 11 0.008841 50 0.008540 40 0.008275 47 0.008240 38 0.008164 24 0.007533 12 0.006766 19 0.006746 30 0.005919 31 0.005904 25 0.005433 9 0.004977 22 0.004771 36 0.004736 13 0.004506 1 0.004145 48 0.003268 27 0.003137 49 0.003057 3 0.002822 21 0.002802 6 0.002777 5 0.002772 8 0.002757 16 0.002701 23 0.002631 18 0.002421 15 0.002265 7 0.002115 14 0.001479 46 0.000937 17 0.000787 28 0.000717 26 0.000637 51 0.000180 20 0.000160 10 0.000020 detailed_occupation_recode 0 0.504624 2 0.043885 26 0.039529 19 0.027130 29 0.025586 36 0.020775 34 0.020173 10 0.018459 16 0.017266 23 0.017001 12 0.016740 33 0.016665 3 0.016013 35 0.015878 38 0.015051 31 0.013527 32 0.012019 37 0.011197 8 0.010781 42 0.009613 30 0.009508 24 0.009257 17 0.008876 28 0.008325 41 0.007979 44 0.007979 43 0.006927 4 0.006836 13 0.006370 18 0.005428 39 0.005097 14 0.004671 5 0.004285 15 0.004085 27 0.003909 25 0.003844 9 0.003699 7 0.003664 11 0.003193 40 0.003092 1 0.002727 21 0.002671 6 0.002210 22 0.002060 45 0.000862 20 0.000356 46 0.000180 education High school graduate 0.242614 Children 0.237677 Some college but no degree 0.139433 Bachelors degree(BA AB BS) 0.099562 7th and 8th grade 0.040131 10th grade 0.037875 11th grade 0.034462 Masters degree(MA MS MEng MEd MSW MBA) 0.032783 9th grade 0.031224 Associates degree-occup /vocational 0.026854 Associates degree-academic program 0.021867 5th or 6th grade 0.016424 12th grade no diploma 0.010655 1st 2nd 3rd or 4th grade 0.009017 Prof school degree (MD DDS DVM LLB JD) 0.008986 Doctorate degree(PhD EdD) 0.006330 Less than 1st grade 0.004105 enroll_in_edu_inst_last_wk Not in universe 0.936950 High school 0.034542 College or university 0.028508 marital_stat Never married 0.433459 Married-civilian spouse present 0.422117 Divorced 0.063702 Widowed 0.052440 Separated 0.017341 Married-spouse absent 0.007608 Married-A F spouse present 0.003333 major_industry_code Not in universe or children 0.504624 Retail trade 0.085554 Manufacturing-durable goods 0.045183 Education 0.041514 Manufacturing-nondurable goods 0.034567 Finance insurance and real estate 0.030798 Construction 0.029992 Business and repair services 0.028323 Medical except hospital 0.023471 Public administration 0.023105 Other professional services 0.022464 Transportation 0.021095 Hospital services 0.019867 Wholesale trade 0.018023 Agriculture 0.015151 Personal services except private HH 0.014720 Social services 0.012775 Entertainment 0.008275 Communications 0.005919 Utilities and sanitary services 0.005904 Private household services 0.004736 Mining 0.002822 Forestry and fisheries 0.000937 Armed Forces 0.000180 major_occupation_code Not in universe 0.504624 Adm support including clerical 0.074362 Professional specialty 0.069867 Executive admin and managerial 0.062624 Other service 0.060640 Sales 0.059056 Precision production craft &amp; repair 0.052716 Machine operators assmblrs &amp; inspctrs 0.031971 Handlers equip cleaners etc 0.020684 Transportation and material moving 0.020148 Farming forestry and fishing 0.015768 Technicians and related support 0.015126 Protective services 0.008325 Private household services 0.003909 Armed Forces 0.000180 race White 0.838826 Black 0.102319 Asian or Pacific Islander 0.029245 Other 0.018329 Amer Indian Aleut or Eskimo 0.011282 hispanic_origin All other 0.861590 Mexican-American 0.040492 Mexican (Mexicano) 0.036256 Central or South American 0.019522 Puerto Rican 0.016605 Other Spanish 0.012455 Cuban 0.005643 NA 0.004380 Do not know 0.001534 Chicano 0.001524 sex Female 0.521163 Male 0.478837 member_of_a_labor_union Not in universe 0.904452 No 0.080362 Yes 0.015186 reason_for_unemployment Not in universe 0.969577 Other job loser 0.010214 Re-entrant 0.010119 Job loser - on layoff 0.004892 Job leaver 0.002997 New entrant 0.002200 full_or_part_time_employment_stat Children or Armed Forces 0.620324 Full-time schedules 0.204167 Not in labor force 0.134360 PT for non-econ reasons usually FT 0.016650 Unemployed full-time 0.011583 PT for econ reasons usually PT 0.006059 Unemployed part- time 0.004225 PT for econ reasons usually FT 0.002631 tax_filer_stat Nonfiler 0.376368 Joint both under 65 0.337720 Single 0.187552 Joint both 65+ 0.041760 Head of household 0.037219 Joint one under 65 &amp; one 65+ 0.019381 region_of_previous_residence Not in universe 0.920946 South 0.024503 West 0.020419 Midwest 0.017918 Northeast 0.013557 Abroad 0.002656 state_of_previous_residence Not in universe 0.920946 California 0.008590 Utah 0.005328 Florida 0.004255 North Carolina 0.004070 ? 0.003548 Abroad 0.003363 Oklahoma 0.003137 Minnesota 0.002887 Indiana 0.002671 North Dakota 0.002501 New Mexico 0.002321 Michigan 0.002210 Alaska 0.001453 Kentucky 0.001223 Arizona 0.001218 New Hampshire 0.001213 Wyoming 0.001208 Colorado 0.001198 Oregon 0.001183 West Virginia 0.001158 Georgia 0.001138 Montana 0.001133 Alabama 0.001083 Ohio 0.001058 Texas 0.001047 Arkansas 0.001027 Mississippi 0.001022 Tennessee 0.001012 Pennsylvania 0.000997 New York 0.000977 Louisiana 0.000962 Vermont 0.000957 Iowa 0.000947 Illinois 0.000902 Nebraska 0.000892 Missouri 0.000877 Nevada 0.000872 Maine 0.000837 Massachusetts 0.000757 Kansas 0.000747 South Dakota 0.000692 Maryland 0.000682 Virginia 0.000632 Connecticut 0.000586 District of Columbia 0.000581 Wisconsin 0.000526 South Carolina 0.000476 New Jersey 0.000376 Delaware 0.000366 Idaho 0.000155 detailed_household_and_family_stat Householder 0.266877 Child &lt;18 never marr not in subfamily 0.252232 Spouse of householder 0.208973 Nonfamily householder 0.111331 Child 18+ never marr Not in a subfamily 0.060294 Secondary individual 0.030683 Other Rel 18+ ever marr not in subfamily 0.009803 Grandchild &lt;18 never marr child of subfamily RP 0.009362 Other Rel 18+ never marr not in subfamily 0.008661 Grandchild &lt;18 never marr not in subfamily 0.005343 Child 18+ ever marr Not in a subfamily 0.005077 Child under 18 of RP of unrel subfamily 0.003669 RP of unrelated subfamily 0.003433 Child 18+ ever marr RP of subfamily 0.003363 Other Rel &lt;18 never marr child of subfamily RP 0.003288 Other Rel 18+ ever marr RP of subfamily 0.003288 Other Rel 18+ spouse of subfamily RP 0.003198 Child 18+ never marr RP of subfamily 0.002952 Other Rel &lt;18 never marr not in subfamily 0.002927 Grandchild 18+ never marr not in subfamily 0.001879 In group quarters 0.000982 Child 18+ spouse of subfamily RP 0.000632 Other Rel 18+ never marr RP of subfamily 0.000471 Child &lt;18 never marr RP of subfamily 0.000401 Spouse of RP of unrelated subfamily 0.000261 Child &lt;18 ever marr not in subfamily 0.000180 Grandchild 18+ ever marr not in subfamily 0.000170 Grandchild 18+ spouse of subfamily RP 0.000050 Child &lt;18 ever marr RP of subfamily 0.000045 Grandchild 18+ ever marr RP of subfamily 0.000045 Grandchild 18+ never marr RP of subfamily 0.000030 Other Rel &lt;18 ever marr RP of subfamily 0.000030 Other Rel &lt;18 never married RP of subfamily 0.000020 Other Rel &lt;18 spouse of subfamily RP 0.000015 Child &lt;18 spouse of subfamily RP 0.000010 Grandchild &lt;18 never marr RP of subfamily 0.000010 Grandchild &lt;18 ever marr not in subfamily 0.000010 Other Rel &lt;18 ever marr not in subfamily 0.000005 detailed_household_summary_in_household Householder 0.378277 Child under 18 never married 0.252733 Spouse of householder 0.209044 Child 18 or older 0.072322 Other relative of householder 0.048631 Nonrelative of householder 0.038096 Group Quarters- Secondary individual 0.000662 Child under 18 ever married 0.000236 migration_code-change_in_msa ? 0.499672 Nonmover 0.413677 MSA to MSA 0.053132 NonMSA to nonMSA 0.014089 Not in universe 0.007598 MSA to nonMSA 0.003959 NonMSA to MSA 0.003082 Abroad to MSA 0.002270 Not identifiable 0.002155 Abroad to nonMSA 0.000366 migration_code-change_in_reg ? 0.499672 Nonmover 0.413677 Same county 0.049177 Different county same state 0.014018 Not in universe 0.007598 Different region 0.005904 Different state same division 0.004967 Abroad 0.002656 Different division same region 0.002331 migration_code-move_within_reg ? 0.499672 Nonmover 0.413677 Same county 0.049177 Different county same state 0.014018 Not in universe 0.007598 Different state in South 0.004877 Different state in West 0.003403 Different state in Midwest 0.002762 Abroad 0.002656 Different state in Northeast 0.002160 live_in_this_house_1_year_ago Not in universe under 1 year old 0.507270 Yes 0.413677 No 0.079054 migration_prev_res_in_sunbelt ? 0.499672 Not in universe 0.421275 No 0.050054 Yes 0.028999 family_members_under_18 Not in universe 0.722884 Both parents present 0.195381 Mother only present 0.064013 Father only present 0.009438 Neither parent present 0.008285 country_of_birth_father United-States 0.797718 Mexico 0.050160 ? 0.033645 Puerto-Rico 0.013432 Italy 0.011086 Canada 0.006916 Germany 0.006796 Dominican-Republic 0.006465 Poland 0.006074 Philippines 0.005784 Cuba 0.005638 El-Salvador 0.004922 China 0.004290 England 0.003974 Columbia 0.003077 India 0.002907 South Korea 0.002656 Ireland 0.002546 Jamaica 0.002321 Vietnam 0.002290 Guatemala 0.002230 Japan 0.001965 Portugal 0.001945 Ecuador 0.001900 Haiti 0.001759 Greece 0.001724 Peru 0.001679 Nicaragua 0.001579 Hungary 0.001534 Scotland 0.001238 Iran 0.001168 Yugoslavia 0.001088 Taiwan 0.000997 Cambodia 0.000982 Honduras 0.000972 France 0.000957 Outlying-U S (Guam USVI etc) 0.000797 Laos 0.000772 Trinadad&amp;Tobago 0.000566 Thailand 0.000536 Hong Kong 0.000531 Holand-Netherlands 0.000256 Panama 0.000125 country_of_birth_mother United-States 0.804313 Mexico 0.049022 ? 0.030668 Puerto-Rico 0.012395 Italy 0.009242 Canada 0.007272 Germany 0.006927 Philippines 0.006170 Poland 0.005563 El-Salvador 0.005553 Cuba 0.005553 Dominican-Republic 0.005528 England 0.004526 China 0.003809 Columbia 0.003067 South Korea 0.003052 Ireland 0.003002 India 0.002912 Vietnam 0.002371 Japan 0.002351 Jamaica 0.002270 Guatemala 0.002225 Ecuador 0.001879 Peru 0.001779 Haiti 0.001769 Portugal 0.001714 Nicaragua 0.001509 Hungary 0.001489 Greece 0.001308 Scotland 0.001208 Taiwan 0.001113 Honduras 0.001093 France 0.001063 Iran 0.000992 Yugoslavia 0.000887 Cambodia 0.000787 Outlying-U S (Guam USVI etc) 0.000787 Laos 0.000777 Thailand 0.000616 Hong Kong 0.000536 Trinadad&amp;Tobago 0.000496 Holand-Netherlands 0.000246 Panama 0.000160 country_of_birth_self United-States 0.887061 Mexico 0.028904 ? 0.017006 Puerto-Rico 0.007017 Germany 0.004265 Philippines 0.004235 Cuba 0.004195 Canada 0.003508 Dominican-Republic 0.003458 El-Salvador 0.003453 China 0.002396 South Korea 0.002361 England 0.002290 Columbia 0.002175 Italy 0.002100 India 0.002045 Vietnam 0.001960 Poland 0.001910 Guatemala 0.001724 Japan 0.001699 Jamaica 0.001604 Peru 0.001343 Ecuador 0.001293 Haiti 0.001143 Nicaragua 0.001093 Taiwan 0.001007 Portugal 0.000872 Iran 0.000787 Greece 0.000737 Honduras 0.000722 Ireland 0.000677 France 0.000606 Outlying-U S (Guam USVI etc) 0.000596 Thailand 0.000566 Laos 0.000526 Hong Kong 0.000501 Cambodia 0.000476 Hungary 0.000396 Scotland 0.000376 Trinadad&amp;Tobago 0.000331 Yugoslavia 0.000331 Panama 0.000140 Holand-Netherlands 0.000115 citizenship Native- Born in the United States 0.887076 Foreign born- Not a citizen of U S 0.067165 Foreign born- U S citizen by naturalization 0.029345 Native- Born abroad of American Parent(s) 0.008801 Native- Born in Puerto Rico or U S Outlying 0.007613 own_business_or_self_employed 0 0.905520 2 0.080958 1 0.013522 fill_inc_questionnaire_for_veteran&#39;s_admin Not in universe 0.990056 No 0.007984 Yes 0.001960 veterans_benefits 2 0.752445 0 0.237612 1 0.009944 year 94 0.500328 95 0.499672 . Some categorical features are purely nominal-having multiple modalities (with modality ? for nan values) and others are ordinal columns like education and year: . # Ediucation modalities df[&#39;education&#39;].unique(), df[&#39;education&#39;].nunique() (array([&#39; High school graduate&#39;, &#39; Some college but no degree&#39;, &#39; 10th grade&#39;, &#39; Children&#39;, &#39; Bachelors degree(BA AB BS)&#39;, &#39; Masters degree(MA MS MEng MEd MSW MBA)&#39;, &#39; Less than 1st grade&#39;, &#39; Associates degree-academic program&#39;, &#39; 7th and 8th grade&#39;, &#39; 12th grade no diploma&#39;, &#39; Associates degree-occup /vocational&#39;, &#39; Prof school degree (MD DDS DVM LLB JD)&#39;, &#39; 5th or 6th grade&#39;, &#39; 11th grade&#39;, &#39; Doctorate degree(PhD EdD)&#39;, &#39; 9th grade&#39;, &#39; 1st 2nd 3rd or 4th grade&#39;], dtype=object), 17) # Year modalities df[&#39;year&#39;].unique(), df[&#39;year&#39;].nunique() (array([&#39; 95&#39;, &#39; 94&#39;], dtype=object), 2) . We can tell Pandas about a suitable ordering of these levels like so: . # Setting the order of education variable education = &#39; Children&#39;,&#39; Less than 1st grade&#39;,&#39; 1st 2nd 3rd or 4th grade&#39;,&#39; 5th or 6th grade&#39;, &#39; 7th and 8th grade&#39;,&#39; 9th grade&#39;,&#39; 10th grade&#39;,&#39; 11th grade&#39;, &#39; 12th grade no diploma&#39;, &#39; High school graduate&#39;, &#39; Associates degree-academic program&#39;,&#39; Associates degree-occup /vocational&#39;, &#39; Prof school degree (MD DDS DVM LLB JD)&#39;,&#39; Some college but no degree&#39;,&#39; Bachelors degree(BA AB BS)&#39;, &#39; Masters degree(MA MS MEng MEd MSW MBA)&#39;,&#39; Doctorate degree(PhD EdD)&#39; len(education) 17 # Setting the order of year variaable year = &#39;94&#39;, &#39;95&#39; # apply the defined ordering fot our data : df[&#39;education&#39;] = df[&#39;education&#39;].astype(&#39;category&#39;) df[&#39;education&#39;].cat.set_categories(education, ordered=True, inplace=True) df[&#39;year&#39;] = df[&#39;year&#39;].astype(&#39;category&#39;) df[&#39;year&#39;].cat.set_categories(year, ordered=True, inplace=True) #Same for test set : test[&#39;education&#39;] = test[&#39;education&#39;].astype(&#39;category&#39;) test[&#39;education&#39;].cat.set_categories(education, ordered=True, inplace=True) test[&#39;year&#39;] = test[&#39;year&#39;].astype(&#39;category&#39;) test[&#39;year&#39;].cat.set_categories(year, ordered=True, inplace=True) /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages/pandas/core/arrays/categorical.py:2631: FutureWarning: The `inplace` parameter in pandas.Categorical.set_categories is deprecated and will be removed in a future version. Removing unused categories will always return a new Categorical object. res = method(*args, **kwargs) . Lets check our continous features: The describe() method shows a summary of the numerical attributes . df[cont].describe() age wage_per_hour capital_gains capital_losses count 199523.000000 199523.000000 199523.00000 199523.000000 mean 34.494199 55.426908 434.71899 37.313788 std 22.310895 274.896454 4697.53128 271.896428 min 0.000000 0.000000 0.00000 0.000000 25% 15.000000 0.000000 0.00000 0.000000 50% 33.000000 0.000000 0.00000 0.000000 75% 50.000000 0.000000 0.00000 0.000000 max 90.000000 9999.000000 99999.00000 4608.000000 dividends_from_stocks num_persons_worked_for_employer count 199523.000000 199523.000000 mean 197.529533 1.956180 std 1984.163658 2.365126 min 0.000000 0.000000 25% 0.000000 0.000000 50% 0.000000 1.000000 75% 0.000000 4.000000 max 99999.000000 6.000000 weeks_worked_in_year count 199523.000000 mean 23.174897 std 24.411488 min 0.000000 25% 0.000000 50% 8.000000 75% 52.000000 max 52.000000 . The count, mean, min, and max rows are self-explanatory.The std row shows the standard deviation, which measures how dispersed the values are. The 25%, 50%, and 75% rows show the corresponding percentiles. . We plot a histogram for each numerical attribute : . %matplotlib inline import matplotlib.pyplot as plt df[cont].hist(bins=50, figsize=(20,15)) plt.show() . . We can see that these attributes have very different scales. . | Some numerical varaibles are countinous like age and others are discrete and finite like weeks_worked_in_year or infinete num_persons_worked_for_employer. . | Some features as wage_per_hour,capital_gains,capital_losses,dividends_from_stocks are tail-heavy: they extend much farther to the median right with high coefficient of variation : . | . df[cont].boxplot(column=[&#39;wage_per_hour&#39;,&#39;capital_gains&#39;,&#39;capital_losses&#39;,&#39;dividends_from_stocks&#39;], figsize=(10,5)) &lt;matplotlib.axes._subplots.AxesSubplot at 0x125ab4190&gt; . . We can see the presence of extreme values for those features. . Using the skewness value, which explains the extent to which the data is normally distributed, in order to confirm that. Ideally, the skewness value should be between -1 and +1, and any major deviation from this range indicates the presence of extreme values. . We can calculate the skwenss value : . # skewness value df[[&#39;wage_per_hour&#39;,&#39;capital_gains&#39;,&#39;capital_losses&#39;,&#39;dividends_from_stocks&#39;]].skew() wage_per_hour 8.935097 capital_gains 18.990822 capital_losses 7.632565 dividends_from_stocks 27.786502 dtype: float64 . Using the IQR score, let’s see the number of obseravtions that are not in the (Q1 - 1.5 IQR) and (Q3 + 1.5 IQR) range : . # IQR score Q1 = df[[&#39;wage_per_hour&#39;,&#39;capital_gains&#39;,&#39;capital_losses&#39;,&#39;dividends_from_stocks&#39;]].quantile(0.25) Q3 = df[[&#39;wage_per_hour&#39;,&#39;capital_gains&#39;,&#39;capital_losses&#39;,&#39;dividends_from_stocks&#39;]].quantile(0.75) IQR = Q3 - Q1 print(IQR) wage_per_hour 0.0 capital_gains 0.0 capital_losses 0.0 dividends_from_stocks 0.0 dtype: float64 # number of observation out of the definied range out = df[[&#39;wage_per_hour&#39;,&#39;capital_gains&#39;,&#39;capital_losses&#39;,&#39;dividends_from_stocks&#39;]] df_out = out[((out &lt; (Q1 - 1.5 * IQR)) |(out &gt; (Q3 + 1.5 * IQR))).any(axis=1)] out.shape, df_out.shape, df_out.shape[0]/out.shape[0] ((199523, 4), (38859, 4), 0.1947595014108649) . From 199.523 observation of the selcted features, 38.859 records (19%) represent extrem values. . For weeks_worked_in_year : . df[&#39;weeks_worked_in_year&#39;].plot( kind=&#39;hist&#39;, bins=53, edgecolor=&#39;black&#39;, color=&#39;blue&#39;, title=&#39;weeks worked in a year&#39;, figsize=(10,5)) &lt;matplotlib.axes._subplots.AxesSubplot at 0x12311dd50&gt; . . For num_persons_worked_for_employer : . df[&#39;num_persons_worked_for_employer&#39;].plot( kind=&#39;hist&#39;, edgecolor=&#39;black&#39;, color=&#39;blue&#39;, title=&#39;num_persons_worked_for_employer&#39;, figsize=(7,5)) &lt;matplotlib.axes._subplots.AxesSubplot at 0x1236b43d0&gt; . . We notice an increase in the 7th bins num_persons_worked_for_employer=6. Check if this variable is capped ? . Exploratory data analysis . Starting with numerical variables : | . import seaborn as sns data_dia = df[dep_var] data = df[[&#39;age&#39;]] data = pd.concat([data_dia,data],axis=1) data = pd.melt(data,id_vars=&quot;income_level&quot;, var_name=&quot;features&quot;, value_name=&#39;value&#39;) plt.figure(figsize=(6,5)) sns.violinplot(x=&quot;features&quot;, y=&quot;value&quot;, hue=&quot;income_level&quot;, data=data,split=True, inner=&quot;quartile&quot;) plt.xticks(rotation=90) (array([0]), &lt;a list of 1 Text xticklabel objects&gt;) . . For the age feature, we can see that the medians of the income levels +/- 50k look separated. The income level of +50k with a median of 50 years old has a lower interquntile range (IQR) with value spread of 10 years. Whereas The income level of -50k has a median of 30 years old has and interquantile range (IQR) of 40 years. So, age can be good for classification. . Let’s look at the weeks_worked_in_year feature : . # get the number of income class in each week weeks_worked_in_year = df.groupby([&quot;weeks_worked_in_year&quot;, &quot;income_level&quot;]) .size() .groupby(level=0).apply(lambda x: 100*x/x.sum()).unstack() # print the percentage class for the first and last weeks weeks_worked_in_year.iloc[[0,1,2, -3,-2,-1]] income_level - 50000. 50000+. weeks_worked_in_year 0 99.379057 0.620943 1 98.275862 1.724138 2 98.908297 1.091703 50 89.279514 10.720486 51 89.010989 10.989011 52 85.199249 14.800751 weeks_worked_in_year.plot(kind=&#39;bar&#39;, stacked=True, edgecolor=&#39;black&#39;, figsize=(12,5)) &lt;matplotlib.axes._subplots.AxesSubplot at 0x125e2ded0&gt; . . We can see that the propotion of people making more than 50k a year is increasing with the number of working weeks in a given year where it can reach more than 14% for those working 52 weeks . However, the -50k level of income is representing the higher propotion regardless of the number of working weeks. We notice that among those how don’t work at all, 0.6% still make more than 50k a year. . Let’s look at num_persons_worked_for_employer : . num_persons_worked_for_employer = df.groupby([&quot;num_persons_worked_for_employer&quot;, &quot;income_level&quot;]) .size() .groupby(level=0).apply(lambda x: 100*x/x.sum()).unstack() num_persons_worked_for_employer income_level - 50000. 50000+. num_persons_worked_for_employer 0 99.379057 0.620943 1 90.942923 9.057077 2 91.687333 8.312667 3 90.763501 9.236499 4 89.776758 10.223242 5 88.980944 11.019056 6 84.990825 15.009175 num_persons_worked_for_employer.plot(kind=&#39;bar&#39;, stacked=True, edgecolor=&#39;black&#39;, figsize=(12,5)) &lt;matplotlib.axes._subplots.AxesSubplot at 0x125fbe4d0&gt; . . The proportion of +50k income level increases with the number of the num_preson_worked_for_employer where it reaches 16% for num_preson_worked_for_employer= 6. . Let’s see the average of wage_per_hour,capital_gains,capital_losses,dividends_from_stocks across the income levels : . avg = df[[&#39;wage_per_hour&#39;,&#39;capital_gains&#39;,&#39;capital_losses&#39;,&#39;dividends_from_stocks&#39;,&#39;income_level&#39;]] .groupby(&#39;income_level&#39;) .mean() avg.plot(kind=&#39;bar&#39;, title = &#39;average across income levels&#39;, figsize=(10,5)) avg wage_per_hour capital_gains capital_losses income_level - 50000. 53.692526 143.848013 27.003730 50000+. 81.640284 4830.930060 193.139557 dividends_from_stocks income_level - 50000. 107.816518 50000+. 1553.448070 . . We can see that people making more than 50k a year, have on average, higher wage per hour,higher return on capital asset and dividends from stock options. . Next, let’s analyse some categorical variables : | . # Education variable pd.crosstab(df[&#39;income_level&#39;], df[&#39;education&#39;], margins = True, normalize = &#39;columns&#39;).style.format(&#39;{:.2%}&#39;) &lt;pandas.io.formats.style.Styler at 0x125f7b790&gt; pd.crosstab(df[&#39;income_level&#39;], df[&#39;education&#39;], margins = True, normalize = &#39;columns&#39;).plot(kind=&#39;bar&#39;,stacked=True, edgecolor=&#39;black&#39;, figsize=(12,10)) plt.legend(bbox_to_anchor=(1.5, 1.0)) &lt;matplotlib.legend.Legend at 0x125b9d110&gt; . . We can see the effect of education on income level where more than 50% of Prof school degree and Doctorate degree earn more than 50k a year. On the other hand, the majority of people (more than 90%) with no degree earn less than 50k a year. . Let’s further this analysis and see the effect of education and the number of working weeks : . pd.crosstab(df[&#39;income_level&#39;], df[&#39;education&#39;], values = df[&#39;weeks_worked_in_year&#39;], aggfunc = &#39;mean&#39;).round(2) education Children Less than 1st grade 1st 2nd 3rd or 4th grade income_level - 50000. 0.0 12.69 15.93 50000+. NaN 0.00 35.00 education 5th or 6th grade 7th and 8th grade 9th grade 10th grade income_level - 50000. 18.56 11.12 13.01 16.23 50000+. 39.64 39.00 43.47 42.29 education 11th grade 12th grade no diploma High school graduate income_level - 50000. 20.42 21.59 30.88 50000+. 46.59 47.71 46.53 education Associates degree-academic program income_level - 50000. 38.48 50000+. 49.63 education Associates degree-occup /vocational income_level - 50000. 37.99 50000+. 49.46 education Prof school degree (MD DDS DVM LLB JD) income_level - 50000. 34.55 50000+. 49.88 education Some college but no degree Bachelors degree(BA AB BS) income_level - 50000. 33.35 37.24 50000+. 47.17 48.66 education Masters degree(MA MS MEng MEd MSW MBA) income_level - 50000. 36.94 50000+. 48.47 education Doctorate degree(PhD EdD) income_level - 50000. 34.51 50000+. 48.23 import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns plt.figure(figsize=(20, 4)) sns.heatmap( pd.crosstab(df[&#39;income_level&#39;], df[&#39;education&#39;], values = df[&#39;weeks_worked_in_year&#39;], aggfunc = &#39;mean&#39;).round(1) ,annot = True ,linewidths=.5 ,cmap=&quot;YlGnBu&quot; ) plt.show() . . We can see that earning more than 50k a year demands high level of education but also lot of hard work ! . Let’s analyse the effect of sex and marital_stat on income level : . pd.crosstab(df[&#39;income_level&#39;], [df[&#39;sex&#39;],df[&#39;marital_stat&#39;]], margins = True, normalize = &#39;columns&#39;).style.format(&#39;{:.2%}&#39;) &lt;pandas.io.formats.style.Styler at 0x123262f50&gt; pd.crosstab(df[&#39;income_level&#39;], [df[&#39;sex&#39;],df[&#39;marital_stat&#39;]]).plot(kind=&#39;bar&#39;,stacked=True, edgecolor=&#39;black&#39;, figsize=(12,10)) &lt;matplotlib.axes._subplots.AxesSubplot at 0x12932b290&gt; . . We can see that the highest proportion of people earning less than 50k a year are mostly female Married-A F spouse present or never married and seperated male. On the other hand, male Married-civilian spouse present represent the highest propotion on the +50k income level. . We can further the analysis more as we have got many interesting features with several modalities but for now let’s see how machine learning models can help us understanding more our data. . Data preparation . We set the feature vector and the target variable : . # setting feature vector and target variable for the train set X = df.drop([&#39;income_level&#39;], axis = 1) y = df[&#39;income_level&#39;] # setting feature vector and target variable for the test set test_x = test.drop([&#39;income_level&#39;], axis = 1) test_y = test[&#39;income_level&#39;] # Cheching the result df.shape, X.shape, y.shape ((199523, 41), (199523, 40), (199523,)) . We will keep the provided test set hidden and will use it as a realtime dataset when we make our model on production in order to avoid the risk of data snooping. . For that, we will be using a validation set derived from our training set (30%). Scikit-Learn provides a few functions to split datasets into multiple subsets in various ways. The simplest function is train_test_split(). . Since we have an imbalanced dataset, we can’t considered purely random sampling methods. For that, we do stratified sampling based on the income level. . from sklearn.model_selection import train_test_split X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.3, random_state=12, stratify=y) # Checking the train and validation set X_train.shape, X_val.shape ((139666, 40), (59857, 40)) # Checking the income level proportion y_train.value_counts(normalize = True), y_val.value_counts(normalize = True) ( - 50000. 0.937945 50000+. 0.062055 Name: income_level, dtype: float64, - 50000. 0.937935 50000+. 0.062065 Name: income_level, dtype: float64) . What if we didn’t stratify with respect to income level ? . We can compare the income level proportions in the overall dataset, in the test set generated with stratified sampling, and in a test set generated using purely random sampling. . def income_cat_proportions(data): return data[&quot;income_level&quot;].value_counts() / len(data) train_set, test_set = train_test_split(df, test_size=0.3, random_state=12) train_set, test_set_strat = train_test_split(df, test_size=0.3, random_state=12,stratify=df[&#39;income_level&#39;]) compare_props = pd.DataFrame({ &quot;Overall&quot;: income_cat_proportions(df), &quot;Stratified&quot;: income_cat_proportions(test_set_strat), &quot;Random&quot;: income_cat_proportions(test_set), }).sort_index() compare_props[&quot;Rand. %error&quot;] = 100 * compare_props[&quot;Random&quot;] / compare_props[&quot;Overall&quot;] - 100 compare_props[&quot;Strat. %error&quot;] = 100 * compare_props[&quot;Stratified&quot;] / compare_props[&quot;Overall&quot;] - 100 . As we can see, the test set generated using stratified sampling has income level proportions almost identical to those in the full dataset, whereas the test set generated using purely random sampling is skewed. . compare_props Overall Stratified Random Rand. %error Strat. %error - 50000. 0.937942 0.937935 0.939305 0.145356 -0.000701 50000+. 0.062058 0.062065 0.060695 -2.196901 0.010601 . Now that we defined our training set, It’s time to prepare the data for our machine Learning algorithms. . Data cleaning : | . We have seen previously that we don’t have any missing values. For some cataegorical features, we assumed that the ? modality is encoded for NaN values. . Handling Text and Categorical Attributes : | . Strating with the target variable income_level, we use LabelEncoder() to encode target labels with value between 0 and n_classes-1 = 1. . We have seen also that we have some ordinal variable as education and year, so we use OrdinalEncoder to encode the categorical features as an integer array. The results in a single column of integers (0 to n_categories - 1) per feature. . Since the remaining categorical features have several modalities per feature, we use also OrdinalEncoder instead of OneHotEncoder. . Working with OneHotEncoder leads, in our case, to high memory consumption. We can combine OneHotEncoder and PCA : The benefit in PCA is that combination of N attributes is better than any individual attribute. And the disadvantage is in harder explanation what exactly that PCA component means. Therefore, for this work, we will sacrifice a bit of predictive power to get more understandable model. . # categorical variables encoding from sklearn.preprocessing import LabelEncoder, OrdinalEncoder # For the traget varaible le = LabelEncoder() y_train = le.fit_transform(y_train) #fit on training set y_val = le.transform(y_val) test_y = le.transform(test_y) # For categorical features : Or = OrdinalEncoder(handle_unknown=&#39;use_encoded_value&#39;, unknown_value = -1) for c in cat : X_train[c] = Or.fit_transform(np.array(X_train[c]).reshape(-1,1).astype(str)) #fit on training set X_val[c] = Or.transform(np.array(X_val[c]).reshape(-1,1).astype(str)) test_x[c] = Or.transform(np.array(test_x[c]).reshape(-1,1).astype(str)) # Cheking categorical features encoding X_train[cat].head(3) class_of_worker detailed_industry_recode detailed_occupation_recode 88634 4.0 31.0 14.0 148296 8.0 37.0 12.0 163953 3.0 0.0 0.0 education enroll_in_edu_inst_last_wk marital_stat 88634 9.0 2.0 2.0 148296 12.0 2.0 2.0 163953 10.0 2.0 4.0 major_industry_code major_occupation_code race hispanic_origin 88634 2.0 0.0 4.0 0.0 148296 12.0 2.0 4.0 0.0 163953 14.0 6.0 4.0 0.0 ... migration_prev_res_in_sunbelt family_members_under_18 88634 ... 0.0 4.0 148296 ... 2.0 4.0 163953 ... 0.0 0.0 country_of_birth_father country_of_birth_mother 88634 40.0 40.0 148296 40.0 40.0 163953 40.0 40.0 country_of_birth_self citizenship own_business_or_self_employed 88634 40.0 4.0 2.0 148296 40.0 4.0 0.0 163953 40.0 4.0 0.0 fill_inc_questionnaire_for_veteran&#39;s_admin veterans_benefits year 88634 1.0 2.0 0.0 148296 1.0 2.0 0.0 163953 1.0 0.0 0.0 [3 rows x 33 columns] # Cheking target feature encoding set(y_train) {0, 1} . Feature Scaling : | . We saw previously that out numerical inputs have different scales like the weeks_worked_in_year and capital_gains. We will be using StandardScaler since standardization is much less affected by outliers. . from sklearn.preprocessing import StandardScaler scaler = StandardScaler() for c in cont: X_train[c] = scaler.fit_transform(np.array(X_train[c]).reshape(-1,1)) # fir on the train set X_val[c] = scaler.transform(np.array(X_val[c]).reshape(-1,1)) test_x[c] = scaler.transform(np.array(test_x[c]).reshape(-1,1)) #checking the standardization X_train[cont].head(3) age wage_per_hour capital_gains capital_losses 88634 0.334269 -0.201648 -0.092139 -0.137611 148296 0.468715 -0.201648 -0.092139 -0.137611 163953 -1.368713 -0.201648 -0.092139 -0.137611 dividends_from_stocks num_persons_worked_for_employer 88634 -0.069026 1.705234 148296 -0.098703 -0.405865 163953 -0.098703 -0.828085 weeks_worked_in_year 88634 1.177773 148296 1.177773 163953 -0.950488 . So far, we have handled the categorical columns and the numerical columns : . # Checking the training set with pd.option_context(&#39;display.max_rows&#39;, None, &#39;display.max_columns&#39;, None): display(X_train.head(3)) X_train.shape, X_val.shape, test_x.shape age class_of_worker detailed_industry_recode 88634 0.334269 4.0 31.0 148296 0.468715 8.0 37.0 163953 -1.368713 3.0 0.0 detailed_occupation_recode education wage_per_hour 88634 14.0 9.0 -0.201648 148296 12.0 12.0 -0.201648 163953 0.0 10.0 -0.201648 enroll_in_edu_inst_last_wk marital_stat major_industry_code 88634 2.0 2.0 2.0 148296 2.0 2.0 12.0 163953 2.0 4.0 14.0 major_occupation_code race hispanic_origin sex 88634 0.0 4.0 0.0 1.0 148296 2.0 4.0 0.0 0.0 163953 6.0 4.0 0.0 1.0 member_of_a_labor_union reason_for_unemployment 88634 1.0 3.0 148296 1.0 3.0 163953 1.0 3.0 full_or_part_time_employment_stat capital_gains capital_losses 88634 1.0 -0.092139 -0.137611 148296 0.0 -0.092139 -0.137611 163953 0.0 -0.092139 -0.137611 dividends_from_stocks tax_filer_stat region_of_previous_residence 88634 -0.069026 2.0 3.0 148296 -0.098703 2.0 3.0 163953 -0.098703 4.0 3.0 state_of_previous_residence detailed_household_and_family_stat 88634 36.0 37.0 148296 36.0 37.0 163953 36.0 8.0 detailed_household_summary_in_household migration_code-change_in_msa 88634 7.0 0.0 148296 7.0 7.0 163953 2.0 0.0 migration_code-change_in_reg migration_code-move_within_reg 88634 0.0 0.0 148296 6.0 7.0 163953 0.0 0.0 live_in_this_house_1_year_ago migration_prev_res_in_sunbelt 88634 1.0 0.0 148296 2.0 2.0 163953 1.0 0.0 num_persons_worked_for_employer family_members_under_18 88634 1.705234 4.0 148296 -0.405865 4.0 163953 -0.828085 0.0 country_of_birth_father country_of_birth_mother 88634 40.0 40.0 148296 40.0 40.0 163953 40.0 40.0 country_of_birth_self citizenship own_business_or_self_employed 88634 40.0 4.0 2.0 148296 40.0 4.0 0.0 163953 40.0 4.0 0.0 fill_inc_questionnaire_for_veteran&#39;s_admin veterans_benefits 88634 1.0 2.0 148296 1.0 2.0 163953 1.0 0.0 weeks_worked_in_year year 88634 1.177773 0.0 148296 1.177773 0.0 163953 -0.950488 0.0 ((139666, 40), (59857, 40), (99762, 40)) . Data modeling . Selecting a Performance Measure : | . Accuracy is the simplest way to measure the effectiveness of a classification task, and it’s the percentage of correct predictions over all predictions. In other words, in a binary classification task, you can calculate this by adding the number of True Positives (TPs) and True Negatives (TNs) and dividing them by a tally of all predictions made. As with regression metrics, you can measure accuracy for both train and test to gauge overfitting. . But, we can get an accuracy of 94%, which sounds pretty good, but it turns out we are always predicting -50k! In other words, even if we get high accuracy, it is meaningless unless we are predicting accurately for the least represented class, +50k. . For this reasing, we will be using F1-score. The F1-score is also called the harmonic average of precision and recall because it’s calculated like this: 2TP / 2TP + FP + FN. Since it includes both precision and recall metrics, which pertain to the proportion of true positives, it’s a good metric choice to use when the dataset is imbalanced, and we don’t prefer either precision or recall. . Base model : | . Let’s start with Decision tree ensembles. . A decision tree asks a series of binary (that is, yes or no) questions about the data. After each question the data at that part of the tree is split between a “yes” and a “no” branch. After one or more questions, either a prediction can be made on the basis of all previous answers or another question is required. . We illustarte a tree classification using 4 leaf nodes. . from sklearn.tree import DecisionTreeClassifier, plot_tree m = DecisionTreeClassifier(max_leaf_nodes=4, random_state=14) # to plot the tree classification m.fit(X_train, y_train) DecisionTreeClassifier(max_leaf_nodes=4, random_state=14) # to get the class output m.classes_ array([0, 1]) !pip install pydotplus Requirement already satisfied: pydotplus in /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages (2.0.2) Requirement already satisfied: pyparsing&gt;=2.0.1 in /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages (from pydotplus) (3.0.4) !pip install graphviz Requirement already satisfied: graphviz in /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages (0.16) from sklearn.tree import export_graphviz from io import StringIO from IPython.display import Image import pydotplus feature_cols = X_train.columns dot_data = StringIO() export_graphviz(m, out_file=dot_data, filled=True, rounded=True, special_characters=True,feature_names = feature_cols, class_names=[&#39;0&#39;,&#39;1&#39;]) graph = pydotplus.graph_from_dot_data(dot_data.getvalue()) Image(graph.create_png()) . . The top node represents the initial model before any splits have been done, when all the data is in the initial income levels. This is the simplest possible model. It is the result of asking zero questions and will always predict the more represented class which is -50k. We use the Gini method to create split points. The strategy is to select each pair of adjacent values as a possible split-point and the point with smaller gini index chosen as the splitting point. In our case, the capital gains at 1.47 was choosen first. . Moving down and to the left, this node shows us that there were 130,999 records for income level of -50k where capital gains was less than 1.47. The class predicted is -50k in this case. Moving down and to the right from the initial model takes us to the records where capital gains was greater than 1.47. The class predicted is +50k in this case where 1370 records have an income of +50k and capital gains &gt;0.4 . The bottom row contains our leaf nodes: the nodes with no answers coming out of them, because there are no more questions to be answered. . Returning back to the top node after the first decision point, we can see that a second binary decision split has been made, based on asking whether weeks_worked_per_year is less than or equal to 0.9. For the group where this is true, the class predicted is -50k with a gini of 0.019 and there are 85,411 records. For the records where this decision is false, the class predicted is -50k with a gini of 0.019, and there are 52,361 records. So again, we can see that the decision tree algorithm has successfully split out more records into two more groups which differ in gini value significantly. . Now, let’s run our base model : . m = DecisionTreeClassifier(random_state=14) m.fit(X_train, y_train) DecisionTreeClassifier(random_state=14) . We evaluate the model on our validation set using accuracy, recall and f1 score : . from sklearn.metrics import accuracy_score, f1_score, recall_score # on the train set accuracy_score(y_train,m.predict(X_train)) , recall_score(y_train,m.predict(X_train)), f1_score(y_train,m.predict(X_train), average=&#39;binary&#39;, pos_label=1) (0.9996348431257428, 0.9943463712934117, 0.997049806212761) # on the valid set accuracy_score(y_val,m.predict(X_val)) , recall_score(y_val,m.predict(X_val)), f1_score(y_val,m.predict(X_val), average=&#39;binary&#39;, pos_label=1) (0.9307683311893346, 0.4888290713324361, 0.4670781893004115) . It’s seems that we are doing badly on the validation set. Let’s see houw many leaf nodes we got : . m.get_n_leaves(), len(X_train) (8005, 139666) . Sklearn’s default settings allow it to continue splitting nodes until there is only one item in each leaf node. Let’s change the stopping rule to tell sklearn to ensure every leaf node contains at least 25 records: . m = DecisionTreeClassifier(min_samples_leaf=25,random_state=14) m.fit(X_train, y_train) # on the train set accuracy_score(y_train,m.predict(X_train)) , recall_score(y_train,m.predict(X_train)), f1_score(y_train,m.predict(X_train), average=&#39;binary&#39;, pos_label=1) (0.9571048071828505, 0.46198223145263645, 0.5720408600614331) # on the valid set accuracy_score(y_val,m.predict(X_val)) , recall_score(y_val,m.predict(X_val)), f1_score(y_val,m.predict(X_val), average=&#39;binary&#39;, pos_label=1) (0.9504986885410228, 0.42449528936742936, 0.515612228216446) . That looks much better. Let’s check the number of leaves again: . m.get_n_leaves(), len(X_train) (1533, 139666) . We got less leaf nodes than before. So, the more we increase the number of leaf nodes, the more is the possibility of overfitting. . Building a decision tree is a good way to create a model of our data. It is very flexible, since it can clearly handle nonlinear relationships and interactions between variables. But we can see there is a fundamental compromise between how well it generalizes (which we can achieve by creating small trees) and how accurate it is on the training set (which we can achieve by using large trees). . So how do we get the best of both worlds? . Ensembling : | . An an example of an Ensemble method is Random Forest : we can train a group of Decision Tree classifiers, each on a different random subset of the training set. The process of subseting the data is called bagging done with max_samples hyperparameter ( we set it at 100.00 samples) and the ramdom selection process this called bootsraping done by setting bootstrap = True. . With bagging, some instances may be sampled several times for any given predictor, while others may not be sampled at all. The remaining sampled are called out-of-bag (oob) instances used as validation set in the training process and done by setting oob_score=True. . We train a Random Forest classifier with 50 trees (each limited to minimum 5 samples per leaf). and instead of searching for the very best feature when splitting a node, we searches for the best feature among a random subset of 50% of our initial features. . from sklearn.ensemble import RandomForestClassifier rf = RandomForestClassifier(n_estimators = 50, max_samples=100_000, max_features=0.5, min_samples_leaf= 5, bootstrap= True,oob_score = True,random_state=14) %%time rf.fit(X_train,y_train) /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages/sklearn/base.py:446: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names &quot;X does not have valid feature names, but&quot; CPU times: user 29.1 s, sys: 643 ms, total: 29.7 s Wall time: 32.8 s RandomForestClassifier(max_features=0.5, max_samples=100000, min_samples_leaf=5, n_estimators=50, oob_score=True, random_state=14) # on the train set accuracy_score(y_train,rf.predict(X_train)) , recall_score(y_train,rf.predict(X_train)), f1_score(y_train,rf.predict(X_train), average=&#39;binary&#39;, pos_label=1) (0.9670571219910358, 0.5334025614399446, 0.6677258611973712) # on the valid set accuracy_score(y_val,rf.predict(X_val)) , recall_score(y_val,rf.predict(X_val)), f1_score(y_val,rf.predict(X_val), average=&#39;binary&#39;, pos_label=1) (0.9546418965200394, 0.41668909825033645, 0.5327826535880227) . Looking at what happens to the oob error rate as we add more and more trees, we you can see that the improvement levels off quite a bit after around 40 trees: . scores =[] for k in range(1, 50): rfc = RandomForestClassifier(n_estimators = k, max_samples=100_000, max_features=0.5, min_samples_leaf= 5, bootstrap= True,oob_score = True,random_state=14) rfc.fit(X_train, y_train) #y_pred = rfc.predict(X_val) #scores.append(accuracy_score(y_test, y_pred)) oob_score_ oob_error = 1 - rfc.oob_score_ scores.append(oob_error) import matplotlib.pyplot as plt %matplotlib inline # plot the relationship between K and testing accuracy # plt.plot(x_axis, y_axis) plt.plot(range(1, 50), scores) plt.xlabel(&#39;Value of n_estimators for Random Forest Classifier&#39;) #plt.ylabel(&#39;Testing Accuracy&#39;) plt.ylabel(&#39;OOB error rate&#39;) /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages/sklearn/base.py:446: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names &quot;X does not have valid feature names, but&quot; /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/_forest.py:554: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates. UserWarning, /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages/sklearn/base.py:446: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names &quot;X does not have valid feature names, but&quot; /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/_forest.py:554: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates. UserWarning, /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages/sklearn/base.py:446: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names &quot;X does not have valid feature names, but&quot; /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/_forest.py:554: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates. UserWarning, /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages/sklearn/base.py:446: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names &quot;X does not have valid feature names, but&quot; /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/_forest.py:554: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates. UserWarning, /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages/sklearn/base.py:446: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names &quot;X does not have valid feature names, but&quot; /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/_forest.py:554: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates. UserWarning, /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages/sklearn/base.py:446: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names &quot;X does not have valid feature names, but&quot; /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/_forest.py:554: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates. UserWarning, /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages/sklearn/base.py:446: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names &quot;X does not have valid feature names, but&quot; /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/_forest.py:554: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates. UserWarning, /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages/sklearn/base.py:446: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names &quot;X does not have valid feature names, but&quot; /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/_forest.py:554: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates. UserWarning, /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages/sklearn/base.py:446: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names &quot;X does not have valid feature names, but&quot; /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/_forest.py:554: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates. UserWarning, /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages/sklearn/base.py:446: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names &quot;X does not have valid feature names, but&quot; /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/_forest.py:554: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates. UserWarning, /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages/sklearn/base.py:446: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names &quot;X does not have valid feature names, but&quot; /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/_forest.py:554: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates. UserWarning, /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages/sklearn/base.py:446: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names &quot;X does not have valid feature names, but&quot; /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/_forest.py:554: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates. UserWarning, /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages/sklearn/base.py:446: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names &quot;X does not have valid feature names, but&quot; /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/_forest.py:554: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates. UserWarning, /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages/sklearn/base.py:446: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names &quot;X does not have valid feature names, but&quot; /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/_forest.py:554: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates. UserWarning, /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages/sklearn/base.py:446: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names &quot;X does not have valid feature names, but&quot; /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/_forest.py:554: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates. UserWarning, /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages/sklearn/base.py:446: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names &quot;X does not have valid feature names, but&quot; /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/_forest.py:554: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates. UserWarning, /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages/sklearn/base.py:446: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names &quot;X does not have valid feature names, but&quot; /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/_forest.py:554: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates. UserWarning, /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages/sklearn/base.py:446: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names &quot;X does not have valid feature names, but&quot; /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/_forest.py:554: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates. UserWarning, /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages/sklearn/base.py:446: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names &quot;X does not have valid feature names, but&quot; /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/_forest.py:554: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates. UserWarning, /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages/sklearn/base.py:446: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names &quot;X does not have valid feature names, but&quot; /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/_forest.py:554: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates. UserWarning, /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages/sklearn/base.py:446: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names &quot;X does not have valid feature names, but&quot; /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/_forest.py:554: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates. UserWarning, /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages/sklearn/base.py:446: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names &quot;X does not have valid feature names, but&quot; /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages/sklearn/base.py:446: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names &quot;X does not have valid feature names, but&quot; /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages/sklearn/base.py:446: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names &quot;X does not have valid feature names, but&quot; /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages/sklearn/base.py:446: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names &quot;X does not have valid feature names, but&quot; /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages/sklearn/base.py:446: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names &quot;X does not have valid feature names, but&quot; /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages/sklearn/base.py:446: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names &quot;X does not have valid feature names, but&quot; /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages/sklearn/base.py:446: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names &quot;X does not have valid feature names, but&quot; /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages/sklearn/base.py:446: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names &quot;X does not have valid feature names, but&quot; /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages/sklearn/base.py:446: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names &quot;X does not have valid feature names, but&quot; /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages/sklearn/base.py:446: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names &quot;X does not have valid feature names, but&quot; /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages/sklearn/base.py:446: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names &quot;X does not have valid feature names, but&quot; /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages/sklearn/base.py:446: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names &quot;X does not have valid feature names, but&quot; /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages/sklearn/base.py:446: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names &quot;X does not have valid feature names, but&quot; /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages/sklearn/base.py:446: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names &quot;X does not have valid feature names, but&quot; /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages/sklearn/base.py:446: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names &quot;X does not have valid feature names, but&quot; /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages/sklearn/base.py:446: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names &quot;X does not have valid feature names, but&quot; /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages/sklearn/base.py:446: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names &quot;X does not have valid feature names, but&quot; /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages/sklearn/base.py:446: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names &quot;X does not have valid feature names, but&quot; /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages/sklearn/base.py:446: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names &quot;X does not have valid feature names, but&quot; /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages/sklearn/base.py:446: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names &quot;X does not have valid feature names, but&quot; /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages/sklearn/base.py:446: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names &quot;X does not have valid feature names, but&quot; /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages/sklearn/base.py:446: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names &quot;X does not have valid feature names, but&quot; /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages/sklearn/base.py:446: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names &quot;X does not have valid feature names, but&quot; /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages/sklearn/base.py:446: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names &quot;X does not have valid feature names, but&quot; /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages/sklearn/base.py:446: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names &quot;X does not have valid feature names, but&quot; /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages/sklearn/base.py:446: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names &quot;X does not have valid feature names, but&quot; /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages/sklearn/base.py:446: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names &quot;X does not have valid feature names, but&quot; /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages/sklearn/base.py:446: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names &quot;X does not have valid feature names, but&quot; Text(0, 0.5, &#39;OOB error rate&#39;) . . Let’s try to improve our model : . We may ask which columns are the strongest predictors, which can we ignore? . It’s not normally enough just to know that a model can make accurate predictions—we also want to know how it’s making predictions. Feature importance gives us insight into this. We can get these directly from sklearn’s random forest by looking in the feature_importances_ attribute. Here’s a simple function we can use to pop them into a DataFrame and sort them: . def rf_feat_importance(m, df): return pd.DataFrame({&#39;cols&#39;:df.columns, &#39;imp&#39;:m.feature_importances_} ).sort_values(&#39;imp&#39;, ascending=False) fi = rf_feat_importance(rf, X_train) fi[:14] cols imp 16 capital_gains 0.161368 18 dividends_from_stocks 0.132382 0 age 0.094106 3 detailed_occupation_recode 0.081452 38 weeks_worked_in_year 0.076202 2 detailed_industry_recode 0.056239 12 sex 0.054394 4 education 0.049652 17 capital_losses 0.047552 29 num_persons_worked_for_employer 0.040222 9 major_occupation_code 0.039596 8 major_industry_code 0.031488 1 class_of_worker 0.014878 19 tax_filer_stat 0.013542 . The feature importances for our model show that the first few most important columns have much higher importance scores than the rest, with (not surprisingly) capital_gains and dividends_from_stocks being at the top of the list. . A plot of the feature importances shows the relative importances more clearly: . def plot_fi(fi): return fi.plot(&#39;cols&#39;, &#39;imp&#39;, &#39;barh&#39;, figsize=(12,7), legend=False) plot_fi(fi[:30]); . . The way these importances are calculated is quite simple yet elegant. The feature importance algorithm loops through each tree, and then recursively explores each branch. At each branch, it looks to see what feature was used for that split, and how much the model improves as a result of that split. The improvement (weighted by the number of rows in that group) is added to the importance score for that feature. This is summed across all branches of all trees, and finally the scores are normalized such that they add to 1. . It seems likely that we could use just a subset of the columns by removing the variables of low importance and still get good results. Let’s try just keeping those with a feature importance greater than 0.005: . to_keep = fi[fi.imp&gt;0.005].cols len(to_keep) 25 . We can retrain our model using just this subset of the columns: . X_train_imp = X_train[to_keep] X_val_imp = X_val[to_keep] m = RandomForestClassifier(n_estimators = 50, max_samples=100_000, max_features=0.5, min_samples_leaf= 5, bootstrap= True,oob_score = True,random_state=14) m.fit(X_train_imp,y_train) /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages/sklearn/base.py:446: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names &quot;X does not have valid feature names, but&quot; RandomForestClassifier(max_features=0.5, max_samples=100000, min_samples_leaf=5, n_estimators=50, oob_score=True, random_state=14) # on the train set accuracy_score(y_train,m.predict(X_train_imp)) , recall_score(y_train,m.predict(X_train_imp)), f1_score(y_train,m.predict(X_train_imp), average=&#39;binary&#39;, pos_label=1) (0.9670857617458795, 0.5334025614399446, 0.6679188037275157) # on the valid set accuracy_score(y_val,m.predict(X_val_imp)) , recall_score(y_val,m.predict(X_val_imp)), f1_score(y_val,m.predict(X_val_imp), average=&#39;binary&#39;, pos_label=1) (0.9543077668443123, 0.4142664872139973, 0.5295028384655084) . Our accuracy is about the same, but we have far fewer columns to study: . len(X_train.columns), len(X_train_imp.columns) (40, 25) . We’ve found that generally the first step to improving a model is simplifying it—48 columns was too many for us to study them all in depth! Furthermore, in practice often a simpler, more interpretable model is easier to roll out and maintain. . This also makes our feature importance plot easier to interpret. Let’s look at it again: . plot_fi(rf_feat_importance(m, X_train_imp)); . . Let’s see if we have redundent feature in our model by determining their similarities : . Determining Similarity: The most similar pairs are found by calculating the rank correlation, which means that all the values are replaced with their rank (i.e., first, second, third, etc. within the column), and then the correlation is calculated. . import scipy from scipy.cluster import hierarchy as hc def cluster_columns(df, figsize=(10,6), font_size=12): corr = np.round(scipy.stats.spearmanr(df).correlation, 4) corr_condensed = hc.distance.squareform(1-corr) z = hc.linkage(corr_condensed, method=&#39;average&#39;) fig = plt.figure(figsize=figsize) hc.dendrogram(z, labels=df.columns, orientation=&#39;left&#39;, leaf_font_size=font_size) plt.show() cluster_columns(X_train_imp) . . Looking good! This is really not much worse than the model with all the fields. Let’s create DataFrames without these columns, and save them: . X_train_final = X_train_imp # train X_val_final = X_val_imp # valid test_x_final = test_x[to_keep] # test set X_train_final.shape , X_val_final.shape, test_x_final.shape ((139666, 25), (59857, 25), (99762, 25)) . Model Assesment : . We have seen the DecisionTreeClassifier as our basemodel, then we tried RandomForestClassifier and finaly we tried to optimize so we can have less features for better interpretation. . Here is the model metrics on our validation set : . models_metrics = {&#39;DTC&#39;: [0.95, 0.42, 0.51], &#39;RF&#39;: [0.96, 0.42, 0.53], &#39;RF_less_feat&#39;: [0.95, 0.41, 0.53] } df = pd.DataFrame(data = models_metrics) df.rename(index={0:&#39;Accuracy&#39;,1:&#39;Recall&#39;, 2: &#39;F1 score&#39;}, inplace=True) ax = df.plot(kind=&#39;bar&#39;, figsize = (10,5), color = [&#39;gold&#39;, &#39;lightgreen&#39;,&#39;lightcoral&#39;], rot = 0, title =&#39;Models performance&#39;, edgecolor = &#39;grey&#39;, alpha = 0.5) for p in ax.patches: ax.annotate(str(p.get_height()), (p.get_x() * 1.01, p.get_height() * 1.0005)) plt.show() . . Based on F1 score, we select the RandomForestClassifier with 25 features as our best model. . Let’s see the experiment results* of this model : . The precision_recall_curve and roc_curve are useful tools to visualize the sensitivity-specificty tradeoff in the classifier. They help inform a data scientist where to set the decision threshold of the model to maximize either sensitivity or specificity. This is called the operating point of the model. . from sklearn.metrics import roc_curve, precision_recall_curve, auc, make_scorer, recall_score, accuracy_score, precision_score, confusion_matrix # We create an array of the class probabilites called y_scores y_scores = m.predict_proba(X_val_imp)[:, 1] # we enerate the precision-recall curve for the classifier: p, r, thresholds = precision_recall_curve(y_val, y_scores) # We calculate the F1 scores f1_scores = 2*r*p/(r+p) . Let’s plot the decision chart of our model : . def plot_precision_recall_vs_threshold(precisions, recalls, thresholds): plt.figure(figsize=(8, 8)) plt.title(&quot;Precision, Recall Scores and F1 scores as a function of the decision threshold&quot;) plt.plot(thresholds, precisions[:-1], &quot;b--&quot;, label=&quot;Precision&quot;) plt.plot(thresholds, recalls[:-1], &quot;g-&quot;, label=&quot;Recall&quot;) plt.plot(thresholds, f1_scores[:-1], &quot;r-&quot;, label=&quot;F1-score&quot;) plt.ylabel(&quot;Score&quot;) plt.xlabel(&quot;Decision Threshold&quot;) plt.legend(loc=&#39;best&#39;) plot_precision_recall_vs_threshold(p, r, thresholds) . . We can see that the the optimal threshold to achieve the highest F1 score is set at 0.30 with 59% F1-score. . print(&#39;Best threshold: &#39;, thresholds[np.argmax(f1_scores)]) print(&#39;Best F1-Score: &#39;, np.max(f1_scores)) Best threshold: 0.30834595959595956 Best F1-Score: 0.5950888192267503 . Let’s creat an animated confusion matrix where the users get to choose the threesholds and we dislpay the confusion matrix and recall vs precision curve : . pip install ipywidgets Requirement already satisfied: ipywidgets in /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages (7.6.5) Requirement already satisfied: ipython-genutils~=0.2.0 in /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages (from ipywidgets) (0.2.0) Requirement already satisfied: ipykernel&gt;=4.5.1 in /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages (from ipywidgets) (5.3.4) Requirement already satisfied: ipython&gt;=4.0.0 in /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages (from ipywidgets) (7.22.0) Requirement already satisfied: jupyterlab-widgets&gt;=1.0.0 in /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages (from ipywidgets) (1.0.0) Requirement already satisfied: nbformat&gt;=4.2.0 in /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages (from ipywidgets) (5.1.3) Requirement already satisfied: traitlets&gt;=4.3.1 in /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages (from ipywidgets) (5.1.1) Requirement already satisfied: widgetsnbextension~=3.5.0 in /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages (from ipywidgets) (3.5.1) Requirement already satisfied: tornado&gt;=4.2 in /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages (from ipykernel&gt;=4.5.1-&gt;ipywidgets) (6.1) Requirement already satisfied: jupyter-client in /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages (from ipykernel&gt;=4.5.1-&gt;ipywidgets) (7.0.1) Requirement already satisfied: appnope in /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages (from ipykernel&gt;=4.5.1-&gt;ipywidgets) (0.1.2) Requirement already satisfied: decorator in /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages (from ipython&gt;=4.0.0-&gt;ipywidgets) (5.1.0) Requirement already satisfied: pygments in /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages (from ipython&gt;=4.0.0-&gt;ipywidgets) (2.10.0) Requirement already satisfied: backcall in /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages (from ipython&gt;=4.0.0-&gt;ipywidgets) (0.2.0) Requirement already satisfied: pexpect&gt;4.3 in /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages (from ipython&gt;=4.0.0-&gt;ipywidgets) (4.8.0) Requirement already satisfied: setuptools&gt;=18.5 in /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages (from ipython&gt;=4.0.0-&gt;ipywidgets) (58.0.4) Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,&lt;3.1.0,&gt;=2.0.0 in /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages (from ipython&gt;=4.0.0-&gt;ipywidgets) (3.0.20) Requirement already satisfied: jedi&gt;=0.16 in /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages (from ipython&gt;=4.0.0-&gt;ipywidgets) (0.18.1) Requirement already satisfied: pickleshare in /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages (from ipython&gt;=4.0.0-&gt;ipywidgets) (0.7.5) Requirement already satisfied: parso&lt;0.9.0,&gt;=0.8.0 in /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages (from jedi&gt;=0.16-&gt;ipython&gt;=4.0.0-&gt;ipywidgets) (0.8.3) Requirement already satisfied: jupyter-core in /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages (from nbformat&gt;=4.2.0-&gt;ipywidgets) (4.9.1) Requirement already satisfied: jsonschema!=2.5.0,&gt;=2.4 in /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages (from nbformat&gt;=4.2.0-&gt;ipywidgets) (3.2.0) Requirement already satisfied: pyrsistent&gt;=0.14.0 in /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages (from jsonschema!=2.5.0,&gt;=2.4-&gt;nbformat&gt;=4.2.0-&gt;ipywidgets) (0.18.0) Requirement already satisfied: importlib-metadata in /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages (from jsonschema!=2.5.0,&gt;=2.4-&gt;nbformat&gt;=4.2.0-&gt;ipywidgets) (4.8.1) Requirement already satisfied: attrs&gt;=17.4.0 in /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages (from jsonschema!=2.5.0,&gt;=2.4-&gt;nbformat&gt;=4.2.0-&gt;ipywidgets) (21.2.0) Requirement already satisfied: six&gt;=1.11.0 in /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages (from jsonschema!=2.5.0,&gt;=2.4-&gt;nbformat&gt;=4.2.0-&gt;ipywidgets) (1.16.0) Requirement already satisfied: ptyprocess&gt;=0.5 in /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages (from pexpect&gt;4.3-&gt;ipython&gt;=4.0.0-&gt;ipywidgets) (0.7.0) Requirement already satisfied: wcwidth in /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,&lt;3.1.0,&gt;=2.0.0-&gt;ipython&gt;=4.0.0-&gt;ipywidgets) (0.2.5) Requirement already satisfied: notebook&gt;=4.4.1 in /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages (from widgetsnbextension~=3.5.0-&gt;ipywidgets) (6.4.5) Requirement already satisfied: prometheus-client in /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages (from notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.5.0-&gt;ipywidgets) (0.11.0) Requirement already satisfied: pyzmq&gt;=17 in /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages (from notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.5.0-&gt;ipywidgets) (22.2.1) Requirement already satisfied: jinja2 in /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages (from notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.5.0-&gt;ipywidgets) (2.11.3) Requirement already satisfied: nbconvert in /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages (from notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.5.0-&gt;ipywidgets) (6.1.0) Requirement already satisfied: terminado&gt;=0.8.3 in /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages (from notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.5.0-&gt;ipywidgets) (0.9.4) Requirement already satisfied: argon2-cffi in /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages (from notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.5.0-&gt;ipywidgets) (20.1.0) Requirement already satisfied: Send2Trash&gt;=1.5.0 in /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages (from notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.5.0-&gt;ipywidgets) (1.8.0) Requirement already satisfied: python-dateutil&gt;=2.1 in /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages (from jupyter-client-&gt;ipykernel&gt;=4.5.1-&gt;ipywidgets) (2.8.2) Requirement already satisfied: nest-asyncio&gt;=1.5 in /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages (from jupyter-client-&gt;ipykernel&gt;=4.5.1-&gt;ipywidgets) (1.5.1) Requirement already satisfied: entrypoints in /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages (from jupyter-client-&gt;ipykernel&gt;=4.5.1-&gt;ipywidgets) (0.3) Requirement already satisfied: cffi&gt;=1.0.0 in /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages (from argon2-cffi-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.5.0-&gt;ipywidgets) (1.15.0) Requirement already satisfied: pycparser in /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages (from cffi&gt;=1.0.0-&gt;argon2-cffi-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.5.0-&gt;ipywidgets) (2.21) Requirement already satisfied: typing-extensions&gt;=3.6.4 in /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages (from importlib-metadata-&gt;jsonschema!=2.5.0,&gt;=2.4-&gt;nbformat&gt;=4.2.0-&gt;ipywidgets) (3.10.0.2) Requirement already satisfied: zipp&gt;=0.5 in /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages (from importlib-metadata-&gt;jsonschema!=2.5.0,&gt;=2.4-&gt;nbformat&gt;=4.2.0-&gt;ipywidgets) (3.6.0) Requirement already satisfied: MarkupSafe&gt;=0.23 in /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages (from jinja2-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.5.0-&gt;ipywidgets) (2.0.1) Requirement already satisfied: jupyterlab-pygments in /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages (from nbconvert-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.5.0-&gt;ipywidgets) (0.1.2) Requirement already satisfied: mistune&lt;2,&gt;=0.8.1 in /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages (from nbconvert-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.5.0-&gt;ipywidgets) (0.8.4) Requirement already satisfied: testpath in /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages (from nbconvert-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.5.0-&gt;ipywidgets) (0.5.0) Requirement already satisfied: pandocfilters&gt;=1.4.1 in /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages (from nbconvert-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.5.0-&gt;ipywidgets) (1.4.3) Requirement already satisfied: defusedxml in /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages (from nbconvert-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.5.0-&gt;ipywidgets) (0.7.1) Requirement already satisfied: bleach in /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages (from nbconvert-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.5.0-&gt;ipywidgets) (4.0.0) Requirement already satisfied: nbclient&lt;0.6.0,&gt;=0.5.0 in /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages (from nbconvert-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.5.0-&gt;ipywidgets) (0.5.3) Requirement already satisfied: async-generator in /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages (from nbclient&lt;0.6.0,&gt;=0.5.0-&gt;nbconvert-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.5.0-&gt;ipywidgets) (1.10) Requirement already satisfied: webencodings in /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages (from bleach-&gt;nbconvert-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.5.0-&gt;ipywidgets) (0.5.1) Requirement already satisfied: packaging in /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages (from bleach-&gt;nbconvert-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.5.0-&gt;ipywidgets) (21.0) Requirement already satisfied: pyparsing&gt;=2.0.2 in /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages (from packaging-&gt;bleach-&gt;nbconvert-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.5.0-&gt;ipywidgets) (3.0.4) Note: you may need to restart the kernel to use updated packages. import ipywidgets as widgets import itertools def plot_confusion_matrix(cm, classes, normalize = False, title = &#39;Confusion matrix&quot;&#39;, cmap = plt.cm.Blues) : plt.imshow(cm, interpolation = &#39;nearest&#39;, cmap = cmap) plt.title(title) plt.colorbar() tick_marks = np.arange(len(classes)) plt.xticks(tick_marks, classes, rotation = 0) plt.yticks(tick_marks, classes) thresh = cm.max() / 2. for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])) : plt.text(j, i, cm[i, j], horizontalalignment = &#39;center&#39;, color = &#39;white&#39; if cm[i, j] &gt; thresh else &#39;black&#39;) plt.tight_layout() plt.ylabel(&#39;True label&#39;) plt.xlabel(&#39;Predicted label&#39;) def adjusted_classes(y_scores, t): &quot;&quot;&quot; This function adjusts class predictions based on the prediction threshold (t). Will only work for binary classification problems. &quot;&quot;&quot; return [1 if y &gt;= t else 0 for y in y_scores] def precision_recall_threshold(p, r, thresholds, t=0.5): &quot;&quot;&quot; plots the precision recall curve and shows the current value for each by identifying the classifier&#39;s threshold (t). &quot;&quot;&quot; # generate new class predictions based on the adjusted_classes # function above and view the resulting confusion matrix. y_pred_adj = adjusted_classes(y_scores, t) cm = confusion_matrix(y_val, y_pred_adj) class_names = [0,1] plt.figure() plot_confusion_matrix(cm, classes=class_names, title=&#39;RF Confusion matrix&#39;) plt.figure(figsize=(8,8)) plt.title(&quot;Precision and Recall curve ^ = current threshold&quot;) plt.step(r, p, color=&#39;b&#39;, alpha=0.2, where=&#39;post&#39;) plt.fill_between(r, p, step=&#39;post&#39;, alpha=0.2, color=&#39;b&#39;) plt.xlabel(&#39;Recall&#39;); plt.ylabel(&#39;Precision&#39;); # plot the current threshold on the line close_default_clf = np.argmin(np.abs(thresholds - t)) plt.plot(r[close_default_clf], p[close_default_clf], &#39;^&#39;, c=&#39;k&#39;, markersize=15) slider = widgets.IntSlider( min=0, max=10, step=1, description=&#39;Slider:&#39;, value=3 # The best threshhold for our model ) display(slider) {&quot;model_id&quot;:&quot;64c4bfbbeb2a4242855806666b132d49&quot;,&quot;version_major&quot;:2,&quot;version_minor&quot;:0} print(f&#39;For this threshold : {slider.value/10}, the confusion matrix is as follow :&#39;) precision_recall_threshold(p, r, thresholds, slider.value/10) For this threshold : 0.3, the confusion matrix is as follow : . . . def plot_roc_curve(fpr, tpr, label=None): plt.figure(figsize=(8,8)) plt.title(&#39;ROC Curve&#39;) plt.plot(fpr, tpr, linewidth=2, label=label) plt.plot([0, 1], [0, 1], &#39;k--&#39;) plt.axis([-0.005, 1, 0, 1.005]) plt.xticks(np.arange(0,1, 0.05), rotation=90) plt.xlabel(&quot;False Positive Rate&quot;) plt.ylabel(&quot;True Positive Rate (Recall)&quot;) plt.legend(loc=&#39;best&#39;) fpr, tpr, auc_thresholds = roc_curve(y_val, y_scores) print(f&#39;AUC : {auc(fpr, tpr)}&#39;) # AUC of ROC plot_roc_curve(fpr, tpr, &#39;recall_optimized&#39;) AUC : 0.9433941778952841 . . Now, let’s test this model on our test set : . accuracy_score(test_y,m.predict(test_x_final)) , recall_score(test_y,m.predict(test_x_final)) , f1_score(test_y,m.predict(test_x_final), average=&#39;binary&#39;, pos_label=1) (0.9546420480744171, 0.4183640478499838, 0.5335532419338213) . Results : Partial dependency and SHAP values . Let’s look at partial dependence plots. . Partial dependence plots try to answer the question: if a row varied on nothing other than the feature in question, how would it impact the dependent variable? . For instance, how does capital_gains and dividends_from_stocks impact probability of belonging to the +50k income levl, all other things being equal? . from sklearn.inspection import plot_partial_dependence fig,ax = plt.subplots(figsize=(20, 8)) plot_partial_dependence(m, X_val_final, [&#39;capital_gains&#39;,&#39;dividends_from_stocks&#39;], percentiles=(0,1), grid_resolution=15, ax=ax); /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function plot_partial_dependence is deprecated; Function `plot_partial_dependence` is deprecated in 1.0 and will be removed in 1.2. Use PartialDependenceDisplay.from_estimator instead warnings.warn(msg, category=FutureWarning) . . Looking first at the dividends_from_stocks plot, we can see a nearly linear relationship between capital dividends_from_stocks and the probabillity of income level. Same for capital_gains at 5 standad deviation from the mean after reaching a steady state above that. . !pip install shap Requirement already satisfied: shap in /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages (0.40.0) Requirement already satisfied: packaging&gt;20.9 in /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages (from shap) (21.0) Requirement already satisfied: cloudpickle in /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages (from shap) (2.0.0) Requirement already satisfied: tqdm&gt;4.25.0 in /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages (from shap) (4.62.3) Requirement already satisfied: pandas in /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages (from shap) (1.3.4) Requirement already satisfied: scipy in /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages (from shap) (1.7.1) Requirement already satisfied: slicer==0.0.7 in /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages (from shap) (0.0.7) Requirement already satisfied: numpy in /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages (from shap) (1.21.2) Requirement already satisfied: numba in /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages (from shap) (0.53.1) Requirement already satisfied: scikit-learn in /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages (from shap) (1.0.1) Requirement already satisfied: pyparsing&gt;=2.0.2 in /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages (from packaging&gt;20.9-&gt;shap) (3.0.4) Requirement already satisfied: llvmlite&lt;0.37,&gt;=0.36.0rc1 in /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages (from numba-&gt;shap) (0.36.0) Requirement already satisfied: setuptools in /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages (from numba-&gt;shap) (58.0.4) Requirement already satisfied: python-dateutil&gt;=2.7.3 in /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages (from pandas-&gt;shap) (2.8.2) Requirement already satisfied: pytz&gt;=2017.3 in /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages (from pandas-&gt;shap) (2021.3) Requirement already satisfied: six&gt;=1.5 in /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages (from python-dateutil&gt;=2.7.3-&gt;pandas-&gt;shap) (1.16.0) Requirement already satisfied: joblib&gt;=0.11 in /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages (from scikit-learn-&gt;shap) (1.1.0) Requirement already satisfied: threadpoolctl&gt;=2.0.0 in /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages (from scikit-learn-&gt;shap) (2.2.0) import shap row_to_show = 20 data_for_prediction = test_x_final.iloc[row_to_show] # use 1 row of data here. Could use multiple rows if desired # Create object that can calculate shap values explainer = shap.TreeExplainer(m) # Calculate Shap values shap_values = explainer.shap_values(data_for_prediction) shap.initjs() shap.force_plot(explainer.expected_value[1], shap_values[1], data_for_prediction) &lt;IPython.core.display.HTML object&gt; &lt;shap.plots._force.AdditiveForceVisualizer at 0x13c39bd50&gt; . The above explanation shows features each contributing to push the model output from the base value (the average model output over the training dataset we passed) to the model output. Features pushing the prediction higher are shown in red, those pushing the prediction lower are in blue . The base_value here is 0.062 while our predicted value is 0.0. | sex = 1 has the biggest impact on increasing the prediction, while | Weeks_worked_im_year (below the average) and Age (below the average) feature has the biggest effect in decreasing the prediction. | . explainer = shap.TreeExplainer(m) # calculate shap values. This is what we will plot. # Calculate shap_values for all of val_X rather than a single row, to have more data for plot. shap_values = explainer.shap_values(test_x_final.iloc[:1000,]) # Make plot. Index of [1] is explained in text below. shap.summary_plot(shap_values[1],test_x_final.iloc[:1000,]) . . For every dot: . Vertical location shows what feature it is depicting | Color shows whether that feature was high or low for that row of the dataset | Horizontal location shows whether the effect of that value caused a higher or lower prediction. | . For the age variable, the point in the upper left was depicts a person whose age level is less thereby reducing the prediction of income level +50k class by 0.2. . Conclusion : . In this work, we presented some techniques for dealing with a machine learning project : . We used Decision Tree ensembles : Random Forest are the easiest to train, because they are extremely resilient to hyperparameter choices and require very little preprocessing. They are very fast to train, and should not overfit if we have enough trees. . | we used the model for feature selection and partial dependence analysis and Shap values, to get a better understanding of our data. . | . For futur improvements : . We can try Gradient Boosting machines as in theory are just as fast to train as random forests, but in practice we will have to try lots of different hyperparameters. They can overfit, but they are often a little more accurate than random forests. . | We can try OneHotEncoder with PCA to deal with the multiple modalities on our categorical variables. . | We can creat new features to challenge the model performance. . | . .",
            "url": "https://younesszaim.github.io/myportfolio/2021/11/20/US-cencus-income-Ensembles,-Bagging-and-Shap-Values.html",
            "relUrl": "/2021/11/20/US-cencus-income-Ensembles,-Bagging-and-Shap-Values.html",
            "date": " • Nov 20, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Sickit Learn For Machine Learning",
            "content": "Sickit-Learn for Machine Learning . In this notebook we will work through an example project end to end using Sickit Learn library. . About . In this chapter we’ll use the California Housing Prices dataset from the StatLib repository This dataset is based on data from the 1990 California census. . This data includes metrics such as the population, median income, and median housing price for each block group in California. Block groups are the smallest geographical unit for which the US Census Bureau publishes sample data (a block group typical). . Your model should learn from this data and be able to predict the median housing price in any district, given all the other metrics. . Goal : Your boss answers that your model’s output (a prediction of a district’s median housing price) will be fed to another Machine Learning system , along with many other signals. This downstream system will determine whether it is worth investing in a given area or not. Getting this right is critical, as it directly affects revenue. . First, you need to frame the problem: is it supervised, unsupervised, or Reinforcement Learning? Is it a classification task, a regression task, or something else? Should you use batch learning or online learning techniques? . Let’s see: it is clearly a typical supervised learning task, since you are given labeled training examples. . It is also a typical regression task, since you are asked to predict a value. More specifically, this is a multiple regression problem, since the system will use multiple features to make a prediction. . It is also a univariate regression problem, since we are only trying to predict a single value for each district. If we were trying to predict multiple values per district, it would be a multivariate regression problem. . Finally, there is no continuous flow of data coming into the system, there is no particular need to adjust to changing data rapidly, and the data is small enough to fit in memory, so plain batch learning should do just fine. . Select a Performance Measure . Performance Measures for our univariate regression problem . Typical performance measure for regression problems : . Root Mean Square Error (RMSE): it gives an idea of how much error the system typically makes in its predictions, with a higher weight for large errors : Since the errors are squared before they are averaged, the RMSE gives a relatively high weight to large errors. This means the RMSE should be more useful when large errors are particularly undesirable. | . RMSE is sensitive to outliers : If we make a single very bad prediction, taking the square will make the error even worse and it may skew the metric towards overestimating the model’s badness. Actually, it’s hard to realize if our model is good or not by looking at the absolute values of MSE or MSE : We would probably want to measure how much our model is better than the constant baseline : A model should at least perform better than the RMSE score constant baseline. . RMSE has the benefit of penalizing large errors more so can be more appropriate in some cases, for example, if being off by 10 is more than twice as bad as being off by 5. But if being off by 10 is just twice as bad as being off by 5, then MAE is more appropriate. . | . Root Mean Square Log Error (RMSLE): It is an extension on root Mean Squared Error (RMSE) that is mainly used when predictions have large deviations | . RMSLE is preferable when : . targets having exponential growth, such as population counts, average sales of a commodity over a span of years etc . | we care about percentage errors rather than the absolute value of errors : The reason we use log is because generally, you care not so much about missing by €10 but missing by 10%. So if it was €1000,000 item and you are €100,000 off or if it was a 10,000 item and you are €1,000 off — we would consider those equivalent scale issues. . | There is a wide range in the target variables and we don’t want to penalize big differences when both the predicted and the actual are big numbers. . | We want to penalize under estimates more than over estimates. . | Let’s imagine two cases of predictions, . | . Case-1: our model makes a prediction of 30 when the actual number is 40 Case-2: our model makes a prediction of 300 when the actual number is 400 . With RMSE the second result is scored as 10 times more than the first result Conversely, with RMSLogE two results are scored the same. RMSLogE takes into account just the ratio of change Lets have a look at the below example . Case-3 : Prediction = 600, Actual = 1000 (the absolute difference is 400) . RMSE = 400, RMSLogE = 0.5108 . Case-4 : Prediction = 1400, Actual = 1000 (the absolute difference is 400) . RMSE = 400, RMSLogE = 0.3365 . When the differences are the same between actual and predicted in both cases. RMSE treated them equally, however RMSLogE penalized the under estimate more than over estimate (under estimated prediction score is higher than over estimated prediction score). Often, penalizing the under estimate more than over estimate is important for prediction of sales and inventory demands. . | . | . Mean Absolute Error (MAE): also called the average absolute deviation : MAE measures the average magnitude of the errors in a set of predictions, without considering their direction. . | R-Squared (R2) : proportional improvement in prediction of the regression model, compared to the mean model (model predicting all given samples as mean value) : - If we were exactly as effective as just predicting the mean, SSres/SStot = 1 and R² = 0 - If we were perfect (i.e. yi = fi for all cases), SSres/SStot = 0 and R² = 1 However, it does not take into consideration of overfitting problem. . Interpreted as the proportion of total variance that is explained by the model. | R² is the ratio between how good your model is (RMSE)vs. how good is the naïve mean model (RMSE). | . | . RMSE vs RMSLE vs MAE . See links below : . RMSE vs MAE : https://medium.com/human-in-a-machine-world/mae-and-rmse-which-metric-is-better-e60ac3bde13d | RMSLE metric and defining baseline : https://www.kaggle.com/carlolepelaars/understanding-the-metric-rmsle/notebook | Model fot metrics : https://www.kaggle.com/residentmario/model-fit-metrics | . Scikit-learn implementation . ### R-square : # sklean from sklearn.metrics import r2_score # hand implemetation import numpy as np def r2_score(y, y_pred): rss_adj = np.sum((y - y_pred)**2) n = len(y) y_bar_adj = (1 / n) * np.sum(y) ess_adj = np.sum((y - y_bar_adj)**2) return 1 - rss_adj / ess_adj r2_score(y, y_pred) ### Root Mean Squared Error (RMSE) from sklearn.metrics import mean_squared_error mean_squared_error(y,y_pred, squared = False) # hand implemetation import math def rmse(y, y_pred): return math.sqrt( ((y-y_pred)**2).mean() ) root_mean_squared_error(y, y_pred) ### Root Mean log Squared Error (RMLSE) from sklearn.metrics import mean_squared_log_error mean_squared_error(y,y_pred, squared = False) # or import numpy as np y = np.log(df.y) RMSLE = rmse(y,y_pred) ### Mean Absolute Error (MAE) from sklearn.metrics import mean_absolute_error # hand implemetation import numpy as np def mae(y,y_pred): return (np.abs(y-y_pred)).mean() NameError Traceback (most recent call last) &lt;ipython-input-1061-7e2a74a2dd02&gt; in &lt;module&gt; 14 return 1 - rss_adj / ess_adj 15 &gt; 16 r2_score(y, y_pred) 17 18 NameError: name &#39;y_pred&#39; is not defined . Download the Data . PATH = &#39;/Users/rmbp/handson-ml2/datasets/&#39; !ls {PATH} housing inception jsb_chorales lifesat titanic import pandas as pd housing = pd.read_csv(f&#39;{PATH}/housing/housing.csv&#39;) housing.head() longitude latitude housing_median_age total_rooms total_bedrooms 0 -122.23 37.88 41.0 880.0 129.0 1 -122.22 37.86 21.0 7099.0 1106.0 2 -122.24 37.85 52.0 1467.0 190.0 3 -122.25 37.85 52.0 1274.0 235.0 4 -122.25 37.85 52.0 1627.0 280.0 population households median_income median_house_value ocean_proximity 0 322.0 126.0 8.3252 452600.0 NEAR BAY 1 2401.0 1138.0 8.3014 358500.0 NEAR BAY 2 496.0 177.0 7.2574 352100.0 NEAR BAY 3 558.0 219.0 5.6431 341300.0 NEAR BAY 4 565.0 259.0 3.8462 342200.0 NEAR BAY . Automating the process of fetching and loading the data . import os import tarfile import urllib DOWNLOAD_ROOT = &quot;https://raw.githubusercontent.com/ageron/handson-ml2/master/&quot; HOUSING_PATH = os.path.join(&quot;/Users/rmbp/Desktop&quot;, &quot;housing&quot;) HOUSING_URL = DOWNLOAD_ROOT + &quot;datasets/housing/housing.tgz&quot; def fetch_housing_data(housing_url=HOUSING_URL, housing_path=HOUSING_PATH): os.makedirs(housing_path, exist_ok=True) tgz_path = os.path.join(housing_path, &quot;housing.tgz&quot;) urllib.request.urlretrieve(housing_url, tgz_path) housing_tgz = tarfile.open(tgz_path) housing_tgz.extractall(path=HOUSING_PATH) housing_tgz.close() fetch_housing_data(HOUSING_URL,HOUSING_PATH) import pandas as pd def load_housing_data(housing_path=HOUSING_PATH): csv_path = os.path.join(housing_path, &quot;housing.csv&quot;) return pd.read_csv(csv_path) load_housing_data().head() longitude latitude housing_median_age total_rooms total_bedrooms 0 -122.23 37.88 41.0 880.0 129.0 1 -122.22 37.86 21.0 7099.0 1106.0 2 -122.24 37.85 52.0 1467.0 190.0 3 -122.25 37.85 52.0 1274.0 235.0 4 -122.25 37.85 52.0 1627.0 280.0 population households median_income median_house_value ocean_proximity 0 322.0 126.0 8.3252 452600.0 NEAR BAY 1 2401.0 1138.0 8.3014 358500.0 NEAR BAY 2 496.0 177.0 7.2574 352100.0 NEAR BAY 3 558.0 219.0 5.6431 341300.0 NEAR BAY 4 565.0 259.0 3.8462 342200.0 NEAR BAY . Take a Quick Look at the Data Structure . housing.head() longitude latitude housing_median_age total_rooms total_bedrooms 0 -122.23 37.88 41.0 880.0 129.0 1 -122.22 37.86 21.0 7099.0 1106.0 2 -122.24 37.85 52.0 1467.0 190.0 3 -122.25 37.85 52.0 1274.0 235.0 4 -122.25 37.85 52.0 1627.0 280.0 population households median_income median_house_value ocean_proximity 0 322.0 126.0 8.3252 452600.0 NEAR BAY 1 2401.0 1138.0 8.3014 358500.0 NEAR BAY 2 496.0 177.0 7.2574 352100.0 NEAR BAY 3 558.0 219.0 5.6431 341300.0 NEAR BAY 4 565.0 259.0 3.8462 342200.0 NEAR BAY housing.info() &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 20640 entries, 0 to 20639 Data columns (total 10 columns): # Column Non-Null Count Dtype -- -- 0 longitude 20640 non-null float64 1 latitude 20640 non-null float64 2 housing_median_age 20640 non-null float64 3 total_rooms 20640 non-null float64 4 total_bedrooms 20433 non-null float64 5 population 20640 non-null float64 6 households 20640 non-null float64 7 median_income 20640 non-null float64 8 median_house_value 20640 non-null float64 9 ocean_proximity 20640 non-null object dtypes: float64(9), object(1) memory usage: 1.6+ MB . There are 20,640 instances in the dataset. Notice that the total_bedrooms attribute has only 20,433 nonnull values, meaning that 207 districts are missing this feature. All attributes are numerical, except the ocean_proximity field. Its type is object. Since we loaded this data from a CSV file, it must be a text attribute. : the values in the ocean_proximity column were repetitive, which means that it is probably a categorical attribute. . housing[&#39;ocean_proximity&#39;].value_counts() &lt;1H OCEAN 9136 INLAND 6551 NEAR OCEAN 2658 NEAR BAY 2290 ISLAND 5 Name: ocean_proximity, dtype: int64 housing.describe(include=&#39;all&#39;).T count unique top freq mean longitude 20640.0 NaN NaN NaN -119.569704 latitude 20640.0 NaN NaN NaN 35.631861 housing_median_age 20640.0 NaN NaN NaN 28.639486 total_rooms 20640.0 NaN NaN NaN 2635.763081 total_bedrooms 20433.0 NaN NaN NaN 537.870553 population 20640.0 NaN NaN NaN 1425.476744 households 20640.0 NaN NaN NaN 499.53968 median_income 20640.0 NaN NaN NaN 3.870671 median_house_value 20640.0 NaN NaN NaN 206855.816909 ocean_proximity 20640 5 &lt;1H OCEAN 9136 NaN std min 25% 50% 75% longitude 2.003532 -124.35 -121.8 -118.49 -118.01 latitude 2.135952 32.54 33.93 34.26 37.71 housing_median_age 12.585558 1.0 18.0 29.0 37.0 total_rooms 2181.615252 2.0 1447.75 2127.0 3148.0 total_bedrooms 421.38507 1.0 296.0 435.0 647.0 population 1132.462122 3.0 787.0 1166.0 1725.0 households 382.329753 1.0 280.0 409.0 605.0 median_income 1.899822 0.4999 2.5634 3.5348 4.74325 median_house_value 115395.615874 14999.0 119600.0 179700.0 264725.0 ocean_proximity NaN NaN NaN NaN NaN max longitude -114.31 latitude 41.95 housing_median_age 52.0 total_rooms 39320.0 total_bedrooms 6445.0 population 35682.0 households 6082.0 median_income 15.0001 median_house_value 500001.0 ocean_proximity NaN . hist() method on the whole dataset will plot a histogram for each numerical attribute . A histogram is used for continuous data, where the bins represent ranges of data, counts the data points in each bin, and shows the bins on the x-axis and the counts on the y-axis. : https://towardsdatascience.com/histograms-and-density-plots-in-python-f6bda88f5ac0 | A bar chart is a plot of categorical variables. | . #This tells Jupyter to set up Matplotlib so it uses Jupyter’s own backend. %matplotlib inline import matplotlib.pyplot as plt housing.hist(bins=60, figsize=(15,10)) plt.show() . . # plot &#39;median_house_value&#39; housing[&#39;median_house_value&#39;].plot(kind=&#39;hist&#39;, bins= 60) &lt;AxesSubplot:ylabel=&#39;Frequency&#39;&gt; . . housing[&#39;ocean_proximity&#39;].value_counts().plot(kind= &#39;barh&#39;) &lt;AxesSubplot:&gt; . . pd.DataFrame(housing[&#39;median_income&#39;].describe()).T count mean std min 25% 50% 75% median_income 20640.0 3.870671 1.899822 0.4999 2.5634 3.5348 4.74325 max median_income 15.0001 n, bins, patches = plt.hist(housing.median_income, bins = int((15.000100 - 0.499900)/0.1),edgecolor = &#39;black&#39; ,color = &#39;blue&#39;) # bins = int((15.000100 - 0.499900)/0.1) : We choose the number of bins with an interval lenght of 100€ . . pd.DataFrame(housing[&#39;housing_median_age&#39;].describe()).T count mean std min 25% 50% 75% max housing_median_age 20640.0 28.639486 12.585558 1.0 18.0 29.0 37.0 52.0 n, bins, patches = plt.hist(housing.housing_median_age, bins = int((52.000000 - 1.000000)/1) , color = &#39;blue&#39; , edgecolor = &#39;black&#39;) bins array([ 1., 2., 3., 4., 5., 6., 7., 8., 9., 10., 11., 12., 13., 14., 15., 16., 17., 18., 19., 20., 21., 22., 23., 24., 25., 26., 27., 28., 29., 30., 31., 32., 33., 34., 35., 36., 37., 38., 39., 40., 41., 42., 43., 44., 45., 46., 47., 48., 49., 50., 51., 52.]) . . # Target pd.DataFrame(housing[&#39;median_house_value&#39;].describe()).T count mean std min 25% median_house_value 20640.0 206855.816909 115395.615874 14999.0 119600.0 50% 75% max median_house_value 179700.0 264725.0 500001.0 n, bins, patches = plt.hist(housing.median_house_value , bins = int((500001.000000 - 14999.000000)/10000) , color = &#39;blue&#39; ,edgecolor = &#39;black&#39;) bins array([ 14999. , 25103.20833333, 35207.41666667, 45311.625 , 55415.83333333, 65520.04166667, 75624.25 , 85728.45833333, 95832.66666667, 105936.875 , 116041.08333333, 126145.29166667, 136249.5 , 146353.70833333, 156457.91666667, 166562.125 , 176666.33333333, 186770.54166667, 196874.75 , 206978.95833333, 217083.16666667, 227187.375 , 237291.58333333, 247395.79166667, 257500. , 267604.20833333, 277708.41666667, 287812.625 , 297916.83333333, 308021.04166667, 318125.25 , 328229.45833333, 338333.66666667, 348437.875 , 358542.08333333, 368646.29166667, 378750.5 , 388854.70833333, 398958.91666667, 409063.125 , 419167.33333333, 429271.54166667, 439375.75 , 449479.95833333, 459584.16666667, 469688.375 , 479792.58333333, 489896.79166667, 500001. ]) . . From the figure below, we can see how the data was computed : . We can see that median_income was scaled and capped at 15 (actually, 15.0001) for higher median incomes, and at 0.5 (actually, 0.4999) for lower median incomes. The numbers represent roughly tens of thousands of dollars. | The housing median age and the median house value were also capped. The latter may be a serious problem since it is our target attribute. In this caseour Machine Learning algorithms may learn that prices never go beyond that limit (€500,000). We need to check with our team to see if this is a problem or not. If the team needs precise predictions even beyond €500,000, then you have two options: Collect proper labels for the districts whose labels were capped. | Remove those districts from the training set (and also from the test set, since your system should not be evaluated poorly if it predicts values beyond €500,000). | . | . We can also see that : . These attributes have very different scales. | Many histograms are tail-heavy : they extend much farther to the right of the median than to the left. | . Create a Test Set . We ahve only taken a quick glance at the data : numeric / categorical features, missing values, scale of attributes, distribution, how values are computed, distribution of the target variable. It’s enough. Why ? . if you look at the test set, you may stumble upon some seemingly interesting pattern in the test data that leads you to select a particular kind of Machine Learning model. When you estimate the generalization error using the test set, your estimate will be too optimistic, and you will launch a system that will not perform as well as expected. This is called data snooping bias. | . Train and test set stability . Creating a test set is theoretically simple: pick some instances randomly, typically 20% of the dataset . import numpy as np def split_train_test(data, test_ratio): shuffled_indices = np.random.permutation(len(data)) test_set_size = int(len(data) * test_ratio) test_indices = shuffled_indices[:test_set_size] train_indices = shuffled_indices[test_set_size:] return data.iloc[train_indices], data.iloc[test_indices] train_set, test_set = split_train_test(housing,0.2) print(len(train_set)) print(len(test_set)) 16512 4128 . Well, this works, but it is not perfect: if you run the program again, it will generate a different test set! . Over time, you (or your Machine Learning algorithms) will get to see the whole dataset, which is what you want to avoid. | . # first run test set split_train_test(housing,0.2)[1].head(5) longitude latitude housing_median_age total_rooms total_bedrooms 12003 -117.57 33.90 7.0 3797.0 850.0 13304 -117.63 34.09 19.0 3490.0 816.0 19037 -121.99 38.36 35.0 2728.0 451.0 9871 -121.82 36.61 24.0 2437.0 438.0 16526 -121.20 37.80 37.0 311.0 61.0 population households median_income median_house_value 12003 2369.0 720.0 3.5525 137600.0 13304 2818.0 688.0 2.8977 126200.0 19037 1290.0 452.0 3.2768 117600.0 9871 1430.0 444.0 3.8015 169100.0 16526 171.0 54.0 4.0972 101800.0 ocean_proximity 12003 INLAND 13304 INLAND 19037 INLAND 9871 &lt;1H OCEAN 16526 INLAND # second run test set split_train_test(housing,0.2)[1].head(5) longitude latitude housing_median_age total_rooms total_bedrooms 5709 -118.23 34.21 32.0 1464.0 406.0 16381 -121.30 38.02 4.0 1515.0 384.0 16458 -121.30 38.13 26.0 2256.0 360.0 8613 -118.37 33.87 23.0 1829.0 331.0 2738 -115.56 32.78 35.0 1185.0 202.0 population households median_income median_house_value 5709 693.0 380.0 2.5463 200000.0 16381 491.0 348.0 2.8523 87500.0 16458 937.0 372.0 5.0528 153700.0 8613 891.0 356.0 6.5755 359900.0 2738 615.0 191.0 4.6154 86200.0 ocean_proximity 5709 &lt;1H OCEAN 16381 INLAND 16458 INLAND 8613 &lt;1H OCEAN 2738 INLAND . Solution : . One solution is to save the test set on the first run and then load it in subsequent runs. | Another option is to set the random number generator’s seed (e.g., with np.ran dom.seed(42))14 before calling np.random.permutation() so that it always generates the same shuffled indices : | . import numpy as np def split_train_test(data, test_ratio): np.random.seed(1997) shuffled_indices = np.random.permutation(len(data)) test_set_size = int(len(data) * test_ratio) test_indices = shuffled_indices[:test_set_size] train_indices = shuffled_indices[test_set_size:] return data.iloc[train_indices], data.iloc[test_indices] # first run test set split_train_test(housing,0.2)[1].head(5) longitude latitude housing_median_age total_rooms total_bedrooms 9009 -118.60 34.07 16.0 319.0 59.0 17779 -121.83 37.38 15.0 4430.0 992.0 20209 -119.21 34.28 27.0 2219.0 312.0 3170 -119.69 36.41 38.0 1016.0 202.0 2200 -119.85 36.83 15.0 2563.0 335.0 population households median_income median_house_value 9009 149.0 64.0 4.6250 433300.0 17779 3278.0 1018.0 4.5533 209900.0 20209 937.0 315.0 5.7601 281100.0 3170 540.0 187.0 2.2885 75000.0 2200 1080.0 356.0 6.7181 160300.0 ocean_proximity 9009 &lt;1H OCEAN 17779 &lt;1H OCEAN 20209 NEAR OCEAN 3170 INLAND 2200 INLAND # second run test set split_train_test(housing,0.2)[1].head(5) longitude latitude housing_median_age total_rooms total_bedrooms 9009 -118.60 34.07 16.0 319.0 59.0 17779 -121.83 37.38 15.0 4430.0 992.0 20209 -119.21 34.28 27.0 2219.0 312.0 3170 -119.69 36.41 38.0 1016.0 202.0 2200 -119.85 36.83 15.0 2563.0 335.0 population households median_income median_house_value 9009 149.0 64.0 4.6250 433300.0 17779 3278.0 1018.0 4.5533 209900.0 20209 937.0 315.0 5.7601 281100.0 3170 540.0 187.0 2.2885 75000.0 2200 1080.0 356.0 6.7181 160300.0 ocean_proximity 9009 &lt;1H OCEAN 17779 &lt;1H OCEAN 20209 NEAR OCEAN 3170 INLAND 2200 INLAND . But both these solutions will break the next time you fetch an updated dataset. . If the dataset is updated, we want to ensure that the test set will remain consistent across multiple runs, even if you refresh the dataset : The new test set will contain 20% of the new instances, but it will not contain any instance that was previously in the training set. . To have a stable train/test split even after updating the dataset, a common solution is to use each instance’s identifier to decide whether or not it should go in the test set (assuming instances have a unique and immutable identifier). For example, we could compute a hash of each instance’s identifier and put that instance in the test set if the hash is lower than or equal to 20% of the maximum hash value. . from zlib import crc32 def test_set_check(identifier, test_ratio): return crc32(np.int64(identifier)) &amp; 0xffffffff &lt; test_ratio * 2**32 # crc32(np.int64(identifier)) = create a hash from a given value # crc32(np.int64(identifier)) &amp; 0xffffffff = make sure the hash value does not exceed 2^32 (or 4294967296). # crc32(np.int64(identifier)) &amp; 0xffffffff &lt; test_ratio * 2**32. # crc32(np.int64(identifier)) &amp; 0xffffffff &lt; test_ratio * 2**32 # This line returns True or False. Let test_ratio be 0.2. # Then, any hash value less than 0.2 * 4294967296 returns True and will be # added to the test set; otherwise, it returns False and will be added to the training set. */ def split_train_test_by_id(data, test_ratio, id_column): ids = data[id_column] # compute a hash of each instance’s identifier in_test_set = ids.apply(lambda id_: test_set_check(id_, test_ratio)) # if hash is lower than or equal to 20% of the maximum hash value return data.loc[~in_test_set], data.loc[in_test_set] . Unfortunately, the housing dataset does not have an identifier column. The simplest solution is to use the row index as the ID: . housing_with_id = housing.reset_index() # adds an `index` column train_set, test_set = split_train_test_by_id(housing_with_id, 0.2, &quot;index&quot;) . If we use the row index as a unique identifier, you need to make sure that new data gets appended to the end of the dataset and that no row ever gets deleted. If this is not possible, then we can try to use the most stable features to build a unique identifier. . For example, a district’s latitude and longitude are guaranteed to be stable for a few million years, so you could combine them into an ID like so: . housing_with_id[&quot;id&quot;] = housing[&quot;longitude&quot;] * 1000 + housing[&quot;latitude&quot;] train_set, test_set = split_train_test_by_id(housing_with_id, 0.2, &quot;id&quot;) . See explanation of this method in : https://ichi.pro/fr/ameliorez-la-repartition-des-tests-de-train-avec-la-fonction-de-hachage-267796356735483 and https://datascience.stackexchange.com/questions/51348/splitting-train-test-sets-by-an-identifier . Train / Test split using Sckit-learn . Scikit-Learn provides a few functions to split datasets into multiple subsets in various ways. The simplest function is train_test_split(), which does pretty much the same thing as the function split_train_test(), with a couple of additional features : . First, there is a random_state parameter random_state that allows you to set the random generator seed and a test size test_size. | Second, we can pass it multiple datasets with an identical number of rows, and it will split them on the same indices (this is very useful, for example, if you have a separate DataFrame for labels): | . from sklearn.model_selection import train_test_split train_set, test_set = train_test_split(housing, test_size=0.2, random_state = 1997) . Sampling bias in Test set . Using train_test_splitmethod, we using purely random sampling methods to generate our test set. This is generally fine if our dataset is large enough (especially relative to the number of attributes), but if it is not, we run the risk of introducing a significant sampling bias. . If an attribute (continues or categorical) is important (after discussing with experts for exemple) : We may want to ensure that the test set is representative of the various categories of that variable in the whole dataset. . Suppose that the median income is a very important attribute to predict median housing prices. . housing[&#39;median_income&#39;].describe() count 20640.000000 mean 3.870671 std 1.899822 min 0.499900 25% 2.563400 50% 3.534800 75% 4.743250 max 15.000100 Name: median_income, dtype: float64 plt.hist(housing[&#39;median_income&#39;] #, bins = int( (housing[&#39;median_income&#39;].max() - housing[&#39;median_income&#39;].min()) / 0.5) , bins = 60 , color = &#39;blue&#39; ,edgecolor = &#39;black&#39; ) plt.show() . . housing[&quot;income_cat&quot;] = pd.cut(housing[&quot;median_income&quot;], bins=[0., 1.5, 3.0, 4.5, 6., np.inf], labels=[1, 2, 3, 4, 5]) housing[&#39;income_cat&#39;] = pd.cut( housing[&#39;median_income&#39;] , bins = [0., 1.5, 3.0, 4.5, 6., np.inf] , labels = [1, 2, 3, 4, 5] ) housing[&quot;income_cat&quot;].hist() &lt;AxesSubplot:&gt; . . housing[&quot;income_cat&quot;].value_counts() / len(housing) 3 0.350581 2 0.318847 4 0.176308 5 0.114438 1 0.039826 Name: income_cat, dtype: float64 from sklearn.model_selection import StratifiedShuffleSplit split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42) for train_index, test_index in split.split(housing, housing[&quot;income_cat&quot;]): strat_train_set = housing.loc[train_index] strat_test_set = housing.loc[test_index] strat_train_set[&quot;income_cat&quot;].value_counts() / len(strat_train_set) 3 0.350594 2 0.318859 4 0.176296 5 0.114462 1 0.039789 Name: income_cat, dtype: float64 strat_test_set[&quot;income_cat&quot;].value_counts() / len(strat_test_set) 3 0.350533 2 0.318798 4 0.176357 5 0.114341 1 0.039971 Name: income_cat, dtype: float64 def income_cat_proportions(data): return data[&quot;income_cat&quot;].value_counts() / len(data) train_set, test_set = train_test_split(housing, test_size=0.2, random_state=42) compare_props = pd.DataFrame({ &quot;Overall&quot;: income_cat_proportions(housing), &quot;Stratified&quot;: income_cat_proportions(strat_test_set), &quot;Random&quot;: income_cat_proportions(test_set), }).sort_index() compare_props[&quot;Rand. %error&quot;] = 100 * compare_props[&quot;Random&quot;] / compare_props[&quot;Overall&quot;] - 100 compare_props[&quot;Strat. %error&quot;] = 100 * compare_props[&quot;Stratified&quot;] / compare_props[&quot;Overall&quot;] - 100 compare_props Overall Stratified Random Rand. %error Strat. %error 1 0.039826 0.039971 0.040213 0.973236 0.364964 2 0.318847 0.318798 0.324370 1.732260 -0.015195 3 0.350581 0.350533 0.358527 2.266446 -0.013820 4 0.176308 0.176357 0.167393 -5.056334 0.027480 5 0.114438 0.114341 0.109496 -4.318374 -0.084674 . Further analysis later | . for set_ in (strat_train_set, strat_test_set): set_.drop(&quot;income_cat&quot;, axis=1, inplace=True) . Discover and Visualize the Data to Gain Insights . First, we make sure that we have put the test set aside and we are only exploring the training set. Also, if the training set is very large, you may want to sample an exploration set, to make manipulations easy and fast. In our case, the set is quite small, so we can just work directly on the full set. . Let’s create a copy so that you can play with it without harming the training set: | . housing = strat_train_set.copy() housing.head(5) longitude latitude housing_median_age total_rooms total_bedrooms 12655 -121.46 38.52 29.0 3873.0 797.0 15502 -117.23 33.09 7.0 5320.0 855.0 2908 -119.04 35.37 44.0 1618.0 310.0 14053 -117.13 32.75 24.0 1877.0 519.0 20496 -118.70 34.28 27.0 3536.0 646.0 population households median_income median_house_value 12655 2237.0 706.0 2.1736 72100.0 15502 2015.0 768.0 6.3373 279600.0 2908 667.0 300.0 2.8750 82700.0 14053 898.0 483.0 2.2264 112500.0 20496 1837.0 580.0 4.4964 238300.0 ocean_proximity income_cat 12655 INLAND 2 15502 NEAR OCEAN 5 2908 INLAND 2 14053 NEAR OCEAN 2 20496 &lt;1H OCEAN 3 . Since we have geographic information (lon / lat), let’s create a scatterplot of all districts to visualize the data : doc of a scatterplot parameter https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.scatter.html | . housing.plot(kind = &#39;scatter&#39;, x = &#39;longitude&#39;, y = &#39;latitude&#39;) &lt;AxesSubplot:xlabel=&#39;longitude&#39;, ylabel=&#39;latitude&#39;&gt; . . Scatter plots work well for hundreds of observations but overplotting becomes an issue once the number of observations gets into tens of thousands. . We can see that in some areas, there are vast numbers of dots, so it is hard to see any particular pattern. . Simple options to address overplotting : . reducing the point size : usisng the s parameter - This parameter indicates the marker size. | alpha blending : using alpha parameter This option indicates the blending value, between 0 (transparent) and 1 (opaque). | . housing.plot(kind = &#39;scatter&#39; ,x = &#39;longitude&#39; ,y = &#39;latitude&#39; , s= 0.2) &lt;AxesSubplot:xlabel=&#39;longitude&#39;, ylabel=&#39;latitude&#39;&gt; . . housing.plot(kind = &#39;scatter&#39; ,x = &#39;longitude&#39; ,y = &#39;latitude&#39; , alpha= 0.1) &lt;AxesSubplot:xlabel=&#39;longitude&#39;, ylabel=&#39;latitude&#39;&gt; . . We can get the names of the cities in the map and conclude which have the highest density - Many article covers this subject - we will do it later . # To save a picture in our folder project : IMAGES_PATH = &quot;/Users/rmbp/Desktop/housing&quot; def save_fig(fig_id, tight_layout=True, fig_extension=&quot;png&quot;, resolution=300): path = os.path.join(IMAGES_PATH, fig_id + &quot;.&quot; + fig_extension) print(&quot;Saving figure&quot;, fig_id) if tight_layout: plt.tight_layout() plt.savefig(path, format=fig_extension, dpi=resolution) housing.plot(kind = &#39;scatter&#39; ,x = &#39;longitude&#39; ,y = &#39;latitude&#39; , alpha= 0.1, c=&#39;black&#39;) save_fig(&quot;better_visualization_plot&quot;) Saving figure better_visualization_plot . . We can see the houses price crossing with the population on the map below : . the parameter s re presenting the radius of each circle will represents the `district’s population`` | the paramter c representing the color will represents the price. | We will use a predefined color map (option cmap) called jet, which ranges from blue (low values) to red (high prices): | . housing.plot(kind=&quot;scatter&quot;, x=&quot;longitude&quot;, y=&quot;latitude&quot;, alpha=0.4, s=housing[&quot;population&quot;]/100, label=&quot;population&quot;, figsize=(10,7), c=&quot;median_house_value&quot;, cmap=plt.get_cmap(&quot;jet&quot;), colorbar=True, ) plt.legend() &lt;matplotlib.legend.Legend at 0x12f4d1650&gt; . . This image tells us that the housing prices are very much related to the location (e.g., close to the ocean) and to the population density. | A clustering algorithm should be useful for detecting the main cluster and for adding new features that measure the proximity to the cluster centers. See later.. Check this blog : https://dev.to/travelleroncode/analyzing-a-dataset-with-unsupervised-learning-31ld | The ocean proximity attribute may be useful as well, although in Northern California the housing prices in coastal districts are not too high, so it is not a simple rule. | . Looking for correlations . If we want to explore our data it is good to compute correlation between numeric variable : Spearman S and Pearon P. W can compute them both since the relation between the Spearman (S) and Pearson (P) correlations will give some good information : . Briefly, S is computed on ranks and so depicts monotonic relationships while P is on true values and depicts linear relationships. . | We the corr method : By default, method = ‘Pearson’ . | . s = {} for x in range(1,100): s[x] = math.exp(x) s = pd.DataFrame(s.items()) s.corr(&#39;pearson&#39;) 0 1 0 1.000000 0.253274 1 0.253274 1.000000 s.corr(&#39;spearman&#39;) 0 1 0 1.0 1.0 1 1.0 1.0 . This is because 𝑦 increases monotonically with 𝑥 so the Spearman correlation is perfect, but not linearly, so the Pearson correlation is imperfect. . Doing both is interesting because if we have S &gt; P, that means that we have a correlation that is monotonic but not linear. Since it is good to have linearity in statistics (it is easier) we can try to apply a transformation on 𝑦(such a log). . corr_matrix = housing.corr() corr_matrix longitude latitude housing_median_age total_rooms longitude 1.000000 -0.924478 -0.105823 0.048909 latitude -0.924478 1.000000 0.005737 -0.039245 housing_median_age -0.105823 0.005737 1.000000 -0.364535 total_rooms 0.048909 -0.039245 -0.364535 1.000000 total_bedrooms 0.076686 -0.072550 -0.325101 0.929391 population 0.108071 -0.115290 -0.298737 0.855103 households 0.063146 -0.077765 -0.306473 0.918396 median_income -0.019615 -0.075146 -0.111315 0.200133 median_house_value -0.047466 -0.142673 0.114146 0.135140 total_bedrooms population households median_income longitude 0.076686 0.108071 0.063146 -0.019615 latitude -0.072550 -0.115290 -0.077765 -0.075146 housing_median_age -0.325101 -0.298737 -0.306473 -0.111315 total_rooms 0.929391 0.855103 0.918396 0.200133 total_bedrooms 1.000000 0.876324 0.980167 -0.009643 population 0.876324 1.000000 0.904639 0.002421 households 0.980167 0.904639 1.000000 0.010869 median_income -0.009643 0.002421 0.010869 1.000000 median_house_value 0.047781 -0.026882 0.064590 0.687151 median_house_value longitude -0.047466 latitude -0.142673 housing_median_age 0.114146 total_rooms 0.135140 total_bedrooms 0.047781 population -0.026882 households 0.064590 median_income 0.687151 median_house_value 1.000000 . Now let’s look at how much each attribute correlates with the median house value: . corr_matrix[&#39;median_house_value&#39;].sort_values(ascending = False) median_house_value 1.000000 median_income 0.687151 total_rooms 0.135140 housing_median_age 0.114146 households 0.064590 total_bedrooms 0.047781 population -0.026882 longitude -0.047466 latitude -0.142673 Name: median_house_value, dtype: float64 corr_matrix = housing.corr(&#39;spearman&#39;) corr_matrix[&#39;median_house_value&#39;].sort_values(ascending = False) median_house_value 1.000000 median_income 0.675714 total_rooms 0.204476 households 0.110722 total_bedrooms 0.084284 housing_median_age 0.083301 population 0.001309 longitude -0.071562 latitude -0.162283 Name: median_house_value, dtype: float64 . Another way to check for correlation between attributes is to use the pandas scatter_matrix() function, which plots every numerical attribute against every other numerical attribute. ( if we have 11 attribiute, we will plot 11**2 plots ) . From the pearson coefficient below, we focus on a few promising attributes that seem most correlated with the median housing value : . from pandas.plotting import scatter_matrix attributes = [&#39;median_house_value&#39;, &#39;median_income&#39;, &#39;total_rooms&#39;, &#39;housing_median_age&#39; ] scatter_matrix(housing[attributes], figsize=(10,6), ) array([[&lt;AxesSubplot:xlabel=&#39;median_house_value&#39;, ylabel=&#39;median_house_value&#39;&gt;, &lt;AxesSubplot:xlabel=&#39;median_income&#39;, ylabel=&#39;median_house_value&#39;&gt;, &lt;AxesSubplot:xlabel=&#39;total_rooms&#39;, ylabel=&#39;median_house_value&#39;&gt;, &lt;AxesSubplot:xlabel=&#39;housing_median_age&#39;, ylabel=&#39;median_house_value&#39;&gt;], [&lt;AxesSubplot:xlabel=&#39;median_house_value&#39;, ylabel=&#39;median_income&#39;&gt;, &lt;AxesSubplot:xlabel=&#39;median_income&#39;, ylabel=&#39;median_income&#39;&gt;, &lt;AxesSubplot:xlabel=&#39;total_rooms&#39;, ylabel=&#39;median_income&#39;&gt;, &lt;AxesSubplot:xlabel=&#39;housing_median_age&#39;, ylabel=&#39;median_income&#39;&gt;], [&lt;AxesSubplot:xlabel=&#39;median_house_value&#39;, ylabel=&#39;total_rooms&#39;&gt;, &lt;AxesSubplot:xlabel=&#39;median_income&#39;, ylabel=&#39;total_rooms&#39;&gt;, &lt;AxesSubplot:xlabel=&#39;total_rooms&#39;, ylabel=&#39;total_rooms&#39;&gt;, &lt;AxesSubplot:xlabel=&#39;housing_median_age&#39;, ylabel=&#39;total_rooms&#39;&gt;], [&lt;AxesSubplot:xlabel=&#39;median_house_value&#39;, ylabel=&#39;housing_median_age&#39;&gt;, &lt;AxesSubplot:xlabel=&#39;median_income&#39;, ylabel=&#39;housing_median_age&#39;&gt;, &lt;AxesSubplot:xlabel=&#39;total_rooms&#39;, ylabel=&#39;housing_median_age&#39;&gt;, &lt;AxesSubplot:xlabel=&#39;housing_median_age&#39;, ylabel=&#39;housing_median_age&#39;&gt;]], dtype=object) . . The main diagonal (top left to bottom right) would be full of straight lines if pandas plotted each variable against itself, which would not be very useful. So instead pandas displays a histogram of each attribute. The diagonal option in scatter_matrix pick between ‘kde’ and ‘hist’ for either Kernel Density Estimation or Histogram plot in the diagonal. . The most promising attribute to predict the median house value is the median income( Pearson and Spearman correlation coefficient = 0.67 ), so let’s zoom in on their correlation scatterplot : . housing.plot( kind = &#39;scatter&#39; ,x = &#39;median_income&#39; ,y = &#39;median_house_value&#39; ,alpha = 0.2) &lt;AxesSubplot:xlabel=&#39;median_income&#39;, ylabel=&#39;median_house_value&#39;&gt; . . This plot reveals a few things : . First, the correlation is indeed very strong; we can clearly see the upward trend, and the points are not too dispersed. | Second, the price cap that we noticed earlier is clearly visible as a horizontal line at $500,000. But this plot reveals other less obvious straight lines: a horizontal line around $450,000, another around $350,000, perhaps one around $280,000, and a few more below that, we can see this picks in the histogram above: As result, we may want to try removing the corresponding districts to prevent our algorithms from learning to reproduce these data quirks. | . housing[&#39;median_house_value&#39;].describe() count 16512.000000 mean 207005.322372 std 115701.297250 min 14999.000000 25% 119800.000000 50% 179500.000000 75% 263900.000000 max 500001.000000 Name: median_house_value, dtype: float64 # Data picks in the target variable plt.hist(housing[&#39;median_house_value&#39;] #, bins = int( (housing[&#39;median_income&#39;].max() - housing[&#39;median_income&#39;].min()) / 0.5) , bins = int ((500001.000000 - 14999.000000)/1000) , color = &#39;blue&#39; ,edgecolor = &#39;black&#39; ) (array([ 3., 0., 0., 0., 0., 0., 0., 3., 0., 0., 0., 2., 1., 1., 0., 1., 0., 4., 1., 3., 1., 2., 4., 2., 3., 4., 5., 10., 13., 11., 14., 11., 19., 17., 21., 27., 24., 36., 33., 39., 59., 30., 53., 50., 38., 43., 39., 45., 45., 35., 54., 48., 74., 59., 62., 54., 47., 61., 51., 39., 51., 41., 33., 47., 37., 39., 64., 43., 54., 59., 63., 55., 106., 73., 55., 86., 53., 84., 74., 73., 81., 70., 77., 71., 64., 79., 59., 45., 73., 50., 55., 49., 55., 68., 65., 62., 72., 110., 78., 54., 56., 66., 55., 79., 53., 57., 55., 60., 59., 43., 83., 61., 54., 45., 59., 52., 61., 51., 55., 65., 59., 68., 144., 51., 75., 72., 65., 75., 74., 64., 68., 79., 57., 52., 38., 111., 76., 64., 67., 77., 63., 94., 75., 83., 76., 96., 74., 145., 73., 74., 92., 88., 55., 61., 61., 79., 68., 52., 64., 53., 91., 50., 57., 63., 68., 52., 69., 59., 84., 74., 65., 70., 114., 44., 56., 63., 70., 69., 57., 51., 64., 52., 37., 49., 40., 59., 41., 35., 32., 52., 47., 45., 34., 47., 51., 41., 37., 51., 53., 50., 51., 44., 49., 66., 44., 55., 50., 49., 46., 38., 107., 56., 50., 48., 48., 52., 60., 51., 44., 52., 40., 41., 49., 44., 43., 53., 49., 27., 51., 39., 43., 30., 47., 37., 22., 50., 33., 36., 42., 43., 35., 20., 34., 40., 29., 29., 37., 44., 41., 39., 38., 40., 40., 39., 35., 30., 43., 34., 34., 65., 21., 33., 25., 29., 38., 22., 23., 26., 30., 20., 22., 24., 36., 22., 25., 28., 26., 26., 21., 24., 26., 16., 15., 9., 31., 12., 17., 19., 19., 18., 17., 18., 14., 15., 19., 11., 22., 14., 18., 21., 23., 15., 9., 24., 16., 17., 18., 23., 20., 28., 12., 12., 21., 11., 22., 17., 22., 19., 22., 19., 25., 21., 15., 14., 20., 25., 22., 20., 18., 22., 22., 16., 13., 22., 75., 14., 19., 19., 19., 15., 22., 13., 18., 16., 21., 16., 19., 24., 11., 13., 16., 17., 13., 11., 15., 4., 18., 9., 8., 26., 8., 14., 6., 8., 12., 12., 11., 10., 12., 14., 5., 13., 16., 7., 7., 11., 10., 12., 14., 15., 9., 11., 10., 10., 22., 4., 2., 12., 2., 10., 12., 11., 2., 4., 14., 9., 10., 10., 5., 13., 5., 13., 8., 13., 7., 9., 3., 8., 8., 12., 5., 5., 5., 5., 2., 11., 7., 6., 9., 11., 7., 7., 7., 9., 7., 7., 6., 7., 8., 9., 6., 7., 5., 2., 28., 6., 5., 7., 6., 5., 3., 6., 10., 6., 6., 1., 6., 4., 4., 3., 3., 6., 5., 3., 5., 3., 4., 6., 3., 10., 1., 2., 8., 4., 1., 3., 1., 3., 10., 7., 2., 4., 4., 3., 3., 4., 3., 4., 4., 2., 6., 2., 2., 5., 810.]), array([ 14999. , 15999.00412371, 16999.00824742, 17999.01237113, 18999.01649485, 19999.02061856, 20999.02474227, 21999.02886598, 22999.03298969, 23999.0371134 , 24999.04123711, 25999.04536082, 26999.04948454, 27999.05360825, 28999.05773196, 29999.06185567, 30999.06597938, 31999.07010309, 32999.0742268 , 33999.07835052, 34999.08247423, 35999.08659794, 36999.09072165, 37999.09484536, 38999.09896907, 39999.10309278, 40999.10721649, 41999.11134021, 42999.11546392, 43999.11958763, 44999.12371134, 45999.12783505, 46999.13195876, 47999.13608247, 48999.14020619, 49999.1443299 , 50999.14845361, 51999.15257732, 52999.15670103, 53999.16082474, 54999.16494845, 55999.16907216, 56999.17319588, 57999.17731959, 58999.1814433 , 59999.18556701, 60999.18969072, 61999.19381443, 62999.19793814, 63999.20206186, 64999.20618557, 65999.21030928, 66999.21443299, 67999.2185567 , 68999.22268041, 69999.22680412, 70999.23092784, 71999.23505155, 72999.23917526, 73999.24329897, 74999.24742268, 75999.25154639, 76999.2556701 , 77999.25979381, 78999.26391753, 79999.26804124, 80999.27216495, 81999.27628866, 82999.28041237, 83999.28453608, 84999.28865979, 85999.29278351, 86999.29690722, 87999.30103093, 88999.30515464, 89999.30927835, 90999.31340206, 91999.31752577, 92999.32164948, 93999.3257732 , 94999.32989691, 95999.33402062, 96999.33814433, 97999.34226804, 98999.34639175, 99999.35051546, 100999.35463918, 101999.35876289, 102999.3628866 , 103999.36701031, 104999.37113402, 105999.37525773, 106999.37938144, 107999.38350515, 108999.38762887, 109999.39175258, 110999.39587629, 111999.4 , 112999.40412371, 113999.40824742, 114999.41237113, 115999.41649485, 116999.42061856, 117999.42474227, 118999.42886598, 119999.43298969, 120999.4371134 , 121999.44123711, 122999.44536082, 123999.44948454, 124999.45360825, 125999.45773196, 126999.46185567, 127999.46597938, 128999.47010309, 129999.4742268 , 130999.47835052, 131999.48247423, 132999.48659794, 133999.49072165, 134999.49484536, 135999.49896907, 136999.50309278, 137999.50721649, 138999.51134021, 139999.51546392, 140999.51958763, 141999.52371134, 142999.52783505, 143999.53195876, 144999.53608247, 145999.54020619, 146999.5443299 , 147999.54845361, 148999.55257732, 149999.55670103, 150999.56082474, 151999.56494845, 152999.56907216, 153999.57319588, 154999.57731959, 155999.5814433 , 156999.58556701, 157999.58969072, 158999.59381443, 159999.59793814, 160999.60206186, 161999.60618557, 162999.61030928, 163999.61443299, 164999.6185567 , 165999.62268041, 166999.62680412, 167999.63092784, 168999.63505155, 169999.63917526, 170999.64329897, 171999.64742268, 172999.65154639, 173999.6556701 , 174999.65979381, 175999.66391753, 176999.66804124, 177999.67216495, 178999.67628866, 179999.68041237, 180999.68453608, 181999.68865979, 182999.69278351, 183999.69690722, 184999.70103093, 185999.70515464, 186999.70927835, 187999.71340206, 188999.71752577, 189999.72164948, 190999.7257732 , 191999.72989691, 192999.73402062, 193999.73814433, 194999.74226804, 195999.74639175, 196999.75051546, 197999.75463918, 198999.75876289, 199999.7628866 , 200999.76701031, 201999.77113402, 202999.77525773, 203999.77938144, 204999.78350515, 205999.78762887, 206999.79175258, 207999.79587629, 208999.8 , 209999.80412371, 210999.80824742, 211999.81237113, 212999.81649485, 213999.82061856, 214999.82474227, 215999.82886598, 216999.83298969, 217999.8371134 , 218999.84123711, 219999.84536082, 220999.84948454, 221999.85360825, 222999.85773196, 223999.86185567, 224999.86597938, 225999.87010309, 226999.8742268 , 227999.87835052, 228999.88247423, 229999.88659794, 230999.89072165, 231999.89484536, 232999.89896907, 233999.90309278, 234999.90721649, 235999.91134021, 236999.91546392, 237999.91958763, 238999.92371134, 239999.92783505, 240999.93195876, 241999.93608247, 242999.94020619, 243999.9443299 , 244999.94845361, 245999.95257732, 246999.95670103, 247999.96082474, 248999.96494845, 249999.96907216, 250999.97319588, 251999.97731959, 252999.9814433 , 253999.98556701, 254999.98969072, 255999.99381443, 256999.99793814, 258000.00206186, 259000.00618557, 260000.01030928, 261000.01443299, 262000.0185567 , 263000.02268041, 264000.02680412, 265000.03092784, 266000.03505155, 267000.03917526, 268000.04329897, 269000.04742268, 270000.05154639, 271000.0556701 , 272000.05979381, 273000.06391753, 274000.06804124, 275000.07216495, 276000.07628866, 277000.08041237, 278000.08453608, 279000.08865979, 280000.09278351, 281000.09690722, 282000.10103093, 283000.10515464, 284000.10927835, 285000.11340206, 286000.11752577, 287000.12164948, 288000.1257732 , 289000.12989691, 290000.13402062, 291000.13814433, 292000.14226804, 293000.14639175, 294000.15051546, 295000.15463918, 296000.15876289, 297000.1628866 , 298000.16701031, 299000.17113402, 300000.17525773, 301000.17938144, 302000.18350515, 303000.18762887, 304000.19175258, 305000.19587629, 306000.2 , 307000.20412371, 308000.20824742, 309000.21237113, 310000.21649485, 311000.22061856, 312000.22474227, 313000.22886598, 314000.23298969, 315000.2371134 , 316000.24123711, 317000.24536082, 318000.24948454, 319000.25360825, 320000.25773196, 321000.26185567, 322000.26597938, 323000.27010309, 324000.2742268 , 325000.27835052, 326000.28247423, 327000.28659794, 328000.29072165, 329000.29484536, 330000.29896907, 331000.30309278, 332000.30721649, 333000.31134021, 334000.31546392, 335000.31958763, 336000.32371134, 337000.32783505, 338000.33195876, 339000.33608247, 340000.34020619, 341000.3443299 , 342000.34845361, 343000.35257732, 344000.35670103, 345000.36082474, 346000.36494845, 347000.36907216, 348000.37319588, 349000.37731959, 350000.3814433 , 351000.38556701, 352000.38969072, 353000.39381443, 354000.39793814, 355000.40206186, 356000.40618557, 357000.41030928, 358000.41443299, 359000.4185567 , 360000.42268041, 361000.42680412, 362000.43092784, 363000.43505155, 364000.43917526, 365000.44329897, 366000.44742268, 367000.45154639, 368000.4556701 , 369000.45979381, 370000.46391753, 371000.46804124, 372000.47216495, 373000.47628866, 374000.48041237, 375000.48453608, 376000.48865979, 377000.49278351, 378000.49690722, 379000.50103093, 380000.50515464, 381000.50927835, 382000.51340206, 383000.51752577, 384000.52164948, 385000.5257732 , 386000.52989691, 387000.53402062, 388000.53814433, 389000.54226804, 390000.54639175, 391000.55051546, 392000.55463918, 393000.55876289, 394000.5628866 , 395000.56701031, 396000.57113402, 397000.57525773, 398000.57938144, 399000.58350515, 400000.58762887, 401000.59175258, 402000.59587629, 403000.6 , 404000.60412371, 405000.60824742, 406000.61237113, 407000.61649485, 408000.62061856, 409000.62474227, 410000.62886598, 411000.63298969, 412000.6371134 , 413000.64123711, 414000.64536082, 415000.64948454, 416000.65360825, 417000.65773196, 418000.66185567, 419000.66597938, 420000.67010309, 421000.6742268 , 422000.67835052, 423000.68247423, 424000.68659794, 425000.69072165, 426000.69484536, 427000.69896907, 428000.70309278, 429000.70721649, 430000.71134021, 431000.71546392, 432000.71958763, 433000.72371134, 434000.72783505, 435000.73195876, 436000.73608247, 437000.74020619, 438000.7443299 , 439000.74845361, 440000.75257732, 441000.75670103, 442000.76082474, 443000.76494845, 444000.76907216, 445000.77319588, 446000.77731959, 447000.7814433 , 448000.78556701, 449000.78969072, 450000.79381443, 451000.79793814, 452000.80206186, 453000.80618557, 454000.81030928, 455000.81443299, 456000.8185567 , 457000.82268041, 458000.82680412, 459000.83092784, 460000.83505155, 461000.83917526, 462000.84329897, 463000.84742268, 464000.85154639, 465000.8556701 , 466000.85979381, 467000.86391753, 468000.86804124, 469000.87216495, 470000.87628866, 471000.88041237, 472000.88453608, 473000.88865979, 474000.89278351, 475000.89690722, 476000.90103093, 477000.90515464, 478000.90927835, 479000.91340206, 480000.91752577, 481000.92164948, 482000.9257732 , 483000.92989691, 484000.93402062, 485000.93814433, 486000.94226804, 487000.94639175, 488000.95051546, 489000.95463918, 490000.95876289, 491000.9628866 , 492000.96701031, 493000.97113402, 494000.97525773, 495000.97938144, 496000.98350515, 497000.98762887, 498000.99175258, 499000.99587629, 500001. ]), &lt;BarContainer object of 485 artists&gt;) . . Check docs on how to detect picks : . Finding peaks in the histograms of the variables : https://www.kaggle.com/simongrest/finding-peaks-in-the-histograms-of-the-variables . | Peak-finding algorithm for Python/SciPy : https://stackoverflow.com/questions/1713335/peak-finding-algorithm-for-python-scipy . | . Experimenting with Attribute Combinations . We identified a few data quirks that we may want to clean up before feeding the data to a Machine Learning algorithm, | We found interesting correlations between attributes, in particular with the target attribute. | We also noticed that some attributes have a tail-heavy distribution, so you may want to transform them (e.g., by computing their logarithm). | One last thing we may want to do before preparing the data for Machine Learning algorithms is to try out various attribute combinations : For example, the total number of rooms in a district is not very useful if we don’t know how many households there are. What we really want is the number of rooms per household. Similarly, the total number of bedrooms by itself is not very useful: you probably want to compare it to the number of rooms. And the population per household also seems like an interesting attribute combination to look at. Let’s create these new attributes: | . # We can see that some attributes are very linked to each others housing[[&#39;total_rooms&#39;,&#39;total_bedrooms&#39;,&#39;households&#39;,&#39;population&#39; ]].corr() total_rooms total_bedrooms households population total_rooms 1.000000 0.930380 0.918484 0.857126 total_bedrooms 0.930380 1.000000 0.979728 0.877747 households 0.918484 0.979728 1.000000 0.907222 population 0.857126 0.877747 0.907222 1.000000 . To highlight the matrix correlation, we can use heatmap from seaborn: . import seaborn as sns cor= housing[[&#39;total_rooms&#39;,&#39;total_bedrooms&#39;,&#39;households&#39;,&#39;population&#39; ]].corr() sns.heatmap(cor, cmap=&#39;Blues&#39;, annot= True) &lt;AxesSubplot:&gt; . . housing[&quot;rooms_per_household&quot;] = housing[&quot;total_rooms&quot;]/housing[&quot;households&quot;] housing[&quot;bedrooms_per_room&quot;] = housing[&quot;total_bedrooms&quot;]/housing[&quot;total_rooms&quot;] housing[&quot;population_per_household&quot;]=housing[&quot;population&quot;]/housing[&quot;households&quot;] # nbre of person per houshold housing[&quot;bedrooms_per_room&quot;].describe() count 20433.000000 mean 0.213039 std 0.057983 min 0.100000 25% 0.175427 50% 0.203162 75% 0.239821 max 1.000000 Name: bedrooms_per_room, dtype: float64 # on average, we have 21 bedrooms for 100 rooms from fractions import Fraction z = Fraction(0.21).limit_denominator() z Fraction(21, 100) corr_matrix = housing.corr() corr_matrix[&#39;median_house_value&#39;].sort_values(ascending = False) median_house_value 1.000000 median_income 0.688075 rooms_per_household 0.151948 total_rooms 0.134153 housing_median_age 0.105623 households 0.065843 total_bedrooms 0.049686 population_per_household -0.023737 population -0.024650 longitude -0.045967 latitude -0.144160 bedrooms_per_room -0.255880 Name: median_house_value, dtype: float64 . The new bedrooms_per_room attribute is much more correlated (0.25)with the median house value than the total number of rooms(0.13) or bedrooms (0.04) : . Apparently houses with a lower bedroom/room ratio tend to be more expensive. | The number of rooms per household is also more informative than the total number of rooms in a district—obviously the larger the houses, the more expensive they are. | . Prepare the Data for Machine Learning Algorithms . let’s revert to a clean training set (by copying strat_train_set once again). | Let’s also separate the predictors and the labels, since we don’t necessarily want to apply the same transformations to the predictors and the target values. | . # drop() creates a copy of the data and does not affect strat_train_set housing = strat_train_set.drop(&quot;median_house_value&quot;, axis=1) housing_labels = strat_train_set[&quot;median_house_value&quot;].copy() housing = strat_train_set.drop(&#39;median_house_value&#39;, axis = 1) housing_labels = strat_train_set[&#39;median_house_value&#39;].copy() . Data Cleaning . Formissing values (like for total_bedrooms), we have three options: . Get rid of the corresponding districts. | Get rid of the whole attribute. | Set the values to some value (zero, the mean, the median, etc.) | We can accomplish these easily using DataFrame’s dropna(), drop(), and fillna() . housing[&#39;total_bedrooms&#39;].describe() count 16354.000000 mean 534.914639 std 412.665649 min 2.000000 25% 295.000000 50% 433.000000 75% 644.000000 max 6210.000000 Name: total_bedrooms, dtype: float64 # in bar chart, NanN&#39;s are filled with 0&#39;s plt.hist(housing[&#39;total_bedrooms&#39;] , bins = int ((6210.000000 - 2.000000 )/500) , color = &#39;blue&#39; , edgecolor = &#39;black&#39; ) (array([1.0221e+04, 4.7670e+03, 9.1400e+02, 2.6100e+02, 9.9000e+01, 5.1000e+01, 1.7000e+01, 1.1000e+01, 7.0000e+00, 3.0000e+00, 2.0000e+00, 1.0000e+00]), array([2.00000000e+00, 5.19333333e+02, 1.03666667e+03, 1.55400000e+03, 2.07133333e+03, 2.58866667e+03, 3.10600000e+03, 3.62333333e+03, 4.14066667e+03, 4.65800000e+03, 5.17533333e+03, 5.69266667e+03, 6.21000000e+03]), &lt;BarContainer object of 12 artists&gt;) . . # to count the number of Nan&#39;s housing[&#39;total_bedrooms&#39;].isna().sum() 158 housing.isna().sum() longitude 0 latitude 0 housing_median_age 0 total_rooms 0 total_bedrooms 158 population 0 households 0 median_income 0 ocean_proximity 0 income_cat 0 dtype: int64 # option 1 : Get rid of the corresponding districts housing.dropna(subset = [&#39;total_bedrooms&#39;]) # drop from 16512 to 16354 using len() # option 2 : Get rid of the whole attribute housing.drop(&#39;total_bedrooms&#39;, axis = 1) # option 3 : Set the values to some value (zero, the mean, the median, etc.) median = housing[&#39;total_bedrooms&#39;].median() housing[&#39;total_bedrooms&#39;].fillna(median, inplace = True) longitude latitude housing_median_age total_rooms population 12655 -121.46 38.52 29.0 3873.0 2237.0 15502 -117.23 33.09 7.0 5320.0 2015.0 2908 -119.04 35.37 44.0 1618.0 667.0 14053 -117.13 32.75 24.0 1877.0 898.0 20496 -118.70 34.28 27.0 3536.0 1837.0 ... ... ... ... ... ... 15174 -117.07 33.03 14.0 6665.0 2026.0 12661 -121.42 38.51 15.0 7901.0 4769.0 19263 -122.72 38.44 48.0 707.0 458.0 19140 -122.70 38.31 14.0 3155.0 1208.0 19773 -122.14 39.97 27.0 1079.0 625.0 households median_income ocean_proximity income_cat 12655 706.0 2.1736 INLAND 2 15502 768.0 6.3373 NEAR OCEAN 5 2908 300.0 2.8750 INLAND 2 14053 483.0 2.2264 NEAR OCEAN 2 20496 580.0 4.4964 &lt;1H OCEAN 3 ... ... ... ... ... 15174 1001.0 5.0900 &lt;1H OCEAN 4 12661 1418.0 2.8139 INLAND 2 19263 172.0 3.1797 &lt;1H OCEAN 3 19140 501.0 4.1964 &lt;1H OCEAN 3 19773 197.0 3.1319 INLAND 3 [16512 rows x 9 columns] . For ‘option’ 3 : fill missings with some value, the median for example, we should : . Compute the median value on the training and use it to fill the missing values in the training set. | Save the median value that you have computed. | Using later for to replace missing values in the test set when we want to evaluate our system | Using it once the system goes live to replace missing values in new data. | . Scikit-Learn provides a handy class to take care of missing values: SimpleImputer. . from sklearn.impute import SimpleImputer . First, you need to create a SimpleImputer instance, specifying that we want to replace each numeric attribute’s missing values with the median of that attribute : . imputer = SimpleImputer(strategy = &#39;median&#39;) housing.dtypes longitude float64 latitude float64 housing_median_age float64 total_rooms float64 total_bedrooms float64 population float64 households float64 median_income float64 ocean_proximity object dtype: object # We drop the categorical variables since the median can only be computed on numerical attributes housing_num = housing.drop([&#39;ocean_proximity&#39;], axis = 1) . Now you can fit the imputer instance to the training data using the fit() method . imputer.fit(housing_num) SimpleImputer(strategy=&#39;median&#39;) . The imputer has simply computed the median of each attribute and stored the result in its statistics_ instance variable. We apply the imputer to all the numerical attributes : . imputer.statistics_ array([-118.51 , 34.26 , 29. , 2119. , 433. , 1164. , 408. , 3.54155]) housing_num.columns Index([&#39;longitude&#39;, &#39;latitude&#39;, &#39;housing_median_age&#39;, &#39;total_rooms&#39;, &#39;total_bedrooms&#39;, &#39;population&#39;, &#39;households&#39;, &#39;median_income&#39;], dtype=&#39;object&#39;) # equivalant to imputer.statistics_ : we have the median for each numeric variable housing_num.median().values array([-118.51 , 34.26 , 29. , 2119. , 433. , 1164. , 408. , 3.54155]) . Now we can use this trained imputer to transform the training set by replacing missing values with the learned medians: . # The result is a plain NumPy array containing the transformed features. X = imputer.transform(housing_num) #If you want to put it back into a pandas DataFrame, it’s simple: housing_tr = pd.DataFrame(X, columns=housing_num.columns, index=housing_num.index) . Alternative method to fit() and transform() method is using directy fit_transform() method . X = imputer.fit_transform(housing_num) housing_tr = pd.DataFrame( X, columns = housing_num.columns , index = housing_num.index) housing_tr longitude latitude housing_median_age total_rooms total_bedrooms 12655 -121.46 38.52 29.0 3873.0 797.0 15502 -117.23 33.09 7.0 5320.0 855.0 2908 -119.04 35.37 44.0 1618.0 310.0 14053 -117.13 32.75 24.0 1877.0 519.0 20496 -118.70 34.28 27.0 3536.0 646.0 ... ... ... ... ... ... 15174 -117.07 33.03 14.0 6665.0 1231.0 12661 -121.42 38.51 15.0 7901.0 1422.0 19263 -122.72 38.44 48.0 707.0 166.0 19140 -122.70 38.31 14.0 3155.0 580.0 19773 -122.14 39.97 27.0 1079.0 222.0 population households median_income 12655 2237.0 706.0 2.1736 15502 2015.0 768.0 6.3373 2908 667.0 300.0 2.8750 14053 898.0 483.0 2.2264 20496 1837.0 580.0 4.4964 ... ... ... ... 15174 2026.0 1001.0 5.0900 12661 4769.0 1418.0 2.8139 19263 458.0 172.0 3.1797 19140 1208.0 501.0 4.1964 19773 625.0 197.0 3.1319 [16512 rows x 8 columns] . Handling Text and Categorical Attributes . So far we have only dealt with numerical attributes, but now let’s look at text attributes. In this dataset, there is just one: the ocean_proximity attribute. Let’s look at its value for the first 10 instances: . housing_cat = housing[[&#39;ocean_proximity&#39;]] housing_cat.head(10) housing_cat.nunique() ocean_proximity 5 dtype: int64 housing[&#39;ocean_proximity&#39;].unique() array([&#39;INLAND&#39;, &#39;NEAR OCEAN&#39;, &#39;&lt;1H OCEAN&#39;, &#39;NEAR BAY&#39;, &#39;ISLAND&#39;], dtype=object) . It’s not arbitrary text: there are a limited number of possible values, each of which represents a category. So this attribute is a categorical attribute. Most Machine Learning algorithms prefer to work with numbers, so let’s convert these categories from text to numbers. For this, we can use Scikit-Learn’s OrdinalEncoder class : . from sklearn.preprocessing import OrdinalEncoder ordinal_encoder = OrdinalEncoder() housing_cat_encoded = ordinal_encoder.fit_transform(housing_cat) # categorical dataframe housing_cat_encoded[:10] array([[1.], [4.], [1.], [4.], [0.], [3.], [0.], [0.], [0.], [0.]]) housing_cat_encoded.shape (16512, 1) # we can get the list of categories using the categories_ instance variable. It is a list containing a 1D array #of categories for each categorical attribute ordinal_encoder.categories_ [array([&#39;&lt;1H OCEAN&#39;, &#39;INLAND&#39;, &#39;ISLAND&#39;, &#39;NEAR BAY&#39;, &#39;NEAR OCEAN&#39;], dtype=object)] np.unique(housing_cat_encoded) array([0., 1., 2., 3., 4.]) . One issue with this representation is that ML algorithms ( OrdinaEncoder) will assume that two nearby values are more similar than two distant values. This may be fine in some cases (e.g., for ordered categories such as “bad,” “average,” “good,” and “excellent”) : ordinal encoding for categorical variables that have a natural rank ordering but it is obviously not the case for the ocean_proximity column (for example, categories 0 and 4 are clearly more similar than categories 0 and 1). . To fix this issue, a common solution is to create one binary attribute per category: one attribute equal to 1 when the category is “&lt;1H OCEAN” (and 0 otherwise), another attribute equal to 1 when the category is “INLAND” (and 0 otherwise), and so on. This is called one-hot encoding. The new attributes are sometimes called dummy attributes. Scikit-Learn provides a OneHotEncoder class to convert categorical values into one-hot vectors . See this blogpost : https://machinelearningmastery.com/one-hot-encoding-for-categorical-data/ . from sklearn.preprocessing import OneHotEncoder cat_encoder = OneHotEncoder() housing_cat_1hot = cat_encoder.fit_transform(housing_cat) housing_cat_1hot &lt;16512x5 sparse matrix of type &#39;&lt;class &#39;numpy.float64&#39;&gt;&#39; with 16512 stored elements in Compressed Sparse Row format&gt; . Notice that the output is a SciPy sparse matrix, instead of a NumPy array. This is very useful when you have categorical attributes with thousands of categories. After onehot encoding, we get a matrix with thousands of columns, and the matrix is full of 0s except for a single 1 per row. Using up tons of memory mostly to store zeros would be very wasteful, so instead a sparse matrix only stores the location of the nonzero elements. we can use it mostly like a normal 2D array,but if we really want to convert it to a (dense) NumPy array, we call the toarray() method: . housing_cat_1hot.toarray() array([[0., 1., 0., 0., 0.], [0., 0., 0., 0., 1.], [0., 1., 0., 0., 0.], ..., [1., 0., 0., 0., 0.], [1., 0., 0., 0., 0.], [0., 1., 0., 0., 0.]]) housing_cat.head(3) ocean_proximity 12655 INLAND 15502 NEAR OCEAN 2908 INLAND # to get the list of categories cat_encoder.categories_ [array([&#39;&lt;1H OCEAN&#39;, &#39;INLAND&#39;, &#39;ISLAND&#39;, &#39;NEAR BAY&#39;, &#39;NEAR OCEAN&#39;], dtype=object)] . If a categorical attribute has a large number of possible categories (e.g., country code, profession, species), then one-hot encoding will result in a large number of input features. This may slow down training and degrade performance. . If this happens, we may want to replace the categorical input with useful numerical features related to the categories: for example, we could replace the ocean_proximity feature with the distance to the ocean (similarly, a country code could be replaced with the country’s population and GDP per capita). Alternatively, we could replace each category with a learnable, low-dimensional vector called an embedding. . Each category’s representation would be learned during training. This is an example of representation learning. . Custom Transformers . retun back for more details ? see blogpost : https://towardsdatascience.com/pipelines-custom-transformers-in-scikit-learn-the-step-by-step-guide-with-python-code-4a7d9b068156 . and this : https://github.com/ageron/handson-ml2/blob/master/02_end_to_end_machine_learning_project.ipynb . housing.values[: ,4:] array([[797.0, 2237.0, 706.0, 2.1736, &#39;INLAND&#39;, 2], [855.0, 2015.0, 768.0, 6.3373, &#39;NEAR OCEAN&#39;, 5], [310.0, 667.0, 300.0, 2.875, &#39;INLAND&#39;, 2], ..., [166.0, 458.0, 172.0, 3.1797, &#39;&lt;1H OCEAN&#39;, 3], [580.0, 1208.0, 501.0, 4.1964, &#39;&lt;1H OCEAN&#39;, 3], [222.0, 625.0, 197.0, 3.1319, &#39;INLAND&#39;, 3]], dtype=object) housing.head(3) longitude latitude housing_median_age total_rooms total_bedrooms 12655 -121.46 38.52 29.0 3873.0 797.0 15502 -117.23 33.09 7.0 5320.0 855.0 2908 -119.04 35.37 44.0 1618.0 310.0 population households median_income ocean_proximity income_cat 12655 2237.0 706.0 2.1736 INLAND 2 15502 2015.0 768.0 6.3373 NEAR OCEAN 5 2908 667.0 300.0 2.8750 INLAND 2 from sklearn.base import BaseEstimator, TransformerMixin rooms_ix, bedrooms_ix, population_ix, households_ix = 3, 4, 5, 6 class CombinedAttributesAdder(BaseEstimator, TransformerMixin): def __init__(self, add_bedrooms_per_room = True): # no *args or **kargs self.add_bedrooms_per_room = add_bedrooms_per_room def fit(self, X, y=None): return self # nothing else to do def transform(self, X): rooms_per_household = X[:, rooms_ix] / X[:, households_ix] population_per_household = X[:, population_ix] / X[:, households_ix] if self.add_bedrooms_per_room: bedrooms_per_room = X[:, bedrooms_ix] / X[:, rooms_ix] return np.c_[X, rooms_per_household, population_per_household, bedrooms_per_room] else: return np.c_[X, rooms_per_household, population_per_household] attr_adder = CombinedAttributesAdder(add_bedrooms_per_room=False ) housing_extra_attribs = attr_adder.transform(housing.values) housing_extra_attribs array([[-121.46, 38.52, 29.0, ..., 2, 5.485835694050992, 3.168555240793201], [-117.23, 33.09, 7.0, ..., 5, 6.927083333333333, 2.6236979166666665], [-119.04, 35.37, 44.0, ..., 2, 5.3933333333333335, 2.223333333333333], ..., [-122.72, 38.44, 48.0, ..., 3, 4.1104651162790695, 2.6627906976744184], [-122.7, 38.31, 14.0, ..., 3, 6.297405189620759, 2.411177644710579], [-122.14, 39.97, 27.0, ..., 3, 5.477157360406092, 3.1725888324873095]], dtype=object) . Feature Scaling . One of the most important transformations you need to apply to your data is feature scaling. With few exceptions, Machine Learning algorithms don’t perform well when the input numerical attributes have very different scales . This is the case for the housing data: total_rooms ranges from about 6 to 39320, while median_income only range from 0 to 15 : . Note that scaling the target values is generally not required. . housing_num.describe().T count mean std min 25% longitude 16512.0 -119.575635 2.001828 -124.3500 -121.80000 latitude 16512.0 35.639314 2.137963 32.5400 33.94000 housing_median_age 16512.0 28.653404 12.574819 1.0000 18.00000 total_rooms 16512.0 2622.539789 2138.417080 6.0000 1443.00000 total_bedrooms 16512.0 533.939438 410.806260 2.0000 296.00000 population 16512.0 1419.687379 1115.663036 3.0000 784.00000 households 16512.0 497.011810 375.696156 2.0000 279.00000 median_income 16512.0 3.875884 1.904931 0.4999 2.56695 50% 75% max longitude -118.51000 -118.010000 -114.3100 latitude 34.26000 37.720000 41.9500 housing_median_age 29.00000 37.000000 52.0000 total_rooms 2119.00000 3141.000000 39320.0000 total_bedrooms 433.00000 641.000000 6210.0000 population 1164.00000 1719.000000 35682.0000 households 408.00000 602.000000 5358.0000 median_income 3.54155 4.745325 15.0001 . There are two common ways to get all attributes to have the same scale: . min-max scaling : ranging from 0 to 1. Scikit-Learn provides a transformer called MinMaxScaler for this. It has a feature_range hyperparameter that lets you change the range if, for some reason, you don’t want 0–1 : https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html . | Standardization : returns has unit variance and unlike min-max scaling, standardization does not bound values to a specific range, which may be a problem for some algorithms (e.g., neural networks often expect an input value ranging from 0 to 1).However, standardization is much less affected by outliers. Scikit-Learn provides a transformer called StandardScaler for standardization : https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html . | . Transformation Pipelines . As we can see, there are many data transformation steps that need to be executed in the right order. Fortunately, Scikit-Learn provides the Pipeline class to help with such sequences of transformations. . Here is a small pipeline for the numerical attributes: . from sklearn.pipeline import Pipeline from sklearn.impute import SimpleImputer from sklearn.preprocessing import StandardScaler,MinMaxScaler # plus the class add attribure that we created . The Pipeline constructor takes a list of name/estimator pairs defining a sequence of steps. All but the last estimator must be transformers (i.e., they must have a fit_transform() method). . # This pipeline is for numeric pipeline, we call is num_pipeline num_pipeline = Pipeline([ (&#39;imputer&#39;, SimpleImputer(strategy = &#39;median&#39;)), #(&#39;attribs_adder&#39;, CombinedAttributesAdder()), (&#39;std_scaler&#39;, StandardScaler()) ]) housing_num_tr = num_pipeline.fit_transform(housing_num) . So far, we have handled the categorical columns and the numerical columns separately. It would be more convenient to have a single transformer able to handle all columns, applying the appropriate transformations to each column. In version 0.20, Scikit-Learn introduced the ColumnTransformer for this purpose, and the good news is that it works great with pandas DataFrames. Let’s use it to apply all the transformations to the housing data: . from sklearn.compose import ColumnTransformer num_attribs = list(housing_num) cat_attribs = [&#39;ocean_proximity&#39;] NameError Traceback (most recent call last) &lt;ipython-input-2-344d101417c2&gt; in &lt;module&gt; -&gt; 1 num_attribs = list(housing_num) 2 cat_attribs = [&#39;ocean_proximity&#39;] NameError: name &#39;housing_num&#39; is not defined full_pipeline = ColumnTransformer([ (&#39;num&#39;, num_pipeline, num_attribs), (&#39;cat&#39;, OneHotEncoder(), cat_attribs) ] ) housing_prepared = full_pipeline.fit_transform(housing) housing_prepared array([[-0.94135046, 1.34743822, 0.02756357, ..., 0. , 0. , 0. ], [ 1.17178212, -1.19243966, -1.72201763, ..., 0. , 0. , 1. ], [ 0.26758118, -0.1259716 , 1.22045984, ..., 0. , 0. , 0. ], ..., [-1.5707942 , 1.31001828, 1.53856552, ..., 0. , 0. , 0. ], [-1.56080303, 1.2492109 , -1.1653327 , ..., 0. , 0. , 0. ], [-1.28105026, 2.02567448, -0.13148926, ..., 0. , 0. , 0. ]]) housing_prepared[0].shape (13,) housing_prepared.shape (16512, 13) . to read carefully for later : . First we import the ColumnTransformer class, next we get the list of numerical column . names and the list of categorical column names, and then we construct a Colum nTransformer. The constructor requires a list of tuples, where each tuple contains a name,22 a transformer, and a list of names (or indices) of columns that the transformer should be applied to. In this example, we specify that the numerical columns should be transformed using the num_pipeline that we defined earlier, and the categorical columns should be transformed using a OneHotEncoder. Finally, we apply this ColumnTransformer to the housing data: it applies each transformer to the appropriate columns and concatenates the outputs along the second axis (the transformers must return the same number of rows). Note that the OneHotEncoder returns a sparse matrix, while the num_pipeline returns a dense matrix. When there is such a mix of sparse and dense matrices, the Colum nTransformer estimates the density of the final matrix (i.e., the ratio of nonzero cells), and it returns a sparse matrix if the density is lower than a given threshold (by default, sparse_threshold=0.3). In this example, it returns a dense matrix. And that’s it! We have a preprocessing pipeline that takes the full housing data and applies the appropriate transformations to each column. Instead of using a transformer, you can specify the string “drop” if you want the columns to be dropped, or you can specify “pass through” if you want the columns to be left untouched. By default, the remaining columns (i.e., the ones that were not listed) will be dropped, but you can set the remainder hyperparameter to any transformer (or to “passthrough”) if you want these columns to be handled differently. If you are using Scikit-Learn 0.19 or earlier, you can use a third-party library such as sklearn-pandas, or you can roll out your own custom transformer to get the same functionality as the ColumnTransformer. Alternatively, you can use the FeatureUnion class, which can apply different transformers and concatenate their outputs. But you cannot specify different columns for each transformer; they all apply to the whole data. It is possible to work around this limitation using a custom transformer for column selection (see the Jupyter notebook for an example). . see link : https://github.com/ageron/handson-ml2/blob/master/02_end_to_end_machine_learning_project.ipynb . Select and Train a Model . we framed the problem | we got the data and explored it | we sampled a training set and a test set | we wrote transformation pipelines to clean up and prepare your data for Machine Learning algorithms automatically. | . Training and Evaluating on the Training Set . Let’s first train a Linear Regression model . housing_prepared.shape (16512, 13) from sklearn.linear_model import LinearRegression lin_reg = LinearRegression() lin_reg.fit(housing_prepared, housing_labels) LinearRegression() . Let’s try it out on a few instances from the training set: . # let&#39;s try the full preprocessing pipeline on a few training instances some_data = housing.iloc[:5] some_labels = housing_labels.iloc[:5] some_data_prepared = full_pipeline.transform(some_data) print(&quot;Predictions:&quot;, lin_reg.predict(some_data_prepared)) Predictions: [ 88983.14806384 305351.35385026 153334.71183453 184302.55162102 246840.18988841] print(&quot;Labels:&quot;, list(some_labels)) Labels: [72100.0, 279600.0, 82700.0, 112500.0, 238300.0] some_data_prepared.shape (5, 13) . Some remarks : If we encotered missing values are in the test set ? ( OneHotEncode() has a paramete : handle_unknown = ‘ignore’) | Indexing and selection data : if we want to modifiy a certain column in the dataframe, we should not proceed in this way : df[‘ocean_proxemity][0]= np.nan but rather copy the dataset first and then df.loc[0,’ocean_proxemity’]=np.nan. Read : https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy | . | . More on sklearn pipelines : https://scikit-learn.org/stable/auto_examples/compose/plot_column_transformer_mixed_types.html . The iloc indexer for Pandas Dataframe is used for integer-location based indexing / selection by position. | The Pandas loc indexer can be used with DataFrames for two different use cases: Selecting rows by label/index | Selecting rows with a boolean / conditional lookup | . | . Read : https://www.shanelynn.ie/pandas-iloc-loc-select-rows-and-columns-dataframe/ . Let’s measure this regression model’s RMSE on the whole training set using Scikit-Learn’s mean_squared_error() function : . from sklearn.metrics import mean_squared_error # housing_predictions = lin_reg.predict(housing_prepared) housing_predictions[:4] array([ 88983.14806384, 305351.35385026, 153334.71183453, 184302.55162102]) housing_labels 12655 72100.0 15502 279600.0 2908 82700.0 14053 112500.0 20496 238300.0 ... 15174 268500.0 12661 90400.0 19263 140400.0 19140 258100.0 19773 62700.0 Name: median_house_value, Length: 16512, dtype: float64 #RMSE lin_rmse = mean_squared_error(housing_predictions,housing_labels, squared= False) lin_rmse 69050.56219504567 . Most districts’ median_housing_values range between $120,000 and $265,000, so a typical prediction error of $69,050 is not very satisfying: . housing_labels.describe() count 16512.000000 mean 207005.322372 std 115701.297250 min 14999.000000 25% 119800.000000 50% 179500.000000 75% 263900.000000 max 500001.000000 Name: median_house_value, dtype: float64 . This is an example of a model underfitting the training data : When this happens it can mean that the features do not provide enough information to make good predictions, or that the model is not powerful enough. As we saw in the previous chapter, the main ways to fix underfitting are to select a more powerful model, to feed the training algorithm with better features, or to reduce the constraints on the model. . Let’s train a DecisionTreeRegressor. This is a powerful model, capable of finding complex nonlinear relationships in the data | . from sklearn.tree import DecisionTreeRegressor tree_reg = DecisionTreeRegressor() tree_reg.fit(housing_prepared, housing_labels) DecisionTreeRegressor() #Now that the model is trained, let’s evaluate it on the training set: housing_predictions = tree_reg.predict(housing_prepared) tree_rmse = mean_squared_error(housing_labels, housing_predictions, squared=False) tree_rmse 0.0 . As we saw earlier, we don’t want to touch the test set until we are ready to launch a model we are confident about, so we need to use part of the training set for training and part of it for model validation. . Better Evaluation Using Cross-Validation . One way to evaluate the Decision Tree model would be to use the train_test_split() function to split the training set into a smaller training set and a validation set, then train our models against the smaller training set and evaluate them against the validation set. . | A great alternative is to use Scikit-Learn’s K-fold cross-validation feature : The following code randomly splits the training set into 10 distinct subsets called folds, then it trains and evaluates the Decision Tree model 10 times, picking a different fold for evaluation every time and training on the other 9 folds. The result is an array containing the 10 evaluation scores: . | . from sklearn.model_selection import cross_val_score # to get the &#39;scoring&#39; options, use ssorted(sklearn.metrics.SCORERS.keys()) scores = - cross_val_score(tree_reg, housing_prepared, housing_labels, scoring=&#39;neg_root_mean_squared_error&#39;, cv=10) scores array([71270.15951523, 68888.32011559, 64997.85188763, 69263.03318422, 68197.14503697, 68963.98885461, 73536.17215975, 69183.4936482 , 66243.08004208, 71783.50940468]) . to get the ‘scoring’ options, use ssorted(sklearn.metrics.SCORERS.keys()) | Scikit-Learn’s cross-validation features expect a utility function (greater is better) rather than a cost function (lower is better), so the scoring function is actually the opposite of the MSE (i.e., a negative value) | . # display the results def display_scores(scores): print(&#39;Scores :&#39;,scores) print(&#39;Mean :&#39;,scores.mean()) print(&#39;Standard deviation :&#39;,scores.std()) display_scores(scores) Scores : [71270.15951523 68888.32011559 64997.85188763 69263.03318422 68197.14503697 68963.98885461 73536.17215975 69183.4936482 66243.08004208 71783.50940468] Mean : 69232.67538489516 Standard deviation : 2394.0765898258674 . Cross-validation allows you to get not only an estimate of the performance of your model, but also a measure of how precise this estimate is (i.e., its standard deviation). The Decision Tree has a score of approximately 69,232, generally ±2,394. We would not have this information if we just used one validation set. | Let’s compute the same scores for the Linear Regression model just to be sure: | . lin_scores = - cross_val_score(lin_reg,housing_prepared, housing_labels, scoring=&#39;neg_root_mean_squared_error&#39;, cv = 10) lin_scores array([72229.03469752, 65318.2240289 , 67706.39604745, 69368.53738998, 66767.61061621, 73003.75273869, 70522.24414582, 69440.77896541, 66930.32945876, 70756.31946074]) display_scores(lin_scores) Scores : [72229.03469752 65318.2240289 67706.39604745 69368.53738998 66767.61061621 73003.75273869 70522.24414582 69440.77896541 66930.32945876 70756.31946074] Mean : 69204.32275494766 Standard deviation : 2372.07079105592 . The Decision Tree model is overfitting so badly that it performs worse than the Linear Regression model. . Let’s try one last model now: the RandomForestRegressor : Random Forests work by training many Decision Trees on random subsets of the features, then averaging out their predictions. Building a model on top of many other models is called Ensemble Learning, and it is often a great way to push ML algorithms even further. . from sklearn.ensemble import RandomForestRegressor forest_reg = RandomForestRegressor() forest_reg.fit(housing_prepared, housing_labels) RandomForestRegressor() forest_reg_scores = - cross_val_score(forest_reg,housing_prepared, housing_labels, scoring=&#39;neg_root_mean_squared_error&#39;, cv = 10) forest_reg_scores array([50311.08798022, 49043.69572163, 46081.95238283, 50467.56214907, 47657.84153211, 49419.96274189, 51772.42197545, 49030.57976501, 47498.17482111, 53167.33074077]) display_scores(forest_reg_scores) Scores : [50311.08798022 49043.69572163 46081.95238283 50467.56214907 47657.84153211 49419.96274189 51772.42197545 49030.57976501 47498.17482111 53167.33074077] Mean : 49445.06098101039 Standard deviation : 1992.3842490271882 forest_predictions = forest_reg.predict(housing_prepared) forst_rmse = mean_squared_error(housing_labels, forest_predictions, squared=False) forst_rmse 18266.74368085342 . Note that the score on the training set(18,266) is still much lower than on the validation sets(49,445 +/-1992.38), meaning that the model is still overfitting the training set. . We should save every model we experiment with so that er can come back easily to any model you want. Make sure you save both the hyperparameters and the trained parameters, as well as the cross-validation scores and perhaps the actual predictions as well. This will allow you to easily compare scores across model types, and compare the types of errors they make. . | We can easily save Scikit-Learn models by using Python’s pickle module or by using the joblib library, which is more efficient at serializing large NumPy arrays (you can install this library using pip): . | . pip install joblib Requirement already satisfied: joblib in /Users/rmbp/opt/anaconda3/lib/python3.7/site-packages (1.1.0) Note: you may need to restart the kernel to use updated packages. import joblib # random forest : joblib.dump(forest_reg,&#39;rmd_forest.pkl&#39;) # saving the model as pkl file and named &#39;rmd_forest.pkl model_reload = joblib.load(&#39;rmd_forest.pkl&#39;) # loading the model rmd_forest_prediction = model_reload.predict(housing_prepared) # saving the predictions rmd_forest_rmse = mean_squared_error(rmd_forest_prediction, housing_labels) # saving the rmse on the train test to check overfit rmd_forest_cross_validation = -cross_val_score(model_reload, housing_prepared, housing_labels, scoring=&#39;neg_root_mean_squared_error&#39;, cv = 10) # rmse on validation to check overfit display_scores(rmd_forest_cross_validation) # cross validation score Scores : [50992.51555592 49288.84220573 46237.11091931 50248.85062075 47806.3116179 49272.797347 51801.0468531 48800.83283468 47540.31917616 53091.63650718] Mean : 49508.0263637728 Standard deviation : 1972.8867918884973 . Fine-Tune Our Model . Grid Search . Using Scikit-Learn’s GridSearchCV, All we need to do is tell it which hyperparameters we want it to experiment with and what valuesto try out, and it will use cross-validation to evaluate all the possible combinations of hyperparameter values : . from sklearn.model_selection import GridSearchCV param_grid = [ {&#39;n_estimators&#39;: [3, 10, 30], &#39;max_features&#39;: [2, 4, 6, 8]}, {&#39;bootstrap&#39;: [False], &#39;n_estimators&#39;: [3, 10], &#39;max_features&#39;: [2, 3, 4]}, ] forest_reg = RandomForestRegressor() grid_search = GridSearchCV(forest_reg, param_grid, cv=5, scoring=&#39;neg_root_mean_squared_error&#39;, return_train_score = True) grid_search.fit(housing_prepared, housing_labels) GridSearchCV(cv=5, estimator=RandomForestRegressor(), param_grid=[{&#39;max_features&#39;: [2, 4, 6, 8], &#39;n_estimators&#39;: [3, 10, 30]}, {&#39;bootstrap&#39;: [False], &#39;max_features&#39;: [2, 3, 4], &#39;n_estimators&#39;: [3, 10]}], return_train_score=True, scoring=&#39;neg_root_mean_squared_error&#39;) ?GridSearchCV . This param_grid tells Scikit-Learn to first evaluate all 3 × 4 = 12 combinations of n_estimators and max_features hyperparameter values specified in the first dict. -Then try all 2 × 3 = 6 combinations of hyperparameter values in the second dict, but this time with the bootstrap hyperparameter set to False instead of True | The grid search will explore 12 + 6 = 18 combinations of `RandomForestRegressor hyperparameter values`` | And it will train each model 5 times (cv=5).In other words, all in all, there will be 18 × 5 = 90 rounds of training. | . We can get the best combination of parameters like this : . grid_search.best_params_ #we should probably try searching again with higher values; the score may continue to improve. {&#39;max_features&#39;: 8, &#39;n_estimators&#39;: 30} grid_search.best_estimator_ RandomForestRegressor(max_features=8, n_estimators=30) cvres = grid_search.cv_results_ for mean_score, params in zip(cvres[&quot;mean_test_score&quot;], cvres[&quot;params&quot;]): print(-mean_score, params) 64530.85647414619 {&#39;max_features&#39;: 2, &#39;n_estimators&#39;: 3} 55067.77832337284 {&#39;max_features&#39;: 2, &#39;n_estimators&#39;: 10} 52781.7167175866 {&#39;max_features&#39;: 2, &#39;n_estimators&#39;: 30} 60327.066875895776 {&#39;max_features&#39;: 4, &#39;n_estimators&#39;: 3} 52586.95798629394 {&#39;max_features&#39;: 4, &#39;n_estimators&#39;: 10} 50346.63948997191 {&#39;max_features&#39;: 4, &#39;n_estimators&#39;: 30} 58398.87657548992 {&#39;max_features&#39;: 6, &#39;n_estimators&#39;: 3} 52075.368300249116 {&#39;max_features&#39;: 6, &#39;n_estimators&#39;: 10} 50093.621910952315 {&#39;max_features&#39;: 6, &#39;n_estimators&#39;: 30} 58628.430409285626 {&#39;max_features&#39;: 8, &#39;n_estimators&#39;: 3} 51936.797178963665 {&#39;max_features&#39;: 8, &#39;n_estimators&#39;: 10} 50089.501357132904 {&#39;max_features&#39;: 8, &#39;n_estimators&#39;: 30} 62303.627420601595 {&#39;bootstrap&#39;: False, &#39;max_features&#39;: 2, &#39;n_estimators&#39;: 3} 54183.89357722118 {&#39;bootstrap&#39;: False, &#39;max_features&#39;: 2, &#39;n_estimators&#39;: 10} 60004.47703668058 {&#39;bootstrap&#39;: False, &#39;max_features&#39;: 3, &#39;n_estimators&#39;: 3} 52603.81684475204 {&#39;bootstrap&#39;: False, &#39;max_features&#39;: 3, &#39;n_estimators&#39;: 10} 58002.985249363366 {&#39;bootstrap&#39;: False, &#39;max_features&#39;: 4, &#39;n_estimators&#39;: 3} 52121.979634950054 {&#39;bootstrap&#39;: False, &#39;max_features&#39;: 4, &#39;n_estimators&#39;: 10} . Rq: Don’t forget that we can treat some of the data preparation steps as hyperparameters. The grid search will automatically find out whether or not to add a feature you were not sure about. Ex : using the add_bedrooms_per_room hyperparameter of your CombinedAttributesAdder transformer). . .",
            "url": "https://younesszaim.github.io/myportfolio/2021/11/15/Sickit-Learn-for-Machine-Learning.html",
            "relUrl": "/2021/11/15/Sickit-Learn-for-Machine-Learning.html",
            "date": " • Nov 15, 2021"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Hi ! I’m Youness ZAIM. . Please check my resume on Linkedin. . You can also email me on zaimyouness9797@gmail.com. .",
          "url": "https://younesszaim.github.io/myportfolio/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://younesszaim.github.io/myportfolio/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}